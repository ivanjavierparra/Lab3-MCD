{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70c14412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c3bf992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>periodo</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>plan_precios_cuidados</th>\n",
       "      <th>cust_request_qty</th>\n",
       "      <th>cust_request_tn</th>\n",
       "      <th>tn</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>sku_size</th>\n",
       "      <th>stock_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201701</td>\n",
       "      <td>10234</td>\n",
       "      <td>20524</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.05300</td>\n",
       "      <td>0.05300</td>\n",
       "      <td>HC</td>\n",
       "      <td>VAJILLA</td>\n",
       "      <td>Cristalino</td>\n",
       "      <td>Importado</td>\n",
       "      <td>500.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201701</td>\n",
       "      <td>10032</td>\n",
       "      <td>20524</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.13628</td>\n",
       "      <td>0.13628</td>\n",
       "      <td>HC</td>\n",
       "      <td>VAJILLA</td>\n",
       "      <td>Cristalino</td>\n",
       "      <td>Importado</td>\n",
       "      <td>500.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201701</td>\n",
       "      <td>10217</td>\n",
       "      <td>20524</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.03028</td>\n",
       "      <td>0.03028</td>\n",
       "      <td>HC</td>\n",
       "      <td>VAJILLA</td>\n",
       "      <td>Cristalino</td>\n",
       "      <td>Importado</td>\n",
       "      <td>500.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201701</td>\n",
       "      <td>10125</td>\n",
       "      <td>20524</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02271</td>\n",
       "      <td>0.02271</td>\n",
       "      <td>HC</td>\n",
       "      <td>VAJILLA</td>\n",
       "      <td>Cristalino</td>\n",
       "      <td>Importado</td>\n",
       "      <td>500.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201701</td>\n",
       "      <td>10012</td>\n",
       "      <td>20524</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1.54452</td>\n",
       "      <td>1.54452</td>\n",
       "      <td>HC</td>\n",
       "      <td>VAJILLA</td>\n",
       "      <td>Cristalino</td>\n",
       "      <td>Importado</td>\n",
       "      <td>500.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2945813</th>\n",
       "      <td>201912</td>\n",
       "      <td>10105</td>\n",
       "      <td>20853</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02230</td>\n",
       "      <td>0.02230</td>\n",
       "      <td>PC</td>\n",
       "      <td>CABELLO</td>\n",
       "      <td>Shampoo Bebe</td>\n",
       "      <td>NIVEA</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1.82373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2945814</th>\n",
       "      <td>201912</td>\n",
       "      <td>10092</td>\n",
       "      <td>20853</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00669</td>\n",
       "      <td>0.00669</td>\n",
       "      <td>PC</td>\n",
       "      <td>CABELLO</td>\n",
       "      <td>Shampoo Bebe</td>\n",
       "      <td>NIVEA</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1.82373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2945815</th>\n",
       "      <td>201912</td>\n",
       "      <td>10006</td>\n",
       "      <td>20853</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.02898</td>\n",
       "      <td>0.02898</td>\n",
       "      <td>PC</td>\n",
       "      <td>CABELLO</td>\n",
       "      <td>Shampoo Bebe</td>\n",
       "      <td>NIVEA</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1.82373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2945816</th>\n",
       "      <td>201912</td>\n",
       "      <td>10018</td>\n",
       "      <td>20853</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.01561</td>\n",
       "      <td>0.01561</td>\n",
       "      <td>PC</td>\n",
       "      <td>CABELLO</td>\n",
       "      <td>Shampoo Bebe</td>\n",
       "      <td>NIVEA</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1.82373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2945817</th>\n",
       "      <td>201912</td>\n",
       "      <td>10020</td>\n",
       "      <td>20853</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.01561</td>\n",
       "      <td>0.01561</td>\n",
       "      <td>PC</td>\n",
       "      <td>CABELLO</td>\n",
       "      <td>Shampoo Bebe</td>\n",
       "      <td>NIVEA</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1.82373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2945818 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         periodo  customer_id  product_id  plan_precios_cuidados  \\\n",
       "0         201701        10234       20524                      0   \n",
       "1         201701        10032       20524                      0   \n",
       "2         201701        10217       20524                      0   \n",
       "3         201701        10125       20524                      0   \n",
       "4         201701        10012       20524                      0   \n",
       "...          ...          ...         ...                    ...   \n",
       "2945813   201912        10105       20853                      0   \n",
       "2945814   201912        10092       20853                      0   \n",
       "2945815   201912        10006       20853                      0   \n",
       "2945816   201912        10018       20853                      0   \n",
       "2945817   201912        10020       20853                      0   \n",
       "\n",
       "         cust_request_qty  cust_request_tn       tn cat1     cat2  \\\n",
       "0                       2          0.05300  0.05300   HC  VAJILLA   \n",
       "1                       1          0.13628  0.13628   HC  VAJILLA   \n",
       "2                       1          0.03028  0.03028   HC  VAJILLA   \n",
       "3                       1          0.02271  0.02271   HC  VAJILLA   \n",
       "4                      11          1.54452  1.54452   HC  VAJILLA   \n",
       "...                   ...              ...      ...  ...      ...   \n",
       "2945813                 1          0.02230  0.02230   PC  CABELLO   \n",
       "2945814                 1          0.00669  0.00669   PC  CABELLO   \n",
       "2945815                 7          0.02898  0.02898   PC  CABELLO   \n",
       "2945816                 4          0.01561  0.01561   PC  CABELLO   \n",
       "2945817                 2          0.01561  0.01561   PC  CABELLO   \n",
       "\n",
       "                 cat3      brand  sku_size  stock_final  \n",
       "0          Cristalino  Importado     500.0          NaN  \n",
       "1          Cristalino  Importado     500.0          NaN  \n",
       "2          Cristalino  Importado     500.0          NaN  \n",
       "3          Cristalino  Importado     500.0          NaN  \n",
       "4          Cristalino  Importado     500.0          NaN  \n",
       "...               ...        ...       ...          ...  \n",
       "2945813  Shampoo Bebe      NIVEA     200.0      1.82373  \n",
       "2945814  Shampoo Bebe      NIVEA     200.0      1.82373  \n",
       "2945815  Shampoo Bebe      NIVEA     200.0      1.82373  \n",
       "2945816  Shampoo Bebe      NIVEA     200.0      1.82373  \n",
       "2945817  Shampoo Bebe      NIVEA     200.0      1.82373  \n",
       "\n",
       "[2945818 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../data/preprocessed/base.csv\", sep=',')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e27089",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/preprocessed/base.csv\", sep=',')\n",
    "df = df.groupby(by=['periodo', 'product_id']).agg({'tn':'sum'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "924ecf08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>periodo</th>\n",
       "      <th>product_id</th>\n",
       "      <th>tn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201701</td>\n",
       "      <td>20001</td>\n",
       "      <td>934.77222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201701</td>\n",
       "      <td>20002</td>\n",
       "      <td>550.15707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201701</td>\n",
       "      <td>20003</td>\n",
       "      <td>1063.45835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201701</td>\n",
       "      <td>20004</td>\n",
       "      <td>555.91614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201701</td>\n",
       "      <td>20005</td>\n",
       "      <td>494.27011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31238</th>\n",
       "      <td>201912</td>\n",
       "      <td>21265</td>\n",
       "      <td>0.05007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31239</th>\n",
       "      <td>201912</td>\n",
       "      <td>21266</td>\n",
       "      <td>0.05121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31240</th>\n",
       "      <td>201912</td>\n",
       "      <td>21267</td>\n",
       "      <td>0.01569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31241</th>\n",
       "      <td>201912</td>\n",
       "      <td>21271</td>\n",
       "      <td>0.00298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31242</th>\n",
       "      <td>201912</td>\n",
       "      <td>21276</td>\n",
       "      <td>0.00892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31243 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       periodo  product_id          tn\n",
       "0       201701       20001   934.77222\n",
       "1       201701       20002   550.15707\n",
       "2       201701       20003  1063.45835\n",
       "3       201701       20004   555.91614\n",
       "4       201701       20005   494.27011\n",
       "...        ...         ...         ...\n",
       "31238   201912       21265     0.05007\n",
       "31239   201912       21266     0.05121\n",
       "31240   201912       21267     0.01569\n",
       "31241   201912       21271     0.00298\n",
       "31242   201912       21276     0.00892\n",
       "\n",
       "[31243 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabcec45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>tn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20001</td>\n",
       "      <td>50340.39558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20002</td>\n",
       "      <td>36337.25439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20003</td>\n",
       "      <td>32004.15274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20004</td>\n",
       "      <td>24178.15379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20005</td>\n",
       "      <td>23191.21852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>21295</td>\n",
       "      <td>0.00699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>21296</td>\n",
       "      <td>0.00651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230</th>\n",
       "      <td>21297</td>\n",
       "      <td>0.00579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>21298</td>\n",
       "      <td>0.00573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>21299</td>\n",
       "      <td>0.00546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1233 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      product_id           tn\n",
       "0          20001  50340.39558\n",
       "1          20002  36337.25439\n",
       "2          20003  32004.15274\n",
       "3          20004  24178.15379\n",
       "4          20005  23191.21852\n",
       "...          ...          ...\n",
       "1228       21295      0.00699\n",
       "1229       21296      0.00651\n",
       "1230       21297      0.00579\n",
       "1231       21298      0.00573\n",
       "1232       21299      0.00546\n",
       "\n",
       "[1233 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prod = df.groupby(by=['product_id']).agg({'tn':'sum'}).reset_index()\n",
    "df_prod.sort_values(by=['tn'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9814c828",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_prod[df_prod['tn']> 1000]\n",
    "df_1.to_csv(\"../../data/preprocessed/mayores_a_1000.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af9c10b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_prod[(df_prod['tn'] >= 500) & (df_prod['tn'] < 1000)]\n",
    "df_1.to_csv(\"../../data/preprocessed/entre_500_y_1000.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fa10716",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_prod[(df_prod['tn'] >= 100) & (df_prod['tn'] < 500)]\n",
    "df_1.to_csv(\"../../data/preprocessed/entre_100_y_500.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb000fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_prod[(df_prod['tn'] >= 1) & (df_prod['tn'] < 100)]\n",
    "df_1.to_csv(\"../../data/preprocessed/entre_1_y_100.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab322be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = df_prod[(df_prod['tn'] < 1) ]\n",
    "df_1.to_csv(\"../../data/preprocessed/menor_a_1.csv\", sep=\",\", index=False)\n",
    "df_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "706b278f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>tn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>21234</td>\n",
       "      <td>0.86192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>21237</td>\n",
       "      <td>0.64274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>21239</td>\n",
       "      <td>0.69976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>21242</td>\n",
       "      <td>0.78131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>21247</td>\n",
       "      <td>0.55726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>21265</td>\n",
       "      <td>0.89541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>21266</td>\n",
       "      <td>0.94659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>21267</td>\n",
       "      <td>0.92835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>21268</td>\n",
       "      <td>0.96121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203</th>\n",
       "      <td>21269</td>\n",
       "      <td>0.87024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>21270</td>\n",
       "      <td>0.81278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>21271</td>\n",
       "      <td>0.72803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>21272</td>\n",
       "      <td>0.72362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>21273</td>\n",
       "      <td>0.62966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>21274</td>\n",
       "      <td>0.53622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>21275</td>\n",
       "      <td>0.53072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>21276</td>\n",
       "      <td>0.45447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>21277</td>\n",
       "      <td>0.50757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>21278</td>\n",
       "      <td>0.32583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213</th>\n",
       "      <td>21279</td>\n",
       "      <td>0.26830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214</th>\n",
       "      <td>21281</td>\n",
       "      <td>0.14742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215</th>\n",
       "      <td>21282</td>\n",
       "      <td>0.11731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>21283</td>\n",
       "      <td>0.11212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217</th>\n",
       "      <td>21284</td>\n",
       "      <td>0.10982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218</th>\n",
       "      <td>21285</td>\n",
       "      <td>0.05390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219</th>\n",
       "      <td>21286</td>\n",
       "      <td>0.04914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>21287</td>\n",
       "      <td>0.03320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1221</th>\n",
       "      <td>21288</td>\n",
       "      <td>0.02638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1222</th>\n",
       "      <td>21289</td>\n",
       "      <td>0.01638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1223</th>\n",
       "      <td>21290</td>\n",
       "      <td>0.01174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1224</th>\n",
       "      <td>21291</td>\n",
       "      <td>0.01092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225</th>\n",
       "      <td>21292</td>\n",
       "      <td>0.00983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>21293</td>\n",
       "      <td>0.00917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227</th>\n",
       "      <td>21294</td>\n",
       "      <td>0.00764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>21295</td>\n",
       "      <td>0.00699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>21296</td>\n",
       "      <td>0.00651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230</th>\n",
       "      <td>21297</td>\n",
       "      <td>0.00579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>21298</td>\n",
       "      <td>0.00573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>21299</td>\n",
       "      <td>0.00546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      product_id       tn\n",
       "1175       21234  0.86192\n",
       "1176       21237  0.64274\n",
       "1178       21239  0.69976\n",
       "1181       21242  0.78131\n",
       "1185       21247  0.55726\n",
       "1199       21265  0.89541\n",
       "1200       21266  0.94659\n",
       "1201       21267  0.92835\n",
       "1202       21268  0.96121\n",
       "1203       21269  0.87024\n",
       "1204       21270  0.81278\n",
       "1205       21271  0.72803\n",
       "1206       21272  0.72362\n",
       "1207       21273  0.62966\n",
       "1208       21274  0.53622\n",
       "1209       21275  0.53072\n",
       "1210       21276  0.45447\n",
       "1211       21277  0.50757\n",
       "1212       21278  0.32583\n",
       "1213       21279  0.26830\n",
       "1214       21281  0.14742\n",
       "1215       21282  0.11731\n",
       "1216       21283  0.11212\n",
       "1217       21284  0.10982\n",
       "1218       21285  0.05390\n",
       "1219       21286  0.04914\n",
       "1220       21287  0.03320\n",
       "1221       21288  0.02638\n",
       "1222       21289  0.01638\n",
       "1223       21290  0.01174\n",
       "1224       21291  0.01092\n",
       "1225       21292  0.00983\n",
       "1226       21293  0.00917\n",
       "1227       21294  0.00764\n",
       "1228       21295  0.00699\n",
       "1229       21296  0.00651\n",
       "1230       21297  0.00579\n",
       "1231       21298  0.00573\n",
       "1232       21299  0.00546"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prod[(df_prod['tn'] < 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4363239d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Supongamos que tu DataFrame se llama df\n",
    "# Ordenar por tn descendente\n",
    "df_sorted = df_prod.sort_values('tn', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Tomar los 30 principales productos\n",
    "top_n = 30\n",
    "top_products = df_sorted.head(top_n)\n",
    "\n",
    "# Graficar\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(top_products['product_id'].astype(str), top_products['tn'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(f'Top {top_n} productos por toneladas vendidas')\n",
    "plt.xlabel('product_id')\n",
    "plt.ylabel('tn')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e5f7e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Supongamos que tu DataFrame se llama df\n",
    "# Ordenar por tn descendente\n",
    "df_sorted = df_prod.sort_values('tn', ascending=True).reset_index(drop=True)\n",
    "\n",
    "# Tomar los 30 principales productos\n",
    "top_n = 30\n",
    "top_products = df_sorted.head(top_n)\n",
    "\n",
    "# Graficar\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(top_products['product_id'].astype(str), top_products['tn'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(f'Top {top_n} productos por toneladas vendidas')\n",
    "plt.xlabel('product_id')\n",
    "plt.ylabel('tn')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d09f0f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4489f486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de periodos con Nan debido a la combinatoria periodo_x_producto: 276\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>periodo</th>\n",
       "      <th>nacimiento_producto</th>\n",
       "      <th>muerte_producto</th>\n",
       "      <th>mes_n</th>\n",
       "      <th>total_meses</th>\n",
       "      <th>producto_nuevo</th>\n",
       "      <th>ciclo_de_vida_inicial</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>brand</th>\n",
       "      <th>sku_size</th>\n",
       "      <th>stock_final</th>\n",
       "      <th>tn</th>\n",
       "      <th>plan_precios_cuidados</th>\n",
       "      <th>cust_request_qty</th>\n",
       "      <th>cust_request_tn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20001</td>\n",
       "      <td>201701</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>934.77222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>479.0</td>\n",
       "      <td>937.72717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20001</td>\n",
       "      <td>201702</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>798.01620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>833.72187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20001</td>\n",
       "      <td>201703</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1303.35771</td>\n",
       "      <td>0.0</td>\n",
       "      <td>509.0</td>\n",
       "      <td>1330.74697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20001</td>\n",
       "      <td>201704</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1069.96130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>1132.94430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20001</td>\n",
       "      <td>201705</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>5</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HC</td>\n",
       "      <td>ROPA LAVADO</td>\n",
       "      <td>Liquido</td>\n",
       "      <td>ARIEL</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1502.20132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>701.0</td>\n",
       "      <td>1550.68936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31357</th>\n",
       "      <td>21281</td>\n",
       "      <td>201704</td>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31358</th>\n",
       "      <td>21281</td>\n",
       "      <td>201705</td>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31359</th>\n",
       "      <td>21281</td>\n",
       "      <td>201706</td>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.09134</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.10539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31360</th>\n",
       "      <td>21281</td>\n",
       "      <td>201707</td>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31361</th>\n",
       "      <td>21281</td>\n",
       "      <td>201708</td>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.03500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.03500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31362 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       product_id  periodo nacimiento_producto muerte_producto  mes_n  \\\n",
       "0           20001   201701          2017-01-01      2019-12-01      1   \n",
       "1           20001   201702          2017-01-01      2019-12-01      2   \n",
       "2           20001   201703          2017-01-01      2019-12-01      3   \n",
       "3           20001   201704          2017-01-01      2019-12-01      4   \n",
       "4           20001   201705          2017-01-01      2019-12-01      5   \n",
       "...           ...      ...                 ...             ...    ...   \n",
       "31357       21281   201704          2017-02-01      2017-08-01      3   \n",
       "31358       21281   201705          2017-02-01      2017-08-01      4   \n",
       "31359       21281   201706          2017-02-01      2017-08-01      5   \n",
       "31360       21281   201707          2017-02-01      2017-08-01      6   \n",
       "31361       21281   201708          2017-02-01      2017-08-01      7   \n",
       "\n",
       "       total_meses  producto_nuevo  ciclo_de_vida_inicial cat1         cat2  \\\n",
       "0               36               0                      0   HC  ROPA LAVADO   \n",
       "1               36               0                      0   HC  ROPA LAVADO   \n",
       "2               36               0                      0   HC  ROPA LAVADO   \n",
       "3               36               0                      0   HC  ROPA LAVADO   \n",
       "4               36               0                      0   HC  ROPA LAVADO   \n",
       "...            ...             ...                    ...  ...          ...   \n",
       "31357            7               1                      1  NaN          NaN   \n",
       "31358            7               1                      0  NaN          NaN   \n",
       "31359            7               1                      0  NaN          NaN   \n",
       "31360            7               1                      0  NaN          NaN   \n",
       "31361            7               1                      0  NaN          NaN   \n",
       "\n",
       "          cat3  brand  sku_size  stock_final          tn  \\\n",
       "0      Liquido  ARIEL    3000.0          NaN   934.77222   \n",
       "1      Liquido  ARIEL    3000.0          NaN   798.01620   \n",
       "2      Liquido  ARIEL    3000.0          NaN  1303.35771   \n",
       "3      Liquido  ARIEL    3000.0          NaN  1069.96130   \n",
       "4      Liquido  ARIEL    3000.0          NaN  1502.20132   \n",
       "...        ...    ...       ...          ...         ...   \n",
       "31357      NaN    NaN       NaN          NaN     0.00000   \n",
       "31358      NaN    NaN       NaN          NaN     0.00000   \n",
       "31359      NaN    NaN       NaN          NaN     0.09134   \n",
       "31360      NaN    NaN       NaN          NaN     0.00000   \n",
       "31361      NaN    NaN       NaN          NaN     0.03500   \n",
       "\n",
       "       plan_precios_cuidados  cust_request_qty  cust_request_tn  \n",
       "0                        0.0             479.0        937.72717  \n",
       "1                        0.0             432.0        833.72187  \n",
       "2                        0.0             509.0       1330.74697  \n",
       "3                        0.0             279.0       1132.94430  \n",
       "4                        0.0             701.0       1550.68936  \n",
       "...                      ...               ...              ...  \n",
       "31357                    NaN               NaN              NaN  \n",
       "31358                    NaN               NaN              NaN  \n",
       "31359                    0.0               8.0          0.10539  \n",
       "31360                    NaN               NaN              NaN  \n",
       "31361                    0.0               1.0          0.03500  \n",
       "\n",
       "[31362 rows x 18 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def combinatoria_periodo_producto():\n",
    "    \"\"\"\n",
    "    Devuelve df con combinatoria de todos los productos con todos los periodos.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(\"../../data/preprocessed/base.csv\", sep=',')\n",
    "    \n",
    "    df[\"periodo_dt\"] = pd.to_datetime(df[\"periodo\"].astype(str), format=\"%Y%m\")\n",
    "\n",
    "    periodos = pd.date_range(start=df['periodo_dt'].min(), end=df['periodo_dt'].max(), freq=\"MS\")\n",
    "    productos = df['product_id'].unique()    \n",
    "    \n",
    "    idx = pd.MultiIndex.from_product([productos, periodos], names=['product_id', 'periodo'])\n",
    "    completo = idx.to_frame(index=False)\n",
    "        \n",
    "    del periodos, productos, idx\n",
    "    gc.collect()\n",
    "    \n",
    "    return completo\n",
    "\n",
    "def getProductos_sinHistoria(meses = 3):\n",
    "    \"\"\"\n",
    "    Devuelve df con productos cuya historia es <= 3 meses.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\"../../data/preprocessed/base.csv\", sep=',')\n",
    "    historia = df.groupby('product_id')['periodo'].agg(['min', 'max']).reset_index()\n",
    "    historia['fecha_min'] = pd.to_datetime(historia['min'], format='%Y%m')\n",
    "    historia['fecha_max'] = pd.to_datetime(historia['max'], format='%Y%m')\n",
    "    historia['meses_diff'] = (\n",
    "        (historia['fecha_max'].dt.year - historia['fecha_min'].dt.year) * 12 + \n",
    "        (historia['fecha_max'].dt.month - historia['fecha_min'].dt.month) + 1  # +1 para incluir ambos extremos\n",
    "    )\n",
    "    prod_sin_historia = historia.loc[historia['meses_diff'] <= meses, 'product_id'].tolist()\n",
    "    del historia, df\n",
    "    gc.collect()\n",
    "    return prod_sin_historia\n",
    "\n",
    "\n",
    "def eliminarProductos_sinNacer(df, data):\n",
    "    \"\"\"\n",
    "    Elimina productos que no tienen periodo de nacimiento: primera venta.\n",
    "    \"\"\"\n",
    "    df[\"periodo_dt\"] = pd.to_datetime(df[\"periodo\"].astype(str), format=\"%Y%m\")\n",
    "    data[\"periodo_dt\"] = pd.to_datetime(data[\"periodo\"].astype(str), format=\"%Y%m\")\n",
    "    \n",
    "    nacimiento_producto = df.groupby(\"product_id\")[\"periodo_dt\"].agg([\"min\"]).reset_index()\n",
    "    # Renombrar columna max a muerte_cliente_dt\n",
    "    nacimiento_producto = nacimiento_producto.rename(columns={'min': 'nacimiento_producto'})\n",
    "\n",
    "    # Unir con df_final para traer fecha de muerte del cliente\n",
    "    data = data.merge(nacimiento_producto, on='product_id', how='left')\n",
    "\n",
    "    # Filtrar filas donde periodo_dt > muerte_cliente_dt\n",
    "    data = data[data['periodo_dt'] >= data['nacimiento_producto']]\n",
    "\n",
    "    data.drop(columns=['periodo_dt'], inplace=True)\n",
    "    del nacimiento_producto\n",
    "    gc.collect()\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def eliminarProductosMuertos(df, data):\n",
    "    \"\"\"\n",
    "    Elimina productos que murieron: última venta.\n",
    "    \"\"\"\n",
    "    df[\"periodo_dt\"] = pd.to_datetime(df[\"periodo\"].astype(str), format=\"%Y%m\")\n",
    "    data[\"periodo_dt\"] = pd.to_datetime(data[\"periodo\"].astype(str), format=\"%Y%m\")\n",
    "    \n",
    "    muerte_producto = df.groupby(\"product_id\")[\"periodo_dt\"].agg([\"max\"]).reset_index()\n",
    "    # Renombrar columna max a muerte_producto\n",
    "    muerte_producto = muerte_producto.rename(columns={'max': 'muerte_producto'})\n",
    "    \n",
    "\n",
    "    # Unir con df_final para traer fecha de muerte del cliente\n",
    "    data = data.merge(muerte_producto, on='product_id', how='left')\n",
    "\n",
    "    # Filtrar filas donde periodo_dt > muerte_cliente_dt\n",
    "    data = data[~((data['periodo_dt'] > data['muerte_producto']) & (data['muerte_producto'] < '2019-12-01'))]\n",
    "\n",
    "    data.drop(columns=['periodo_dt'], inplace=True)\n",
    "    del muerte_producto\n",
    "    gc.collect()\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def marcarProductosNuevos_3M(data):\n",
    "    \"\"\"\n",
    "    Productos Nuevos = aquellos que tienen menos de 12 meses de historia.\n",
    "    Sus primeros 3 meses de historia no se tienen en cuenta para predecir.\n",
    "    En este metodo se tomó la decisión de no eliminarlos, sino marcarlos.\n",
    "    \"\"\"\n",
    "    data[\"periodo_dt\"] = pd.to_datetime(data[\"periodo\"].astype(str), format=\"%Y%m\")\n",
    "    \n",
    "    data = data.sort_values(by=['product_id', 'periodo_dt'])\n",
    "    \n",
    "    data['mes_n'] = data.groupby('product_id').cumcount() + 1\n",
    "    \n",
    "    meses_totales = data.groupby(\"product_id\")['periodo_dt'].count().rename('total_meses').reset_index()\n",
    "    \n",
    "    data = data.merge(meses_totales, on='product_id')\n",
    "    \n",
    "    data['producto_nuevo'] = (data['total_meses'] <= 12).astype(int)\n",
    "    \n",
    "    \n",
    "    data['ciclo_de_vida_inicial'] = ((data['mes_n'] <= 3) & (data['producto_nuevo'] == 1)).astype(int)\n",
    "    \n",
    "    data.drop(columns=['periodo_dt'], inplace=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "df = pd.read_csv(\"../../data/preprocessed/base.csv\", sep=',')\n",
    "data = combinatoria_periodo_producto()\n",
    "data['periodo'] = data['periodo'].dt.year * 100 + data['periodo'].dt.month\n",
    "prod_sin_historia = getProductos_sinHistoria()\n",
    "data = data[~data['product_id'].isin(prod_sin_historia)]\n",
    "#### PRODUCTOS QUE NO NACIERON ####\n",
    "data = eliminarProductos_sinNacer(df, data)\n",
    "#### PRODUCTOS QUE MURIERON ####\n",
    "data = eliminarProductosMuertos(df, data)\n",
    "#### 3MESES PRODUCTOS NUEVOS: HISTORIA MENOR A 12 MESES ####\n",
    "data = marcarProductosNuevos_3M(data)\n",
    "#### MERGE CON PRODUCTOS ####\n",
    "productos = pd.read_csv(\"../../data/raw/tb_productos.csv\", sep='\\t')\n",
    "productos = productos.drop_duplicates(subset=['product_id'], keep='first')\n",
    "data = data.merge(productos, how='left', on=\"product_id\")\n",
    "del productos\n",
    "\n",
    "#### MERGE CON STOCKS ####\n",
    "stocks = pd.read_csv(\"../../data/raw/tb_stocks.csv\", sep='\\t')\n",
    "stocks = stocks.groupby(by=[\"periodo\", \"product_id\"]).agg({\"stock_final\": \"sum\"}).reset_index()\n",
    "data = data.merge(stocks, how='left', on=['periodo', 'product_id'])\n",
    "del stocks\n",
    "\n",
    "#### MERGE CON SELLIN ####\n",
    "sellin = pd.read_csv(\"../../data/raw/sell-in.csv\", sep='\\t')\n",
    "sellin = sellin.groupby(by=[\"periodo\",\"product_id\"]).agg({\"tn\":\"sum\", \"plan_precios_cuidados\":\"sum\", \"cust_request_qty\":\"sum\", \"cust_request_tn\":\"sum\"}).reset_index()\n",
    "data = data.merge(sellin, how='left', on=['periodo', 'product_id'])\n",
    "del sellin\n",
    "gc.collect()\n",
    "#### COMPLETO TN CON CEROS ####\n",
    "####  ¿cuantos?\n",
    "print(f\"Total de periodos con Nan debido a la combinatoria periodo_x_producto: {data['tn'].isna().sum()}\")\n",
    "#### Lo completo con ceros\n",
    "data['tn'] = data['tn'].fillna(0)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d80c6974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(453, 2)\n",
      "(8669, 18)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# df = pd.read_csv(\"../../data/preprocessed/base.csv\", sep=',')\n",
    "# df = df.groupby(by=['periodo', 'product_id']).agg({'tn':'sum'}).reset_index()\n",
    "# print(df.shape)\n",
    "products = pd.read_csv(\"../../data/preprocessed/entre_1_y_100.csv\", sep=\",\")\n",
    "print(products.shape)\n",
    "data = data[data['product_id'].isin(products['product_id'].unique())]\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "939f4234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_tn_mas_dos(df):\n",
    "    \"\"\"\n",
    "    Crea una nueva columna target que contiene las ventas en el mes+2.\n",
    "    Si estoy en enero de 2017 entonces target tendrá las toneladas de marzo de 2017\n",
    "    \"\"\"\n",
    "    # Asegurarte de tener 'periodo_dt' (datetime) en completo\n",
    "    df['periodo_dt'] = pd.to_datetime(df['periodo'], format='%Y%m')\n",
    "\n",
    "    # Crear DataFrame auxiliar con tn como target y fecha adelantada\n",
    "    ventas_futuras = df[['periodo_dt', 'product_id', 'tn']].copy()\n",
    "    ventas_futuras['periodo_target_dt'] = ventas_futuras['periodo_dt'] - pd.DateOffset(months=2)\n",
    "    ventas_futuras = ventas_futuras.rename(columns={'tn': 'target'})\n",
    "\n",
    "    # Merge con completo usando periodo adelantado\n",
    "    df = df.merge(\n",
    "        ventas_futuras[['periodo_target_dt', 'product_id', 'target']],\n",
    "        how='left',\n",
    "        left_on=['periodo_dt', 'product_id'],\n",
    "        right_on=['periodo_target_dt', 'product_id']\n",
    "    )\n",
    "\n",
    "    # Eliminar columna auxiliar\n",
    "    df = df.drop(columns=['periodo_target_dt', 'periodo_dt'])\n",
    "    del ventas_futuras\n",
    "    gc.collect()\n",
    "    \n",
    "    return df\n",
    "\n",
    "data = target_tn_mas_dos(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3892ab26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>periodo</th>\n",
       "      <th>nacimiento_producto</th>\n",
       "      <th>muerte_producto</th>\n",
       "      <th>mes_n</th>\n",
       "      <th>total_meses</th>\n",
       "      <th>producto_nuevo</th>\n",
       "      <th>ciclo_de_vida_inicial</th>\n",
       "      <th>sku_size</th>\n",
       "      <th>stock_final</th>\n",
       "      <th>...</th>\n",
       "      <th>brand_NIVEA</th>\n",
       "      <th>brand_OFF</th>\n",
       "      <th>brand_SHAMPOO1</th>\n",
       "      <th>brand_SHAMPOO2</th>\n",
       "      <th>brand_SHAMPOO3</th>\n",
       "      <th>brand_SKIN1</th>\n",
       "      <th>brand_TWININGS</th>\n",
       "      <th>brand_VICHY</th>\n",
       "      <th>brand_VIVERE</th>\n",
       "      <th>brand_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20615</td>\n",
       "      <td>201907</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>400.0</td>\n",
       "      <td>16.02735</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20615</td>\n",
       "      <td>201908</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>400.0</td>\n",
       "      <td>14.06775</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20615</td>\n",
       "      <td>201909</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>400.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20615</td>\n",
       "      <td>201910</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>3.55514</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20615</td>\n",
       "      <td>201911</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>14.32394</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8664</th>\n",
       "      <td>21264</td>\n",
       "      <td>201908</td>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>26</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.07166</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8665</th>\n",
       "      <td>21264</td>\n",
       "      <td>201909</td>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>27</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.05681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8666</th>\n",
       "      <td>21264</td>\n",
       "      <td>201910</td>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>28</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.11844</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8667</th>\n",
       "      <td>21264</td>\n",
       "      <td>201911</td>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.10544</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8668</th>\n",
       "      <td>21264</td>\n",
       "      <td>201912</td>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.09653</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8669 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      product_id  periodo nacimiento_producto muerte_producto  mes_n  \\\n",
       "0          20615   201907          2019-07-01      2019-12-01      1   \n",
       "1          20615   201908          2019-07-01      2019-12-01      2   \n",
       "2          20615   201909          2019-07-01      2019-12-01      3   \n",
       "3          20615   201910          2019-07-01      2019-12-01      4   \n",
       "4          20615   201911          2019-07-01      2019-12-01      5   \n",
       "...          ...      ...                 ...             ...    ...   \n",
       "8664       21264   201908          2017-07-01      2019-12-01     26   \n",
       "8665       21264   201909          2017-07-01      2019-12-01     27   \n",
       "8666       21264   201910          2017-07-01      2019-12-01     28   \n",
       "8667       21264   201911          2017-07-01      2019-12-01     29   \n",
       "8668       21264   201912          2017-07-01      2019-12-01     30   \n",
       "\n",
       "      total_meses  producto_nuevo  ciclo_de_vida_inicial  sku_size  \\\n",
       "0               6               1                      1     400.0   \n",
       "1               6               1                      1     400.0   \n",
       "2               6               1                      1     400.0   \n",
       "3               6               1                      0     400.0   \n",
       "4               6               1                      0     400.0   \n",
       "...           ...             ...                    ...       ...   \n",
       "8664           30               0                      0      20.0   \n",
       "8665           30               0                      0      20.0   \n",
       "8666           30               0                      0      20.0   \n",
       "8667           30               0                      0      20.0   \n",
       "8668           30               0                      0      20.0   \n",
       "\n",
       "      stock_final  ...  brand_NIVEA  brand_OFF  brand_SHAMPOO1  \\\n",
       "0        16.02735  ...          1.0        0.0             0.0   \n",
       "1        14.06775  ...          1.0        0.0             0.0   \n",
       "2         0.00000  ...          1.0        0.0             0.0   \n",
       "3         3.55514  ...          1.0        0.0             0.0   \n",
       "4        14.32394  ...          1.0        0.0             0.0   \n",
       "...           ...  ...          ...        ...             ...   \n",
       "8664      0.07166  ...          0.0        0.0             0.0   \n",
       "8665      0.05681  ...          0.0        0.0             0.0   \n",
       "8666      0.11844  ...          0.0        0.0             0.0   \n",
       "8667      0.10544  ...          0.0        0.0             0.0   \n",
       "8668      0.09653  ...          0.0        0.0             0.0   \n",
       "\n",
       "      brand_SHAMPOO2  brand_SHAMPOO3  brand_SKIN1  brand_TWININGS  \\\n",
       "0                0.0             0.0          0.0             0.0   \n",
       "1                0.0             0.0          0.0             0.0   \n",
       "2                0.0             0.0          0.0             0.0   \n",
       "3                0.0             0.0          0.0             0.0   \n",
       "4                0.0             0.0          0.0             0.0   \n",
       "...              ...             ...          ...             ...   \n",
       "8664             0.0             0.0          0.0             1.0   \n",
       "8665             0.0             0.0          0.0             1.0   \n",
       "8666             0.0             0.0          0.0             1.0   \n",
       "8667             0.0             0.0          0.0             1.0   \n",
       "8668             0.0             0.0          0.0             1.0   \n",
       "\n",
       "      brand_VICHY  brand_VIVERE  brand_nan  \n",
       "0             0.0           0.0        0.0  \n",
       "1             0.0           0.0        0.0  \n",
       "2             0.0           0.0        0.0  \n",
       "3             0.0           0.0        0.0  \n",
       "4             0.0           0.0        0.0  \n",
       "...           ...           ...        ...  \n",
       "8664          0.0           0.0        0.0  \n",
       "8665          0.0           0.0        0.0  \n",
       "8666          0.0           0.0        0.0  \n",
       "8667          0.0           0.0        0.0  \n",
       "8668          0.0           0.0        0.0  \n",
       "\n",
       "[8669 rows x 128 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import gc\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\n",
    "\n",
    "\n",
    "def aplicarOHE(df):\n",
    "    \"\"\"\n",
    "    Aplica OneHotEncoder a las columnas categóricas.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Seleccionar columnas categóricas\n",
    "    categoricas = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    # Crear codificador\n",
    "    encoder = OneHotEncoder(sparse_output=False, drop=None)\n",
    "\n",
    "    # Ajustar y transformar\n",
    "    encoded = encoder.fit_transform(df[categoricas])\n",
    "\n",
    "    # Convertir a DataFrame\n",
    "    encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(categoricas), index=df.index)\n",
    "\n",
    "    # Unir con el resto del DataFrame\n",
    "    df_ohe = df.drop(columns=categoricas).join(encoded_df)\n",
    "    \n",
    "    # Liberar memoria\n",
    "    del categoricas, encoded, encoded_df, encoder\n",
    "    gc.collect()\n",
    "    \n",
    "    return df_ohe\n",
    "\n",
    "##### aplicamos OHE\n",
    "data = aplicarOHE(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "27693880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from calendar import monthrange\n",
    "import gc\n",
    "from prophet import Prophet\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import skew\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "import calendar\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from neuralprophet import NeuralProphet\n",
    "from tqdm import tqdm\n",
    "from tslearn.metrics import cdist_dtw, dtw\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def calcular_cantidad_lags(desde, hasta):\n",
    "    \"\"\"\n",
    "    Calcula la cantidad de lags necesarios entre dos fechas.\n",
    "    \"\"\"\n",
    "    # Convertir fechas a formato datetime\n",
    "    fdesde = pd.to_datetime(str(desde), format='%Y%m')\n",
    "    fhasta = pd.to_datetime(str(hasta), format='%Y%m')\n",
    "    \n",
    "    # Calcular diferencia en meses\n",
    "    lags = (fhasta.year - fdesde.year) * 12 + (fhasta.month - fdesde.month)\n",
    "    return lags + 1  # +1 para incluir el mes actual\n",
    "\n",
    "\n",
    "\n",
    "def get_lags(df, col, hasta=201912):\n",
    "    \"\"\"\n",
    "    Calcula los lags de la columna indicada por 'col' hasta la fecha indicada.\n",
    "    \"\"\"\n",
    "    df = df.sort_values(['product_id', 'periodo'])\n",
    "\n",
    "    lags = calcular_cantidad_lags(201701, hasta)\n",
    "    \n",
    "    # Calcular lags\n",
    "    for lag in range(1, lags):\n",
    "        df[f'{col}_lag_{lag}'] = df.groupby('product_id')[col].shift(lag)\n",
    "    \n",
    "    # Liberar memoria\n",
    "    gc.collect()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_lagsEspecificos(df, lags=[1, 2, 3, 6, 12, 24], col='tn'):\n",
    "    \"\"\"\n",
    "    Calcula lags específicos de la columna indicada por 'col' hasta la fecha indicada.\n",
    "    \"\"\"\n",
    "    df = df.sort_values(['product_id', 'periodo'])\n",
    "    \n",
    "    # Calcular lags específicos\n",
    "    for lag in lags:\n",
    "        df[f'{col}_lag_{lag}'] = df.groupby('product_id')[col].shift(lag)\n",
    "    \n",
    "    # Liberar memoria\n",
    "    gc.collect()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_delta_lags(df, col, hasta=201912):\n",
    "    \"\"\"\n",
    "    Calcula los delta lags (diferencias entre lags consecutivos) para la columna 'col'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ordenar\n",
    "    df = df.sort_values(['product_id', 'periodo'])\n",
    "    \n",
    "    \n",
    "    lags = calcular_cantidad_lags(201701, hasta)\n",
    "\n",
    "    # Calcular delta lags\n",
    "    for i in range(1, lags):\n",
    "        lag_actual = i\n",
    "        lag_anterior = i - 1\n",
    "        if (lag_anterior == 0): continue\n",
    "        df[f'{col}_delta_lag_{lag_actual}'] = df[f'{col}_lag_{lag_actual}'] - df[f'{col}_lag_{lag_anterior}']\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_delta_lags_especificos(df, lags=[1, 2, 3, 6, 12, 24], col='tn'):\n",
    "    \"\"\"\n",
    "    Calcula los delta lags (diferencias entre lags específicos) para la columna 'col'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ordenar\n",
    "    df = df.sort_values(['product_id', 'periodo'])\n",
    "    \n",
    "    # Calcular delta lags específicos\n",
    "    for i in range(1, len(lags)):\n",
    "        lag_actual = lags[i]\n",
    "        lag_anterior = lags[i - 1]\n",
    "        if (lag_anterior == 0): continue\n",
    "        df[f'{col}_delta_lag_{lag_actual}'] = df[f'{col}_lag_{lag_actual}'] - df[f'{col}_lag_{lag_anterior}']\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_rolling_means(df, col, hasta=201912):\n",
    "    \"\"\"\n",
    "    Calcula medias móviles mensuales de la columna 'col' para las ventanas especificadas,\n",
    "    usando solo datos hasta el período 'hasta'.\n",
    "    \"\"\"\n",
    "    # Filtrar y ordenar\n",
    "    df_historico = df[df['periodo'].astype(int) <= hasta].copy()\n",
    "    df_historico = df_historico.sort_values(['product_id', 'periodo'])\n",
    "    \n",
    "    ventanas = calcular_cantidad_lags(201701, hasta)\n",
    "    ventanas = ventanas + 1\n",
    "    \n",
    "    # Calcular medias móviles para cada ventana\n",
    "    for ventana in range(1, ventanas):\n",
    "        df_historico[f'{col}_rolling_mean_{ventana}'] = (\n",
    "            df_historico.groupby('product_id')[col]\n",
    "            .rolling(window=ventana, min_periods=ventana)\n",
    "            .mean()\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "    \n",
    "    # Combinar con el DataFrame original\n",
    "    df = df.merge(\n",
    "        df_historico[['product_id', 'periodo'] + [f'{col}_rolling_mean_{v}' for v in range(1, ventanas)]],\n",
    "        on=['product_id', 'periodo'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_rolling_means_especificos(df, lags=[1, 2, 3, 6, 12, 24], col='tn'):\n",
    "    \"\"\"\n",
    "    Calcula medias móviles mensuales de la columna 'col' para las ventanas especificadas,\n",
    "    usando solo datos hasta el período 'hasta'.\n",
    "    \"\"\"\n",
    "    # Filtrar y ordenar\n",
    "    df_historico = df.copy()\n",
    "    df_historico = df_historico.sort_values(['product_id', 'periodo'])\n",
    "    \n",
    "    # Calcular medias móviles para cada ventana específica\n",
    "    for ventana in lags:\n",
    "        df_historico[f'{col}_rolling_mean_{ventana}'] = (\n",
    "            df_historico.groupby('product_id')[col]\n",
    "            .rolling(window=ventana, min_periods=ventana)\n",
    "            .mean()\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "    \n",
    "    # Combinar con el DataFrame original\n",
    "    df = df.merge(\n",
    "        df_historico[['product_id', 'periodo'] + [f'{col}_rolling_mean_{v}' for v in lags]],\n",
    "        on=['product_id', 'periodo'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_rolling_stds(df, col, hasta=201912):\n",
    "    \"\"\"\n",
    "    Calcula desvíos estándar móviles mensuales de la columna 'col',\n",
    "    usando solo datos hasta el período 'hasta'.\n",
    "    \"\"\"\n",
    "    # Filtrar y ordenar\n",
    "    df_historico = df[df['periodo'].astype(int) <= hasta].copy()\n",
    "    df_historico = df_historico.sort_values(['product_id', 'periodo'])\n",
    "    \n",
    "    ventanas = calcular_cantidad_lags(201701, hasta)\n",
    "    ventanas = ventanas + 1\n",
    "    \n",
    "    # Calcular desvíos estándar móviles para cada ventana\n",
    "    for ventana in range(1, ventanas):\n",
    "        df_historico[f'{col}_rolling_std_{ventana}'] = (\n",
    "            df_historico.groupby('product_id')[col]\n",
    "            .rolling(window=ventana, min_periods=ventana)  # min_periods=ventana para evitar valores con datos insuficientes\n",
    "            .std(ddof=0)  # ddof=0 para desviación estándar poblacional\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "    \n",
    "    # Combinar con el DataFrame original\n",
    "    df = df.merge(\n",
    "        df_historico[['product_id', 'periodo'] + [f'{col}_rolling_std_{v}' for v in range(1, ventanas)]],\n",
    "        on=['product_id', 'periodo'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_rolling_stds_especificos(df, lags=[1, 2, 3, 6, 12, 24], col='tn'):\n",
    "    \"\"\"\n",
    "    Calcula desvíos estándar móviles mensuales de la columna 'col' para las ventanas especificadas,\n",
    "    usando solo datos hasta el período 'hasta'.\n",
    "    \"\"\"\n",
    "    # Filtrar y ordenar\n",
    "    df_historico = df.copy()\n",
    "    df_historico = df_historico.sort_values(['product_id', 'periodo'])\n",
    "    \n",
    "    # Calcular desvíos estándar móviles para cada ventana específica\n",
    "    for ventana in lags:\n",
    "        df_historico[f'{col}_rolling_std_{ventana}'] = (\n",
    "            df_historico.groupby('product_id')[col]\n",
    "            .rolling(window=ventana, min_periods=ventana)  # min_periods=ventana para evitar valores con datos insuficientes\n",
    "            .std(ddof=0)  # ddof=0 para desviación estándar poblacional\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "    \n",
    "    # Combinar con el DataFrame original\n",
    "    df = df.merge(\n",
    "        df_historico[['product_id', 'periodo'] + [f'{col}_rolling_std_{v}' for v in lags]],\n",
    "        on=['product_id', 'periodo'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_rolling_medians(df, col, hasta=201912):\n",
    "    \"\"\"\n",
    "    Calcula medianas móviles mensuales de la columna 'col' para las ventanas especificadas,\n",
    "    usando solo datos hasta el período 'hasta'.\n",
    "    \"\"\"\n",
    "    # Filtrar y ordenar\n",
    "    df_historico = df[df['periodo'].astype(int) <= hasta].copy()\n",
    "    df_historico = df_historico.sort_values(['product_id', 'periodo'])\n",
    "    \n",
    "    ventanas = calcular_cantidad_lags(201701, hasta)\n",
    "    ventanas = ventanas + 1\n",
    "    \n",
    "    # Calcular medianas móviles para cada ventana\n",
    "    for ventana in range(1, ventanas):\n",
    "        df_historico[f'{col}_rolling_median_{ventana}'] = (\n",
    "            df_historico.groupby('product_id')[col]\n",
    "            .rolling(window=ventana, min_periods=ventana)\n",
    "            .median()  # Cambio clave: usando median() en lugar de mean()\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "    \n",
    "    # Combinar con el DataFrame original\n",
    "    df = df.merge(\n",
    "        df_historico[['product_id', 'periodo'] + [f'{col}_rolling_median_{v}' for v in range(1, ventanas)]],\n",
    "        on=['product_id', 'periodo'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_rolling_medians_especificos(df, lags=[1, 2, 3, 6, 12, 24], col='tn'):\n",
    "    \"\"\"\n",
    "    Calcula medianas móviles mensuales de la columna 'col' para las ventanas especificadas,\n",
    "    usando solo datos hasta el período 'hasta'.\n",
    "    \"\"\"\n",
    "    # Filtrar y ordenar\n",
    "    df_historico = df.copy()\n",
    "    df_historico = df_historico.sort_values(['product_id', 'periodo'])\n",
    "    \n",
    "    # Calcular medianas móviles para cada ventana específica\n",
    "    for ventana in lags:\n",
    "        df_historico[f'{col}_rolling_median_{ventana}'] = (\n",
    "            df_historico.groupby('product_id')[col]\n",
    "            .rolling(window=ventana, min_periods=ventana)\n",
    "            .median()  # Cambio clave: usando median() en lugar de mean()\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "    \n",
    "    # Combinar con el DataFrame original\n",
    "    df = df.merge(\n",
    "        df_historico[['product_id', 'periodo'] + [f'{col}_rolling_median_{v}' for v in lags]],\n",
    "        on=['product_id', 'periodo'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_rolling_mins(df, col, hasta=201912):\n",
    "    \"\"\"\n",
    "    Calcula mínimos móviles mensuales de la columna 'col',\n",
    "    usando solo datos hasta el período 'hasta'.\n",
    "    \"\"\"\n",
    "    # Filtrar y ordenar\n",
    "    df_historico = df[df['periodo'].astype(int) <= hasta].copy()\n",
    "    df_historico = df_historico.sort_values(['product_id', 'periodo'])\n",
    "    \n",
    "    ventanas = calcular_cantidad_lags(201701, hasta)\n",
    "    ventanas = ventanas + 1\n",
    "    \n",
    "    # Calcular mínimos móviles para cada ventana\n",
    "    for ventana in range(1, ventanas):\n",
    "        df_historico[f'{col}_rolling_min_{ventana}'] = (\n",
    "            df_historico.groupby('product_id')[col]\n",
    "            .rolling(window=ventana, min_periods=ventana)  # min_periods=ventana para NaN con datos insuficientes\n",
    "            .min()\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "    \n",
    "    # Combinar con el DataFrame original\n",
    "    df = df.merge(\n",
    "        df_historico[['product_id', 'periodo'] + [f'{col}_rolling_min_{v}' for v in range(1, ventanas)]],\n",
    "        on=['product_id', 'periodo'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_rolling_mins_especificos(df, lags=[1, 2, 3, 6, 12, 24], col='tn'):  \n",
    "    \"\"\"\n",
    "    Calcula mínimos móviles mensuales de la columna 'col' para las ventanas especificadas,\n",
    "    usando solo datos hasta el período 'hasta'.\n",
    "    \"\"\"\n",
    "    # Filtrar y ordenar\n",
    "    df_historico = df.copy()\n",
    "    df_historico = df_historico.sort_values(['product_id', 'periodo'])\n",
    "    \n",
    "    # Calcular mínimos móviles para cada ventana específica\n",
    "    for ventana in lags:\n",
    "        df_historico[f'{col}_rolling_min_{ventana}'] = (\n",
    "            df_historico.groupby('product_id')[col]\n",
    "            .rolling(window=ventana, min_periods=ventana)  # min_periods=ventana para NaN con datos insuficientes\n",
    "            .min()\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "    \n",
    "    # Combinar con el DataFrame original\n",
    "    df = df.merge(\n",
    "        df_historico[['product_id', 'periodo'] + [f'{col}_rolling_min_{v}' for v in lags]],\n",
    "        on=['product_id', 'periodo'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_rolling_maxs(df, col, hasta=201912):\n",
    "    \"\"\"\n",
    "    Calcula máximos móviles mensuales de la columna 'col',\n",
    "    usando solo datos hasta el período 'hasta'.\n",
    "    \"\"\"\n",
    "    # Filtrar y ordenar\n",
    "    df_historico = df[df['periodo'].astype(int) <= hasta].copy()\n",
    "    df_historico = df_historico.sort_values(['product_id', 'periodo'])\n",
    "    \n",
    "    ventanas = calcular_cantidad_lags(201701, hasta)\n",
    "    ventanas = ventanas + 1\n",
    "    \n",
    "    # Calcular mínimos móviles para cada ventana\n",
    "    for ventana in range(1, ventanas):\n",
    "        df_historico[f'{col}_rolling_max_{ventana}'] = (\n",
    "            df_historico.groupby('product_id')[col]\n",
    "            .rolling(window=ventana, min_periods=ventana)  # min_periods=ventana para NaN con datos insuficientes\n",
    "            .max()\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "    \n",
    "    # Combinar con el DataFrame original\n",
    "    df = df.merge(\n",
    "        df_historico[['product_id', 'periodo'] + [f'{col}_rolling_max_{v}' for v in range(1, ventanas)]],\n",
    "        on=['product_id', 'periodo'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_rolling_maxs_especificos(df, lags=[1, 2, 3, 6, 12, 24], col='tn'):\n",
    "    \"\"\"\n",
    "    Calcula máximos móviles mensuales de la columna 'col' para las ventanas especificadas,\n",
    "    usando solo datos hasta el período 'hasta'.\n",
    "    \"\"\"\n",
    "    # Filtrar y ordenar\n",
    "    df_historico = df.copy()\n",
    "    df_historico = df_historico.sort_values(['product_id', 'periodo'])\n",
    "    \n",
    "    # Calcular máximos móviles para cada ventana específica\n",
    "    for ventana in lags:\n",
    "        df_historico[f'{col}_rolling_max_{ventana}'] = (\n",
    "            df_historico.groupby('product_id')[col]\n",
    "            .rolling(window=ventana, min_periods=ventana)  # min_periods=ventana para NaN con datos insuficientes\n",
    "            .max()\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "    \n",
    "    # Combinar con el DataFrame original\n",
    "    df = df.merge(\n",
    "        df_historico[['product_id', 'periodo'] + [f'{col}_rolling_max_{v}' for v in lags]],\n",
    "        on=['product_id', 'periodo'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_rolling_skewness(df, col, hasta=201912):\n",
    "    \"\"\"\n",
    "    Calcula el skewness móvil mensual de la columna 'col' para ventanas especificadas,\n",
    "    usando solo datos hasta el período 'hasta'.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame con columnas 'product_id', 'periodo' y 'col'.\n",
    "        col (str): Nombre de la columna a analizar.\n",
    "        hasta (int): Fecha límite en formato YYYYMM (ej: 201912 para diciembre 2019).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame original con columnas añadidas de skewness móvil.\n",
    "    \"\"\"\n",
    "    # Filtrar y ordenar datos históricos\n",
    "    df_historico = df[df['periodo'].astype(int) <= hasta].copy()\n",
    "    df_historico = df_historico.sort_values(['product_id', 'periodo'])\n",
    "    \n",
    "    # Calcular número de ventanas posibles\n",
    "    ventanas = calcular_cantidad_lags(201701, hasta) + 1\n",
    "    \n",
    "    # Calcular skewness móvil para cada ventana\n",
    "    for ventana in range(1, ventanas):\n",
    "        df_historico[f'{col}_rolling_skew_{ventana}'] = (\n",
    "            df_historico.groupby('product_id')[col]\n",
    "            .rolling(window=ventana, min_periods=ventana)\n",
    "            .apply(skew, raw=True)  # Usamos scipy.stats.skew\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "    \n",
    "    # Combinar con el DataFrame original\n",
    "    df = df.merge(\n",
    "        df_historico[['product_id', 'periodo'] + [f'{col}_rolling_skew_{v}' for v in range(1, ventanas)]],\n",
    "        on=['product_id', 'periodo'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_rolling_skewness_especificos(df, lags=[1, 2, 3, 6, 12, 24], col='tn'):\n",
    "    \"\"\"\n",
    "    Calcula el skewness móvil mensual de la columna 'col' para ventanas específicas,\n",
    "    usando solo datos hasta el período 'hasta'.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame con columnas 'product_id', 'periodo' y 'col'.\n",
    "        lags (list): Lista de ventanas específicas para calcular skewness.\n",
    "        col (str): Nombre de la columna a analizar.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame original con columnas añadidas de skewness móvil.\n",
    "    \"\"\"\n",
    "    # Filtrar y ordenar datos históricos\n",
    "    df_historico = df.copy()\n",
    "    df_historico = df_historico.sort_values(['product_id', 'periodo'])\n",
    "    \n",
    "    # Calcular skewness móvil para cada ventana específica\n",
    "    for ventana in lags:\n",
    "        df_historico[f'{col}_rolling_skew_{ventana}'] = (\n",
    "            df_historico.groupby('product_id')[col]\n",
    "            .rolling(window=ventana, min_periods=ventana)\n",
    "            .apply(skew, raw=True)  # Usamos scipy.stats.skew\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "    \n",
    "    # Combinar con el DataFrame original\n",
    "    df = df.merge(\n",
    "        df_historico[['product_id', 'periodo'] + [f'{col}_rolling_skew_{v}' for v in lags]],\n",
    "        on=['product_id', 'periodo'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_autocorrelaciones(df, col, hasta=201912):\n",
    "    \"\"\"\n",
    "    Calcula autocorrelaciones para cada combinación de producto y periodo,\n",
    "    considerando todos los lags posibles hasta la fecha límite.\n",
    "    \"\"\"\n",
    "    # Ordenar por producto y periodo\n",
    "    df = df.sort_values(['product_id', 'periodo'])\n",
    "    \n",
    "    # Lista para almacenar resultados\n",
    "    resultados = []\n",
    "    \n",
    "    for (producto, periodo), grupo in df.groupby(['product_id', 'periodo']):\n",
    "        # Filtrar datos históricos hasta el periodo actual (inclusive)\n",
    "        datos_historicos = df[\n",
    "            (df['product_id'] == producto) & \n",
    "            (df['periodo'] <= periodo)\n",
    "        ][col].dropna()\n",
    "        \n",
    "        # Calcular número máximo de lags posibles (periodos disponibles - 1)\n",
    "        max_lags_posibles = len(datos_historicos) - 1\n",
    "        max_lags_posibles = max(max_lags_posibles, 0)  # Asegurar no negativo\n",
    "        \n",
    "        # Diccionario para este registro\n",
    "        registro = {\n",
    "            'product_id': producto,\n",
    "            'periodo': periodo\n",
    "        }\n",
    "        \n",
    "        # Calcular autocorrelación para cada lag posible\n",
    "        for lag in range(1, max_lags_posibles + 1):\n",
    "            autocorr = datos_historicos.autocorr(lag=lag)\n",
    "            registro[f'autocorr_lag_{lag}'] = autocorr\n",
    "        \n",
    "        resultados.append(registro)\n",
    "    \n",
    "    # Convertir a DataFrame y rellenar NaNs para lags no disponibles\n",
    "    df_resultado = pd.DataFrame(resultados)\n",
    "    df = df.merge(df_resultado, on=['product_id', 'periodo'], how='left')\n",
    "    \n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ca6e05c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:1535: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  return window_func(values, begin, end, min_periods)\n",
      "\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:1535: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  return window_func(values, begin, end, min_periods)\n",
      "\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:2889: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar, dtype=dtype)\n",
      "\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:2748: RuntimeWarning: divide by zero encountered in divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:2748: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n",
      "\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = get_lags(data, \"tn\", 201912)\n",
    "data = get_delta_lags(data, \"tn\", 201912)\n",
    "data = get_rolling_means(data, \"tn\", 201912)\n",
    "data = get_rolling_stds(data, \"tn\", 201912)\n",
    "data = get_rolling_mins(data, \"tn\", 201912)\n",
    "data = get_rolling_maxs(data, \"tn\", 201912)\n",
    "data = get_rolling_skewness(data, \"tn\", 201912)\n",
    "data = get_autocorrelaciones(data, \"tn\", 201912)\n",
    "data = get_rolling_medians(data, \"tn\", 201912)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "89c93ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from optuna.storages import RDBStorage\n",
    "from optuna.artifacts import FileSystemArtifactStore, upload_artifact\n",
    "import os\n",
    "import json\n",
    "\n",
    "def guardar_hiperparametros(best_params, name='lgb_v1'):\n",
    "    \"\"\"\n",
    "    Guarda los mejores hiperparámetros en un archivo JSON.\n",
    "    \"\"\"\n",
    "    # Guardar best_params en un archivo JSON\n",
    "    with open(f'./{name}.json', 'w') as f:\n",
    "        json.dump(best_params, f, indent=4)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def optimizar_con_optuna_ivan(train, version=\"v1\"):\n",
    "    \"\"\"\n",
    "    Optimiza los hiperparámetros de un modelo LightGBM utilizando Optuna sin usar TimeSeriesSplit.\n",
    "    \"\"\"\n",
    "    # Asegurarse de que 'periodo' esté en formato datetime\n",
    "    train = train.sort_values(\"periodo\")  # o la columna de fecha\n",
    "\n",
    "    # Eliminar columnas no necesarias\n",
    "    datetime_cols = train.select_dtypes(include=['datetime', 'datetime64']).columns.tolist()\n",
    "    X = train.drop(columns=[*datetime_cols, 'target'])\n",
    "    y = train['target']\n",
    "\n",
    "    # División simple en entrenamiento/validación (por ejemplo 80/20)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 15, 200),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
    "            'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "            'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "            'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 1, 50),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "            'max_bin': trial.suggest_int('max_bin', 100, 500),\n",
    "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 100),\n",
    "            'extra_trees': trial.suggest_categorical('extra_trees', [True, False]),\n",
    "            'seed': 42,\n",
    "            'verbosity': -1\n",
    "        }\n",
    "\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            num_boost_round=1000,\n",
    "            valid_sets=[val_data],\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
    "                lgb.log_evaluation(0)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        y_pred = model.predict(X_val)\n",
    "        rmse = mean_squared_error(y_val, y_pred)\n",
    "        return rmse\n",
    "\n",
    "    def print_best_trial(study, trial):\n",
    "        print(f\"Mejor trial hasta ahora: RMSE={study.best_value:.4f}, Parámetros={study.best_trial.params}\")\n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(\n",
    "        objective,\n",
    "        n_trials=50,\n",
    "        callbacks=[print_best_trial],\n",
    "        timeout=3600\n",
    "    )\n",
    "\n",
    "    print(\"Mejores hiperparámetros:\", study.best_params)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_params.update({\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbosity': -1\n",
    "    })\n",
    "\n",
    "    guardar_hiperparametros(best_params, version)\n",
    "\n",
    "\n",
    "\n",
    "def levantar_hiperparametros(nombre):\n",
    "    \"\"\"\n",
    "    Levanta los hiperparámetros guardados en un archivo JSON.\n",
    "    \n",
    "    Args:\n",
    "        nombre (str): Nombre del archivo (sin extensión .json).\n",
    "    \n",
    "    Returns:\n",
    "        dict: Diccionario con los hiperparámetros. None si hay error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(f'./{nombre}.json', 'r') as f:\n",
    "            best_params = json.load(f)  # ¡Usar json.load() en lugar de json.dump()!\n",
    "        return best_params\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Archivo './best_params/{nombre}.json' no encontrado.\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: El archivo './best_params/{nombre}.json' no tiene formato JSON válido.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def optimizar_con_optuna(train, version=\"v1\"):\n",
    "    \"\"\"\n",
    "    Optimiza los hiperparámetros de un modelo LightGBM utilizando Optuna.\n",
    "    \"\"\"\n",
    "    # Asegurarse de que 'periodo' esté en formato datetime\n",
    "    train = train.sort_values(\"periodo\")  # o la columna de fecha\n",
    "\n",
    "    \n",
    "    # Eliminar columnas no necesarias\n",
    "    datetime_cols = train.select_dtypes(include=['datetime', 'datetime64']).columns.tolist()\n",
    "    # cols_to_drop = ['target', 'periodo'] + datetime_cols  # Asegúrate de incluir 'periodo'\n",
    "    X = train.drop(columns=[*datetime_cols, 'target'])\n",
    "    y = train['target']\n",
    "\n",
    "    # Sample Weights (ej: ponderar por toneladas históricas)\n",
    "    # sample_weight = train['tn_zscore'].values if 'tn_zscore' in train.columns else None\n",
    "   \n",
    "    # ---------------------------------------------------\n",
    "   \n",
    "    tscv = TimeSeriesSplit(\n",
    "        n_splits=5,\n",
    "        test_size=1,  # Validar 1 mes (el mes+2 desde el último mes de entrenamiento)\n",
    "        gap=1         # Respetar el mes intermedio (ej: entrenar hasta 201806, predecir 201808)\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 15, 200),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),  # Log-scale para LR pequeñas\n",
    "            'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
    "            'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),  # Frecuencia de bagging\n",
    "            'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),  # Log-scale para regularización\n",
    "            'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 1, 50),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 15),  # Profundidad máxima\n",
    "            'max_bin': trial.suggest_int('max_bin', 100, 500),  # Optimizar bins: 100 a 255\n",
    "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 100),\n",
    "            'extra_trees': trial.suggest_categorical('extra_trees', [True, False]),  # Alternativa a GBDT\n",
    "            'seed': 42,\n",
    "            'verbosity': -1\n",
    "        }\n",
    "        \n",
    "        rmse_scores = []\n",
    "        \n",
    "        for train_idx, val_idx in tscv.split(X):\n",
    "            X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            \n",
    "            # Sample Weights para el fold actual\n",
    "            # if sample_weight is not None:\n",
    "            #     sample_weight_fold = sample_weight[train_idx]\n",
    "            # else:\n",
    "            #     sample_weight_fold = None\n",
    "            \n",
    "            train_data = lgb.Dataset(\n",
    "                X_train_fold, \n",
    "                label=y_train_fold,\n",
    "                # weight=sample_weight_fold  # Aplicar sample_weight\n",
    "            )\n",
    "            val_data = lgb.Dataset(X_val_fold, label=y_val_fold, reference=train_data)\n",
    "            \n",
    "            model = lgb.train(\n",
    "                params,\n",
    "                train_data,\n",
    "                num_boost_round=1000,\n",
    "                valid_sets=[val_data],\n",
    "                callbacks=[\n",
    "                    lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
    "                    lgb.log_evaluation(0)\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            y_pred = model.predict(X_val_fold)\n",
    "            # rmse = mean_squared_error(y_val_fold, y_pred, squared=False)\n",
    "            rmse = mean_squared_error(y_val_fold, y_pred)\n",
    "            rmse_scores.append(rmse)\n",
    "        \n",
    "        return np.mean(rmse_scores)\n",
    "\n",
    "    \n",
    "    # ---------------------------------------------------\n",
    "    def print_best_trial(study, trial):\n",
    "        print(f\"Mejor trial hasta ahora: RMSE={study.best_value:.4f}, Parámetros={study.best_trial.params}\")\n",
    "\n",
    "    study = optuna.create_study(direction='minimize') # Minimizar RMSE\n",
    "    study.optimize(\n",
    "        objective,\n",
    "        n_trials=50,  # Aumentar trials para búsqueda exhaustiva\n",
    "        callbacks=[print_best_trial],\n",
    "        timeout=3600  # Límite de tiempo opcional (1 hora)\n",
    "    )\n",
    "\n",
    "    print(\"Mejores hiperparámetros:\", study.best_params)\n",
    "    # ---------------------------------------------------\n",
    "    best_params = study.best_params\n",
    "    best_params.update({\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbosity': -1\n",
    "    })\n",
    "\n",
    "    \n",
    "    # Dataset completo con sample_weight\n",
    "    # final_train_data = lgb.Dataset(\n",
    "    #     X, \n",
    "    #     label=y,\n",
    "    #     # weight=sample_weight  # Aplicar sample_weight global\n",
    "    # )\n",
    "\n",
    "    # Entrenar con early stopping en un pequeño holdout (opcional)\n",
    "    # final_model = lgb.train(\n",
    "    #     best_params,\n",
    "    #     final_train_data,\n",
    "    #     num_boost_round=1000,\n",
    "    #     callbacks=[lgb.log_evaluation(50)]\n",
    "    # )\n",
    "\n",
    "    # Guardar modelo\n",
    "    # final_model.save_model('modelo_final_lightgbm.txt')\n",
    "    guardar_hiperparametros(best_params, version)\n",
    "\n",
    "\n",
    "def semillerio_en_prediccion(train, test, version=\"v1\"):\n",
    "    \"\"\"\n",
    "    Entrena un modelo LightGBM con múltiples semillas y promedia las predicciones. \n",
    "    \"\"\"\n",
    "    \n",
    "    datetime_cols = train.select_dtypes(include=['datetime', 'datetime64']).columns.tolist()\n",
    "    X_train = train.drop(columns=[*datetime_cols, 'target'])\n",
    "    y_train = train['target']    \n",
    "    X_test = test.drop(columns=[*datetime_cols, 'target'])\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "    \n",
    "    # Número de repeticiones con semillas distintas\n",
    "    best_params = levantar_hiperparametros(version)\n",
    "    \n",
    "    # Cargar datos de entrenamiento y prueba\n",
    "    seeds = [42, 101, 202, 303, 404]\n",
    "    predictions = []\n",
    "\n",
    "    # Lista para almacenar los feature importance de cada modelo\n",
    "    feature_importances = []\n",
    "    feature_names = X_train.columns.tolist()  # Nombres de las features\n",
    "    \n",
    "\n",
    "    for seed in seeds:\n",
    "        params = best_params.copy()\n",
    "        params['seed'] = seed\n",
    "        \n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            num_boost_round=1000,\n",
    "            valid_sets=[train_data],\n",
    "            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "        )\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        predictions.append(y_pred)\n",
    "        \n",
    "        # Obtener feature importance para este modelo\n",
    "        importance = model.feature_importance(importance_type='gain')  # 'gain' o 'split'\n",
    "        feature_importances.append(importance)\n",
    "\n",
    "    # Promediar predicciones\n",
    "    final_prediction = np.mean(predictions, axis=0)   \n",
    "    \n",
    "    # Crear DataFrame con IDs y predicciones\n",
    "    result_df = test[['periodo', 'product_id', 'target']].copy()\n",
    "    result_df['pred'] = final_prediction\n",
    "    \n",
    "    # Procesar y guardar feature importance\n",
    "    #############################################\n",
    "    # 1. Promediar los feature importance de todos los modelos\n",
    "    avg_importance = np.mean(feature_importances, axis=0)\n",
    "    \n",
    "    # 2. Crear DataFrame con los resultados\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': avg_importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # 3. Guardar a CSV\n",
    "    # importance_df.to_csv('feature_importance.csv', index=False)\n",
    "    \n",
    "    # 4. Guardar a JSON (opcional)\n",
    "    importance_dict = importance_df.set_index('feature')['importance'].to_dict()\n",
    "    with open(f'./fe_{version}.json', 'w') as f:\n",
    "        json.dump(importance_dict, f, indent=4)\n",
    "    #############################################\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def semillerio_en_prediccion_ivan(train, test, version=\"v1\"):\n",
    "    \"\"\"\n",
    "    Entrena un modelo LightGBM con múltiples semillas y promedia las predicciones.\n",
    "    Utiliza validación explícita (20%) para evitar data leakage.\n",
    "    Devuelve las predicciones promedio y guarda la importancia promedio de features.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Identificar columnas de tipo datetime y eliminarlas\n",
    "    datetime_cols = train.select_dtypes(include=['datetime', 'datetime64']).columns.tolist()\n",
    "    \n",
    "    # Separar features y target\n",
    "    X = train.drop(columns=[*datetime_cols, 'target'])\n",
    "    y = train['target']\n",
    "    X_test = test.drop(columns=[*datetime_cols, 'target'])\n",
    "\n",
    "    # Levantar hiperparámetros optimizados\n",
    "    best_params = levantar_hiperparametros(version)\n",
    "    if best_params is None:\n",
    "        raise ValueError(\"No se encontraron hiperparámetros para esta versión\")\n",
    "\n",
    "    # Configurar semillas\n",
    "    seeds = [42, 101, 202, 303, 404]\n",
    "    predictions = []\n",
    "    rmses = []\n",
    "\n",
    "    feature_importances = []\n",
    "    feature_names = X.columns.tolist()\n",
    "\n",
    "    for seed in seeds:\n",
    "        # Dividir en train y validación (sin shuffle si es time series, con shuffle si no)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=0.2, shuffle=True, random_state=seed\n",
    "        )\n",
    "\n",
    "        # Preparar datasets para LightGBM\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        valid_data = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "        # Copiar hiperparámetros y fijar semilla\n",
    "        params = best_params.copy()\n",
    "        params['seed'] = seed\n",
    "\n",
    "        # Entrenar modelo con early stopping\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            num_boost_round=1000,\n",
    "            valid_sets=[valid_data],\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(50, verbose=False),\n",
    "                lgb.log_evaluation(0)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Predecir test y guardar predicción\n",
    "        y_pred = model.predict(X_test)\n",
    "        predictions.append(y_pred)\n",
    "\n",
    "        # Evaluar en holdout\n",
    "        val_pred = model.predict(X_val)\n",
    "        rmse = mean_squared_error(y_val, val_pred, squared=False)\n",
    "        print(f\"Seed {seed} - RMSE en validación: {rmse:.4f}\")\n",
    "        rmses.append(rmse)\n",
    "\n",
    "        # Guardar importancia de features\n",
    "        importance = model.feature_importance(importance_type='gain')\n",
    "        feature_importances.append(importance)\n",
    "\n",
    "    # Promediar predicciones del test\n",
    "    final_prediction = np.mean(predictions, axis=0)\n",
    "\n",
    "    # Armar DataFrame con predicciones\n",
    "    result_df = test[['periodo', 'product_id', 'target']].copy()\n",
    "    result_df['pred'] = final_prediction\n",
    "\n",
    "    # Promediar importancia de features\n",
    "    avg_importance = np.mean(feature_importances, axis=0)\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': avg_importance\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "\n",
    "    # Guardar importancia en JSON\n",
    "    importance_dict = importance_df.set_index('feature')['importance'].to_dict()\n",
    "    with open(f'./fe_{version}.json', 'w') as f:\n",
    "        json.dump(importance_dict, f, indent=4)\n",
    "\n",
    "    # Mostrar resumen\n",
    "    print(f\"RMSE promedio en validación: {np.mean(rmses):.4f}\")\n",
    "    print(f\"Desvío estándar del RMSE: {np.std(rmses):.4f}\")\n",
    "\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9d9a6520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import json\n",
    "\n",
    "def levantar_hiperparametros(nombre):\n",
    "    try:\n",
    "        with open(f'./{nombre}.json', 'r') as f:\n",
    "            best_params = json.load(f)\n",
    "        return best_params\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Archivo './{nombre}.json' no encontrado.\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: El archivo './{nombre}.json' no tiene formato JSON válido.\")\n",
    "        return None\n",
    "\n",
    "def entrenar_modelo_temporal(df, version=\"lgb_time_v1\", fecha_corte=202312):\n",
    "    \"\"\"\n",
    "    Entrena un modelo LightGBM respetando la temporalidad (periodo en formato entero yyyymm).\n",
    "    Elimina columnas datetime automáticamente para evitar errores de tipo.\n",
    "    \"\"\"\n",
    "    # Ordenar por producto y periodo\n",
    "    df = df.sort_values(['product_id', 'periodo'])\n",
    "    \n",
    "    # Asegurar que no haya NaNs en el target\n",
    "    df = df.dropna(subset=['target'])\n",
    "\n",
    "    # Convertir fecha de corte a entero por seguridad\n",
    "    fecha_corte = int(fecha_corte)\n",
    "\n",
    "    # Separar en entrenamiento y validación temporal\n",
    "    train_df = df[df['periodo'] < fecha_corte].copy()\n",
    "    val_df = df[df['periodo'] >= fecha_corte].copy()\n",
    "\n",
    "    print(f\"Train: {train_df['periodo'].min()} → {train_df['periodo'].max()} | {train_df.shape}\")\n",
    "    print(f\"Val:   {val_df['periodo'].min()} → {val_df['periodo'].max()} | {val_df.shape}\")\n",
    "\n",
    "    # Detectar columnas datetime que LightGBM no puede usar\n",
    "    datetime_cols = df.select_dtypes(include='datetime64[ns]').columns.tolist()\n",
    "\n",
    "    # Preparar matrices X e y (sin columnas problemáticas)\n",
    "    X_train = train_df.drop(columns=['target', 'periodo', *datetime_cols], errors='ignore')\n",
    "    y_train = train_df['target']\n",
    "    X_val = val_df.drop(columns=['target', 'periodo', *datetime_cols], errors='ignore')\n",
    "    y_val = val_df['target']\n",
    "\n",
    "    # Cargar hiperparámetros guardados\n",
    "    best_params = levantar_hiperparametros(version)\n",
    "    if best_params is None:\n",
    "        raise ValueError(\"No se encontraron hiperparámetros para esta versión\")\n",
    "\n",
    "    best_params.update({\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'verbosity': -1\n",
    "    })\n",
    "\n",
    "    # Crear datasets LightGBM\n",
    "    lgb_train = lgb.Dataset(X_train, label=y_train)\n",
    "    lgb_val = lgb.Dataset(X_val, label=y_val, reference=lgb_train)\n",
    "\n",
    "    # Entrenamiento con early stopping\n",
    "    model = lgb.train(\n",
    "        best_params,\n",
    "        lgb_train,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[lgb_val],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(50),\n",
    "            lgb.log_evaluation(50)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Predicción y evaluación\n",
    "    y_pred = model.predict(X_val)\n",
    "    rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "    print(f\"📉 RMSE en validación temporal: {rmse:.4f}\")\n",
    "\n",
    "    # Guardar modelo entrenado\n",
    "    model.save_model(f'model_{version}.txt')\n",
    "\n",
    "    # Importancia de variables\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': model.feature_importance(importance_type='gain')\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    importance_df.to_csv(f'feature_importance_{version}.csv', index=False)\n",
    "\n",
    "    # DataFrame de resultados\n",
    "    result_df = val_df[['product_id', 'periodo']].copy()\n",
    "    result_df['target'] = y_val.values\n",
    "    result_df['pred'] = y_pred\n",
    "\n",
    "    return model, result_df, importance_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b982e600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecir_con_modelo(df_test, version=\"lgb_time_v1\"):\n",
    "    \"\"\"\n",
    "    Carga un modelo LightGBM entrenado y realiza predicciones sobre un nuevo dataset.\n",
    "    Elimina columnas datetime y 'periodo' antes de predecir.\n",
    "    \n",
    "    Args:\n",
    "        df_test (pd.DataFrame): Dataset con variables ya generadas (lags, rolling, etc.)\n",
    "        version (str): Nombre del modelo/version entrenada (coincide con .txt y .json guardados)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame con columnas: product_id, periodo, pred\n",
    "    \"\"\"\n",
    "    import lightgbm as lgb\n",
    "    import pandas as pd\n",
    "\n",
    "    # Cargar modelo\n",
    "    model = lgb.Booster(model_file=f'model_{version}.txt')\n",
    "\n",
    "    # Detectar y eliminar columnas datetime (LightGBM no las acepta)\n",
    "    datetime_cols = df_test.select_dtypes(include='datetime64[ns]').columns.tolist()\n",
    "\n",
    "    # Preparar features para predicción\n",
    "    X_test = df_test.drop(columns=['periodo', *datetime_cols], errors='ignore')\n",
    "\n",
    "    # Realizar predicciones\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Armar DataFrame de resultados\n",
    "    resultado = df_test[['product_id', 'periodo']].copy()\n",
    "    resultado['pred'] = y_pred\n",
    "\n",
    "    return resultado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7ff542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = data[data['periodo'] <= 201912]\n",
    "# test = data[data['periodo'] == 201912]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4e278414",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (py.warnings._showwarnmsg) - C:\\Users\\iparra\\AppData\\Local\\Temp\\ipykernel_19752\\2820485344.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['target'].fillna(0, inplace=True)\n",
      "\n",
      "[I 2025-06-24 14:24:17,452] A new study created in memory with name: no-name-cd8de534-21bf-4a14-ac58-ae1252c701b1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Iniciando optimización de hiperparámetros (Optuna, 50 trials)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:24:17,676] Trial 0 finished with value: 1.460057624426693 and parameters: {'num_leaves': 91, 'learning_rate': 0.0578592471302025, 'feature_fraction': 0.5335644996785998, 'bagging_fraction': 0.5360396619290303, 'bagging_freq': 2, 'lambda_l1': 1.3219531031516678e-06, 'lambda_l2': 1.8782021488699185e-06, 'min_child_samples': 16, 'max_depth': 4, 'min_data_in_leaf': 31}. Best is trial 0 with value: 1.460057624426693.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:24:18,068] Trial 1 finished with value: 1.366460083375657 and parameters: {'num_leaves': 47, 'learning_rate': 0.21620130208963034, 'feature_fraction': 0.7995898308610423, 'bagging_fraction': 0.7764792213026056, 'bagging_freq': 2, 'lambda_l1': 0.18523163092614583, 'lambda_l2': 1.2818520824169845e-05, 'min_child_samples': 11, 'max_depth': 8, 'min_data_in_leaf': 82}. Best is trial 1 with value: 1.366460083375657.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:24:19,251] Trial 2 finished with value: 1.0858727570585363 and parameters: {'num_leaves': 104, 'learning_rate': 0.13475317109440937, 'feature_fraction': 0.5429589168857655, 'bagging_fraction': 0.9476382000506596, 'bagging_freq': 9, 'lambda_l1': 0.02926242745463449, 'lambda_l2': 0.18294217376554026, 'min_child_samples': 30, 'max_depth': 8, 'min_data_in_leaf': 22}. Best is trial 2 with value: 1.0858727570585363.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:24:20,817] Trial 3 finished with value: 1.3100976386974825 and parameters: {'num_leaves': 174, 'learning_rate': 0.015148171389063074, 'feature_fraction': 0.9477912042521834, 'bagging_fraction': 0.9038139800406275, 'bagging_freq': 5, 'lambda_l1': 2.821998149163252e-05, 'lambda_l2': 6.388182448599385e-08, 'min_child_samples': 26, 'max_depth': 8, 'min_data_in_leaf': 15}. Best is trial 2 with value: 1.0858727570585363.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:24:21,257] Trial 4 finished with value: 1.4287654454978538 and parameters: {'num_leaves': 151, 'learning_rate': 0.033099631053313774, 'feature_fraction': 0.6043782555241475, 'bagging_fraction': 0.632390345773859, 'bagging_freq': 9, 'lambda_l1': 1.4937819542635176e-06, 'lambda_l2': 9.18036434304166e-07, 'min_child_samples': 7, 'max_depth': 6, 'min_data_in_leaf': 20}. Best is trial 2 with value: 1.0858727570585363.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:24:22,320] Trial 5 finished with value: 1.425901733556132 and parameters: {'num_leaves': 165, 'learning_rate': 0.043248903622817504, 'feature_fraction': 0.7731385133226822, 'bagging_fraction': 0.7574222910198446, 'bagging_freq': 7, 'lambda_l1': 1.8525572671787124e-08, 'lambda_l2': 0.0003097292286340371, 'min_child_samples': 44, 'max_depth': 11, 'min_data_in_leaf': 33}. Best is trial 2 with value: 1.0858727570585363.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:24:22,572] Trial 6 finished with value: 1.485734702275058 and parameters: {'num_leaves': 36, 'learning_rate': 0.015164916048160006, 'feature_fraction': 0.979993524720139, 'bagging_fraction': 0.8710336509632357, 'bagging_freq': 10, 'lambda_l1': 0.1266712257473509, 'lambda_l2': 1.0408240315205858e-05, 'min_child_samples': 43, 'max_depth': 3, 'min_data_in_leaf': 63}. Best is trial 2 with value: 1.0858727570585363.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:24:23,132] Trial 7 finished with value: 1.4731481958283617 and parameters: {'num_leaves': 181, 'learning_rate': 0.022525766796420096, 'feature_fraction': 0.7819699317903818, 'bagging_fraction': 0.6791383083839229, 'bagging_freq': 5, 'lambda_l1': 2.171408060638711e-07, 'lambda_l2': 5.00948766465795, 'min_child_samples': 22, 'max_depth': 14, 'min_data_in_leaf': 95}. Best is trial 2 with value: 1.0858727570585363.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:24:23,633] Trial 8 finished with value: 1.4144912236917853 and parameters: {'num_leaves': 32, 'learning_rate': 0.018701769403670913, 'feature_fraction': 0.5424013169653822, 'bagging_fraction': 0.8508948103489, 'bagging_freq': 1, 'lambda_l1': 0.0005642613851631294, 'lambda_l2': 2.9838345933720996e-07, 'min_child_samples': 41, 'max_depth': 15, 'min_data_in_leaf': 11}. Best is trial 2 with value: 1.0858727570585363.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:24:24,303] Trial 9 finished with value: 1.2878051138603839 and parameters: {'num_leaves': 126, 'learning_rate': 0.275258331912726, 'feature_fraction': 0.6195398414180036, 'bagging_fraction': 0.8148224661897534, 'bagging_freq': 9, 'lambda_l1': 2.705034662804474e-05, 'lambda_l2': 0.015241686137685836, 'min_child_samples': 36, 'max_depth': 10, 'min_data_in_leaf': 42}. Best is trial 2 with value: 1.0858727570585363.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:24:24,877] Trial 10 finished with value: 1.4453499507242926 and parameters: {'num_leaves': 86, 'learning_rate': 0.1044366469163132, 'feature_fraction': 0.6778737579865199, 'bagging_fraction': 0.9977315058749846, 'bagging_freq': 7, 'lambda_l1': 9.023077433703103, 'lambda_l2': 1.240463603503302, 'min_child_samples': 31, 'max_depth': 12, 'min_data_in_leaf': 55}. Best is trial 2 with value: 1.0858727570585363.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:24:29,618] Trial 11 finished with value: 1.1477710014195088 and parameters: {'num_leaves': 127, 'learning_rate': 0.25748856110048873, 'feature_fraction': 0.6592378121946861, 'bagging_fraction': 0.9996002719737404, 'bagging_freq': 8, 'lambda_l1': 0.002240561815346189, 'lambda_l2': 0.07129487102406604, 'min_child_samples': 34, 'max_depth': 10, 'min_data_in_leaf': 38}. Best is trial 2 with value: 1.0858727570585363.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:24:29,961] Trial 12 finished with value: 1.3927556631956983 and parameters: {'num_leaves': 123, 'learning_rate': 0.14447072760984062, 'feature_fraction': 0.6870544441200659, 'bagging_fraction': 0.9977388396844776, 'bagging_freq': 7, 'lambda_l1': 0.0024247958256187105, 'lambda_l2': 0.06058304420248676, 'min_child_samples': 33, 'max_depth': 6, 'min_data_in_leaf': 39}. Best is trial 2 with value: 1.0858727570585363.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:24:30,378] Trial 13 finished with value: 1.4474950099855906 and parameters: {'num_leaves': 69, 'learning_rate': 0.08665120531154229, 'feature_fraction': 0.5182519850358016, 'bagging_fraction': 0.9266289261881564, 'bagging_freq': 8, 'lambda_l1': 0.01685453648328937, 'lambda_l2': 0.029269591189314895, 'min_child_samples': 50, 'max_depth': 9, 'min_data_in_leaf': 69}. Best is trial 2 with value: 1.0858727570585363.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:24:33,312] Trial 14 finished with value: 1.1716186638845192 and parameters: {'num_leaves': 139, 'learning_rate': 0.18441531094919178, 'feature_fraction': 0.6155643073106132, 'bagging_fraction': 0.95958312745435, 'bagging_freq': 10, 'lambda_l1': 4.601333336062874, 'lambda_l2': 0.0009338331199348193, 'min_child_samples': 25, 'max_depth': 12, 'min_data_in_leaf': 6}. Best is trial 2 with value: 1.0858727570585363.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:24:33,676] Trial 15 finished with value: 1.4272901831974305 and parameters: {'num_leaves': 103, 'learning_rate': 0.11024587579662357, 'feature_fraction': 0.8870397309591207, 'bagging_fraction': 0.9312204079647974, 'bagging_freq': 6, 'lambda_l1': 0.01121356166411442, 'lambda_l2': 0.2884606372791008, 'min_child_samples': 19, 'max_depth': 6, 'min_data_in_leaf': 48}. Best is trial 2 with value: 1.0858727570585363.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:24:34,021] Trial 16 finished with value: 1.442436778379484 and parameters: {'num_leaves': 60, 'learning_rate': 0.29135743855603435, 'feature_fraction': 0.7015296343030447, 'bagging_fraction': 0.6876310598945757, 'bagging_freq': 4, 'lambda_l1': 5.804852199683594e-05, 'lambda_l2': 0.0014455233621615768, 'min_child_samples': 37, 'max_depth': 8, 'min_data_in_leaf': 26}. Best is trial 2 with value: 1.0858727570585363.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:24:34,988] Trial 17 finished with value: 1.343171729671865 and parameters: {'num_leaves': 114, 'learning_rate': 0.07077721937383569, 'feature_fraction': 0.5871451867238359, 'bagging_fraction': 0.8142812141391804, 'bagging_freq': 8, 'lambda_l1': 0.31095508575585584, 'lambda_l2': 0.4484161904308435, 'min_child_samples': 31, 'max_depth': 13, 'min_data_in_leaf': 22}. Best is trial 2 with value: 1.0858727570585363.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:24:35,443] Trial 18 finished with value: 1.4324824975367374 and parameters: {'num_leaves': 147, 'learning_rate': 0.14613085014232602, 'feature_fraction': 0.6547550660992676, 'bagging_fraction': 0.8810313833730903, 'bagging_freq': 9, 'lambda_l1': 0.0007815101250235829, 'lambda_l2': 0.005227107702065865, 'min_child_samples': 49, 'max_depth': 10, 'min_data_in_leaf': 52}. Best is trial 2 with value: 1.0858727570585363.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:24:36,006] Trial 19 finished with value: 1.2701105874328376 and parameters: {'num_leaves': 82, 'learning_rate': 0.17421272664039972, 'feature_fraction': 0.7312207884444881, 'bagging_fraction': 0.9640271103952909, 'bagging_freq': 8, 'lambda_l1': 0.02517196372785977, 'lambda_l2': 7.2812242290084885, 'min_child_samples': 29, 'max_depth': 5, 'min_data_in_leaf': 40}. Best is trial 2 with value: 1.0858727570585363.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:24:36,451] Trial 20 finished with value: 1.4742449509961502 and parameters: {'num_leaves': 192, 'learning_rate': 0.12137079614360585, 'feature_fraction': 0.8384259263707452, 'bagging_fraction': 0.5008220246945138, 'bagging_freq': 6, 'lambda_l1': 0.8021419898278445, 'lambda_l2': 0.00017888324540342844, 'min_child_samples': 38, 'max_depth': 7, 'min_data_in_leaf': 65}. Best is trial 2 with value: 1.0858727570585363.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:24:38,533] Trial 21 finished with value: 1.0556925774831083 and parameters: {'num_leaves': 141, 'learning_rate': 0.21511060585791258, 'feature_fraction': 0.5868605317436675, 'bagging_fraction': 0.9585091568387321, 'bagging_freq': 10, 'lambda_l1': 6.181935432823129, 'lambda_l2': 0.002332850462197734, 'min_child_samples': 25, 'max_depth': 12, 'min_data_in_leaf': 5}. Best is trial 21 with value: 1.0556925774831083.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:24:41,857] Trial 22 finished with value: 1.100440392633941 and parameters: {'num_leaves': 134, 'learning_rate': 0.22662001567057524, 'feature_fraction': 0.5720626884747196, 'bagging_fraction': 0.9622560607759478, 'bagging_freq': 10, 'lambda_l1': 1.157911301464825, 'lambda_l2': 0.10616037972961875, 'min_child_samples': 23, 'max_depth': 10, 'min_data_in_leaf': 7}. Best is trial 21 with value: 1.0556925774831083.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:24:45,396] Trial 23 finished with value: 1.0964085628726619 and parameters: {'num_leaves': 105, 'learning_rate': 0.2076466950787073, 'feature_fraction': 0.5609564434301071, 'bagging_fraction': 0.9442336599267828, 'bagging_freq': 10, 'lambda_l1': 1.4452401154096932, 'lambda_l2': 0.0049913817655888775, 'min_child_samples': 15, 'max_depth': 12, 'min_data_in_leaf': 7}. Best is trial 21 with value: 1.0556925774831083.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:24:45,958] Trial 24 finished with value: 1.3975811136623846 and parameters: {'num_leaves': 100, 'learning_rate': 0.1553351420154698, 'feature_fraction': 0.5068718241262876, 'bagging_fraction': 0.8360934971663037, 'bagging_freq': 10, 'lambda_l1': 0.06321479765232824, 'lambda_l2': 4.807268435083366e-05, 'min_child_samples': 14, 'max_depth': 13, 'min_data_in_leaf': 15}. Best is trial 21 with value: 1.0556925774831083.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:24:47,970] Trial 25 finished with value: 1.3411741815777636 and parameters: {'num_leaves': 113, 'learning_rate': 0.010174765128683028, 'feature_fraction': 0.5670336493682862, 'bagging_fraction': 0.9091303506196718, 'bagging_freq': 9, 'lambda_l1': 1.9566113388430257, 'lambda_l2': 0.00396053265780295, 'min_child_samples': 5, 'max_depth': 15, 'min_data_in_leaf': 18}. Best is trial 21 with value: 1.0556925774831083.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:24:50,485] Trial 26 finished with value: 0.7997227950117618 and parameters: {'num_leaves': 16, 'learning_rate': 0.08870088047622861, 'feature_fraction': 0.6325348444346423, 'bagging_fraction': 0.9505889514846665, 'bagging_freq': 10, 'lambda_l1': 0.6052352607739411, 'lambda_l2': 0.008244679935937766, 'min_child_samples': 18, 'max_depth': 12, 'min_data_in_leaf': 5}. Best is trial 26 with value: 0.7997227950117618.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:24:51,543] Trial 27 finished with value: 1.2218292641521904 and parameters: {'num_leaves': 16, 'learning_rate': 0.08413378126809384, 'feature_fraction': 0.6352672697369423, 'bagging_fraction': 0.8983891338074572, 'bagging_freq': 9, 'lambda_l1': 6.41116677877802, 'lambda_l2': 0.5194097569074316, 'min_child_samples': 19, 'max_depth': 11, 'min_data_in_leaf': 27}. Best is trial 26 with value: 0.7997227950117618.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:25:06,107] Trial 28 finished with value: 1.1258736202102504 and parameters: {'num_leaves': 199, 'learning_rate': 0.0643655007940391, 'feature_fraction': 0.722220473545402, 'bagging_fraction': 0.7814115003904681, 'bagging_freq': 10, 'lambda_l1': 0.3446351404356873, 'lambda_l2': 0.014619373781977384, 'min_child_samples': 27, 'max_depth': 13, 'min_data_in_leaf': 5}. Best is trial 26 with value: 0.7997227950117618.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:25:06,690] Trial 29 finished with value: 1.4286345352868712 and parameters: {'num_leaves': 161, 'learning_rate': 0.05637694716271186, 'feature_fraction': 0.508303490232474, 'bagging_fraction': 0.5727318977797711, 'bagging_freq': 3, 'lambda_l1': 0.039204050921192356, 'lambda_l2': 0.00011630310072416238, 'min_child_samples': 20, 'max_depth': 9, 'min_data_in_leaf': 32}. Best is trial 26 with value: 0.7997227950117618.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:25:08,738] Trial 30 finished with value: 0.9663641719151823 and parameters: {'num_leaves': 16, 'learning_rate': 0.0883277268297075, 'feature_fraction': 0.5510105836739911, 'bagging_fraction': 0.8534126867427538, 'bagging_freq': 9, 'lambda_l1': 0.0052863837549344435, 'lambda_l2': 0.0009037827564453944, 'min_child_samples': 16, 'max_depth': 11, 'min_data_in_leaf': 13}. Best is trial 26 with value: 0.7997227950117618.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:25:10,635] Trial 31 finished with value: 1.0392532352113302 and parameters: {'num_leaves': 22, 'learning_rate': 0.04426218509747683, 'feature_fraction': 0.537792202988916, 'bagging_fraction': 0.8659294631611536, 'bagging_freq': 9, 'lambda_l1': 0.005681039082253728, 'lambda_l2': 0.00094046467083174, 'min_child_samples': 12, 'max_depth': 11, 'min_data_in_leaf': 12}. Best is trial 26 with value: 0.7997227950117618.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:25:12,060] Trial 32 finished with value: 1.0090891325138454 and parameters: {'num_leaves': 19, 'learning_rate': 0.04373352506576941, 'feature_fraction': 0.5883245932216501, 'bagging_fraction': 0.8447616803383041, 'bagging_freq': 8, 'lambda_l1': 0.00467331277326258, 'lambda_l2': 0.000743615080399587, 'min_child_samples': 8, 'max_depth': 11, 'min_data_in_leaf': 13}. Best is trial 26 with value: 0.7997227950117618.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:25:12,415] Trial 33 finished with value: 1.4441465586533615 and parameters: {'num_leaves': 15, 'learning_rate': 0.043143657427001526, 'feature_fraction': 0.5387931760606629, 'bagging_fraction': 0.7262615368286669, 'bagging_freq': 8, 'lambda_l1': 0.003778582716474972, 'lambda_l2': 2.1811627277421336e-05, 'min_child_samples': 11, 'max_depth': 11, 'min_data_in_leaf': 12}. Best is trial 26 with value: 0.7997227950117618.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:25:15,520] Trial 34 finished with value: 1.056091137656178 and parameters: {'num_leaves': 31, 'learning_rate': 0.038313920463249974, 'feature_fraction': 0.640057991893773, 'bagging_fraction': 0.7956681074855753, 'bagging_freq': 7, 'lambda_l1': 0.0001209876868989367, 'lambda_l2': 0.0009486001303240396, 'min_child_samples': 10, 'max_depth': 11, 'min_data_in_leaf': 13}. Best is trial 26 with value: 0.7997227950117618.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:25:16,130] Trial 35 finished with value: 1.4231895208419363 and parameters: {'num_leaves': 48, 'learning_rate': 0.04892335153935577, 'feature_fraction': 0.5935084681841749, 'bagging_fraction': 0.8517073949422074, 'bagging_freq': 9, 'lambda_l1': 0.007936291100245973, 'lambda_l2': 4.9354618351623535e-06, 'min_child_samples': 9, 'max_depth': 14, 'min_data_in_leaf': 25}. Best is trial 26 with value: 0.7997227950117618.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:25:16,552] Trial 36 finished with value: 1.4143821839497928 and parameters: {'num_leaves': 22, 'learning_rate': 0.031134603686992713, 'feature_fraction': 0.5422885770962879, 'bagging_fraction': 0.747528443893796, 'bagging_freq': 8, 'lambda_l1': 0.00020925595862543805, 'lambda_l2': 1.3256651471629568e-08, 'min_child_samples': 12, 'max_depth': 11, 'min_data_in_leaf': 16}. Best is trial 26 with value: 0.7997227950117618.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:25:17,179] Trial 37 finished with value: 1.4174545323459598 and parameters: {'num_leaves': 46, 'learning_rate': 0.027990084163470933, 'feature_fraction': 0.5017562562935296, 'bagging_fraction': 0.882971373605413, 'bagging_freq': 9, 'lambda_l1': 9.274044773842948e-06, 'lambda_l2': 0.0004642418184318919, 'min_child_samples': 5, 'max_depth': 9, 'min_data_in_leaf': 19}. Best is trial 26 with value: 0.7997227950117618.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:25:17,675] Trial 38 finished with value: 1.455186344954977 and parameters: {'num_leaves': 44, 'learning_rate': 0.07942842162200796, 'feature_fraction': 0.5593012741053717, 'bagging_fraction': 0.8474088746891507, 'bagging_freq': 9, 'lambda_l1': 0.07492885678995277, 'lambda_l2': 7.264174733102729e-05, 'min_child_samples': 17, 'max_depth': 14, 'min_data_in_leaf': 87}. Best is trial 26 with value: 0.7997227950117618.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:25:19,180] Trial 39 finished with value: 1.2393904406282403 and parameters: {'num_leaves': 26, 'learning_rate': 0.05202112591488208, 'feature_fraction': 0.6135441431330187, 'bagging_fraction': 0.8086151777784071, 'bagging_freq': 6, 'lambda_l1': 0.0012119995833017416, 'lambda_l2': 0.0005436775538825497, 'min_child_samples': 13, 'max_depth': 13, 'min_data_in_leaf': 29}. Best is trial 26 with value: 0.7997227950117618.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:25:19,885] Trial 40 finished with value: 1.3759711931276355 and parameters: {'num_leaves': 56, 'learning_rate': 0.02468331102614652, 'feature_fraction': 0.5230498222623017, 'bagging_fraction': 0.8688598023374637, 'bagging_freq': 8, 'lambda_l1': 4.630147147740659e-06, 'lambda_l2': 0.010280134521411218, 'min_child_samples': 17, 'max_depth': 12, 'min_data_in_leaf': 11}. Best is trial 26 with value: 0.7997227950117618.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:25:25,086] Trial 41 finished with value: 1.0110695781588135 and parameters: {'num_leaves': 34, 'learning_rate': 0.03664957220988021, 'feature_fraction': 0.5852408779809246, 'bagging_fraction': 0.890686518647981, 'bagging_freq': 10, 'lambda_l1': 0.17210580434104575, 'lambda_l2': 0.0014140614866776347, 'min_child_samples': 9, 'max_depth': 12, 'min_data_in_leaf': 10}. Best is trial 26 with value: 0.7997227950117618.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:25:30,463] Trial 42 finished with value: 0.9492858632683825 and parameters: {'num_leaves': 39, 'learning_rate': 0.038045710387281065, 'feature_fraction': 0.5885381014679132, 'bagging_fraction': 0.9072177697492717, 'bagging_freq': 10, 'lambda_l1': 0.17436647536302924, 'lambda_l2': 0.00025902020202402056, 'min_child_samples': 8, 'max_depth': 10, 'min_data_in_leaf': 10}. Best is trial 26 with value: 0.7997227950117618.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:25:35,150] Trial 43 finished with value: 1.0682447004499873 and parameters: {'num_leaves': 36, 'learning_rate': 0.03564102125385028, 'feature_fraction': 0.6387873596629419, 'bagging_fraction': 0.9102385298699084, 'bagging_freq': 10, 'lambda_l1': 0.2056189041157288, 'lambda_l2': 0.0002640197081336613, 'min_child_samples': 8, 'max_depth': 10, 'min_data_in_leaf': 22}. Best is trial 26 with value: 0.7997227950117618.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:25:39,024] Trial 44 finished with value: 0.9814427553749707 and parameters: {'num_leaves': 39, 'learning_rate': 0.06325360426976984, 'feature_fraction': 0.6047298516799526, 'bagging_fraction': 0.8312549876559359, 'bagging_freq': 10, 'lambda_l1': 0.1306185215470731, 'lambda_l2': 3.540319185961621e-06, 'min_child_samples': 7, 'max_depth': 9, 'min_data_in_leaf': 9}. Best is trial 26 with value: 0.7997227950117618.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:25:40,562] Trial 45 finished with value: 1.1196535177904694 and parameters: {'num_leaves': 72, 'learning_rate': 0.06408007405798946, 'feature_fraction': 0.6702302155865463, 'bagging_fraction': 0.8319854089294773, 'bagging_freq': 10, 'lambda_l1': 0.5778433858812986, 'lambda_l2': 1.6814254231501454e-06, 'min_child_samples': 6, 'max_depth': 7, 'min_data_in_leaf': 18}. Best is trial 26 with value: 0.7997227950117618.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:25:41,900] Trial 46 finished with value: 1.2488253566808292 and parameters: {'num_leaves': 42, 'learning_rate': 0.10714629092449272, 'feature_fraction': 0.6114586458615366, 'bagging_fraction': 0.9288225708721712, 'bagging_freq': 9, 'lambda_l1': 0.06184222110042879, 'lambda_l2': 9.884295262806617e-08, 'min_child_samples': 7, 'max_depth': 9, 'min_data_in_leaf': 34}. Best is trial 26 with value: 0.7997227950117618.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:25:43,706] Trial 47 finished with value: 1.053150351291218 and parameters: {'num_leaves': 55, 'learning_rate': 0.09123049539094139, 'feature_fraction': 0.8110645601106514, 'bagging_fraction': 0.7488876261523788, 'bagging_freq': 10, 'lambda_l1': 0.016670787544655594, 'lambda_l2': 5.815300275905707e-06, 'min_child_samples': 16, 'max_depth': 10, 'min_data_in_leaf': 9}. Best is trial 26 with value: 0.7997227950117618.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:25:47,485] Trial 48 finished with value: 1.0669228620815414 and parameters: {'num_leaves': 26, 'learning_rate': 0.07412436989491801, 'feature_fraction': 0.6996982114871158, 'bagging_fraction': 0.784028084647335, 'bagging_freq': 1, 'lambda_l1': 0.11338193152595098, 'lambda_l2': 2.3905196170519904e-05, 'min_child_samples': 22, 'max_depth': 9, 'min_data_in_leaf': 15}. Best is trial 26 with value: 0.7997227950117618.\n",
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n",
      "[I 2025-06-24 14:25:48,401] Trial 49 finished with value: 1.1465637095509724 and parameters: {'num_leaves': 15, 'learning_rate': 0.06179716594632891, 'feature_fraction': 0.6249516251065648, 'bagging_fraction': 0.9758140451095797, 'bagging_freq': 7, 'lambda_l1': 4.0125390844578605e-08, 'lambda_l2': 0.031970311454135696, 'min_child_samples': 7, 'max_depth': 8, 'min_data_in_leaf': 23}. Best is trial 26 with value: 0.7997227950117618.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Mejor RMSE: 0.7997\n",
      "✅ Mejores parámetros: {'num_leaves': 16, 'learning_rate': 0.08870088047622861, 'feature_fraction': 0.6325348444346423, 'bagging_fraction': 0.9505889514846665, 'bagging_freq': 10, 'lambda_l1': 0.6052352607739411, 'lambda_l2': 0.008244679935937766, 'min_child_samples': 18, 'max_depth': 12, 'min_data_in_leaf': 5}\n",
      "Train: 201701 → 201911 | (8393, 448)\n",
      "Val:   201912 → 201912 | (276, 448)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's rmse: 1.28166\n",
      "[100]\tvalid_0's rmse: 1.10866\n",
      "[150]\tvalid_0's rmse: 1.04346\n",
      "[200]\tvalid_0's rmse: 0.985849\n",
      "[250]\tvalid_0's rmse: 0.954386\n",
      "[300]\tvalid_0's rmse: 0.91681\n",
      "[350]\tvalid_0's rmse: 0.895484\n",
      "[400]\tvalid_0's rmse: 0.881654\n",
      "[450]\tvalid_0's rmse: 0.881314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (py.warnings._showwarnmsg) - c:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\tvalid_0's rmse: 0.879657\n",
      "Early stopping, best iteration is:\n",
      "[470]\tvalid_0's rmse: 0.878057\n",
      "📉 RMSE en validación temporal: 0.8781\n"
     ]
    }
   ],
   "source": [
    "# train['target'].fillna(0, inplace=True)\n",
    "# test['target'].fillna(0, inplace=True)\n",
    "\n",
    "# optimizar_con_optuna_ivan(train, version='v2')\n",
    "data['target'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# Paso 1: Optimización de hiperparámetros\n",
    "optimizados = optimizar_hiperparametros_lgb(data, fecha_corte=201912, version=\"lgb_time_v4\", n_trials=50)\n",
    "\n",
    "# Paso 2: Entrenamiento con los mejores parámetros\n",
    "modelo, resultados, importancia = entrenar_modelo_temporal(data, version=\"lgb_time_v4\", fecha_corte=201912)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc15ef60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203, 405)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "97f3a8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>periodo</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20615</td>\n",
       "      <td>201912</td>\n",
       "      <td>1.791601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20621</td>\n",
       "      <td>201912</td>\n",
       "      <td>1.280576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20662</td>\n",
       "      <td>201912</td>\n",
       "      <td>1.226885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>20673</td>\n",
       "      <td>201912</td>\n",
       "      <td>1.165545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>20674</td>\n",
       "      <td>201912</td>\n",
       "      <td>-0.257625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    product_id  periodo      pred\n",
       "5        20615   201912  1.791601\n",
       "11       20621   201912  1.280576\n",
       "17       20662   201912  1.226885\n",
       "23       20673   201912  1.165545\n",
       "30       20674   201912 -0.257625"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_kaggle = semillerio_en_prediccion_ivan(train, test, version='v2')\n",
    "test = data[data['periodo']==201912]\n",
    "test = test.drop(columns=['target'])\n",
    "predicciones = predecir_con_modelo(test, version=\"lgb_time_v4\")\n",
    "predicciones.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f71e0e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>periodo</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20615</td>\n",
       "      <td>201912</td>\n",
       "      <td>1.791601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20621</td>\n",
       "      <td>201912</td>\n",
       "      <td>1.280576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20662</td>\n",
       "      <td>201912</td>\n",
       "      <td>1.226885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>20673</td>\n",
       "      <td>201912</td>\n",
       "      <td>1.165545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>20674</td>\n",
       "      <td>201912</td>\n",
       "      <td>-0.257625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8569</th>\n",
       "      <td>21257</td>\n",
       "      <td>201912</td>\n",
       "      <td>-0.131212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8584</th>\n",
       "      <td>21259</td>\n",
       "      <td>201912</td>\n",
       "      <td>-0.068762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8623</th>\n",
       "      <td>21262</td>\n",
       "      <td>201912</td>\n",
       "      <td>-0.091515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8638</th>\n",
       "      <td>21263</td>\n",
       "      <td>201912</td>\n",
       "      <td>-0.058948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8668</th>\n",
       "      <td>21264</td>\n",
       "      <td>201912</td>\n",
       "      <td>-0.125779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>276 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      product_id  periodo      pred\n",
       "5          20615   201912  1.791601\n",
       "11         20621   201912  1.280576\n",
       "17         20662   201912  1.226885\n",
       "23         20673   201912  1.165545\n",
       "30         20674   201912 -0.257625\n",
       "...          ...      ...       ...\n",
       "8569       21257   201912 -0.131212\n",
       "8584       21259   201912 -0.068762\n",
       "8623       21262   201912 -0.091515\n",
       "8638       21263   201912 -0.058948\n",
       "8668       21264   201912 -0.125779\n",
       "\n",
       "[276 rows x 3 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "22fbe25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones.to_csv(\"./prediciones_entre_1_y_100.csv\", sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bf678159",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = pd.read_csv(\"prediciones_entre_1_y_100.csv\", sep=',')\n",
    "p2 = pd.read_csv(\"prediciones_entre_100_y_500.csv\", sep=',')\n",
    "p3 = pd.read_csv(\"prediciones_entre_500_y_1000.csv\", sep=',')\n",
    "p4 = pd.read_csv(\"prediciones_mayores_a_1000.csv\", sep=',')\n",
    "p5 = pd.read_csv(\"prediciones_menores_a_1.csv\", sep=',')\n",
    "\n",
    "pf = pd.concat([p1, p2, p3, p4, p5], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d3963809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(886, 3)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3b5f1cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>21263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>21265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>21266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>21267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>21276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>780 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     product_id\n",
       "0         20001\n",
       "1         20002\n",
       "2         20003\n",
       "3         20004\n",
       "4         20005\n",
       "..          ...\n",
       "775       21263\n",
       "776       21265\n",
       "777       21266\n",
       "778       21267\n",
       "779       21276\n",
       "\n",
       "[780 rows x 1 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "productos_ok = pd.read_csv(\"../../data/raw/product_id_apredecir201912.csv\")\n",
    "productos_ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "dd6c4bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>periodo</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20615</td>\n",
       "      <td>201912</td>\n",
       "      <td>1.791601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20621</td>\n",
       "      <td>201912</td>\n",
       "      <td>1.280576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20662</td>\n",
       "      <td>201912</td>\n",
       "      <td>1.226885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20673</td>\n",
       "      <td>201912</td>\n",
       "      <td>1.165545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20674</td>\n",
       "      <td>201912</td>\n",
       "      <td>-0.257625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>20242</td>\n",
       "      <td>201912</td>\n",
       "      <td>2.385573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>21265</td>\n",
       "      <td>201912</td>\n",
       "      <td>0.012797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>21266</td>\n",
       "      <td>201912</td>\n",
       "      <td>0.005002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>21267</td>\n",
       "      <td>201912</td>\n",
       "      <td>0.028273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>21276</td>\n",
       "      <td>201912</td>\n",
       "      <td>0.015420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>780 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     product_id  periodo      pred\n",
       "0         20615   201912  1.791601\n",
       "1         20621   201912  1.280576\n",
       "2         20662   201912  1.226885\n",
       "3         20673   201912  1.165545\n",
       "4         20674   201912 -0.257625\n",
       "..          ...      ...       ...\n",
       "879       20242   201912  2.385573\n",
       "881       21265   201912  0.012797\n",
       "882       21266   201912  0.005002\n",
       "883       21267   201912  0.028273\n",
       "885       21276   201912  0.015420\n",
       "\n",
       "[780 rows x 3 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf_final = pf.copy()\n",
    "pf_final = pf_final[pf_final['product_id'].isin(productos_ok['product_id'].unique())]\n",
    "pf_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "23304b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_final.rename(columns={'pred':'tn'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "faed848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_final = pf_final[['product_id', 'tn']]\n",
    "pf_final.to_csv(\"predicciones_lgb_particionado.csv\", sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c185090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c485751d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class LightGBMTimeSeriesOptimizer:\n",
    "    \"\"\"\n",
    "    Optimizador bayesiano para LightGBM en series de tiempo con ensemble de semillas\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 df: pd.DataFrame,\n",
    "                 target_col: str,\n",
    "                 feature_cols: List[str],\n",
    "                 date_col: str,\n",
    "                 weight_col: str = 'tn',\n",
    "                 train_months: List[int] = None,\n",
    "                 val_months: List[int] = None,\n",
    "                 n_folds: int = 5,\n",
    "                 n_trials: int = 20,\n",
    "                 seeds: List[int] = [42, 123, 456, 789, 999]):\n",
    "        \"\"\"\n",
    "        Inicializa el optimizador\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame con los datos\n",
    "            target_col: Nombre de la columna objetivo\n",
    "            feature_cols: Lista de columnas de características\n",
    "            date_col: Nombre de la columna de fecha\n",
    "            weight_col: Nombre de la columna de pesos\n",
    "            train_months: Lista de meses para entrenamiento (1-12)\n",
    "            val_months: Lista de meses para validación (1-12)\n",
    "            n_folds: Número de folds para CV\n",
    "            n_trials: Número de iteraciones de optimización\n",
    "            seeds: Lista de semillas para ensemble\n",
    "        \"\"\"\n",
    "        self.df = df.copy()\n",
    "        self.target_col = target_col\n",
    "        self.feature_cols = feature_cols\n",
    "        self.date_col = date_col\n",
    "        self.weight_col = weight_col\n",
    "        self.train_months = train_months or list(range(1, 13))\n",
    "        self.val_months = val_months or list(range(1, 13))\n",
    "        self.n_folds = n_folds\n",
    "        self.n_trials = n_trials\n",
    "        self.seeds = seeds\n",
    "        \n",
    "        # Preparar datos\n",
    "        self._prepare_data()\n",
    "        \n",
    "        # Mejores parámetros encontrados\n",
    "        self.best_params = None\n",
    "        self.best_score = float('inf')\n",
    "        \n",
    "    def _prepare_data(self):\n",
    "        \"\"\"Prepara los datos filtrando por meses especificados\"\"\"\n",
    "        # Convertir fecha si es necesario\n",
    "        if not pd.api.types.is_datetime64_any_dtype(self.df[self.date_col]):\n",
    "            self.df[self.date_col] = pd.to_datetime(self.df[self.date_col])\n",
    "        \n",
    "        # Extraer mes\n",
    "        self.df['month'] = self.df[self.date_col].dt.month\n",
    "        \n",
    "        # Filtrar datos de entrenamiento\n",
    "        train_mask = self.df['month'].isin(self.train_months)\n",
    "        self.train_data = self.df[train_mask].copy()\n",
    "        \n",
    "        # Filtrar datos de validación\n",
    "        val_mask = self.df['month'].isin(self.val_months)\n",
    "        self.val_data = self.df[val_mask].copy()\n",
    "        \n",
    "        print(f\"Datos de entrenamiento: {len(self.train_data)} registros\")\n",
    "        print(f\"Datos de validación: {len(self.val_data)} registros\")\n",
    "        print(f\"Meses de entrenamiento: {self.train_months}\")\n",
    "        print(f\"Meses de validación: {self.val_months}\")\n",
    "    \n",
    "    def _xgb_to_lgb_params(self, trial) -> Dict:\n",
    "        \"\"\"\n",
    "        Convierte hiperparámetros de XGBoost a LightGBM equivalentes\n",
    "        \"\"\"\n",
    "        # Mapeo de parámetros XGBoost -> LightGBM\n",
    "        params = {\n",
    "            # max_depth -> max_depth (mismo nombre)\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "            \n",
    "            # learning_rate -> learning_rate (mismo nombre)\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            \n",
    "            # n_estimators -> num_iterations\n",
    "            'num_iterations': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            \n",
    "            # min_child_weight -> min_child_samples (conversión aproximada)\n",
    "            'min_child_samples': trial.suggest_int('min_child_weight', 1, 10) * 5,\n",
    "            \n",
    "            # subsample -> bagging_fraction\n",
    "            'bagging_fraction': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'bagging_freq': 1,  # Necesario para usar bagging_fraction\n",
    "            \n",
    "            # colsample_bytree -> feature_fraction\n",
    "            'feature_fraction': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            \n",
    "            # reg_alpha -> reg_alpha (mismo nombre)\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "            \n",
    "            # reg_lambda -> reg_lambda (mismo nombre)\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "            \n",
    "            # Parámetros adicionales de LightGBM\n",
    "            'objective': 'regression',\n",
    "            'metric': 'mae',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'verbosity': -1,\n",
    "            'force_col_wise': True,\n",
    "            'deterministic': True\n",
    "        }\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def _train_with_seeds(self, params: Dict, X_train: pd.DataFrame, \n",
    "                         y_train: pd.Series, X_val: pd.DataFrame, \n",
    "                         y_val: pd.Series, weights_train: pd.Series) -> float:\n",
    "        \"\"\"\n",
    "        Entrena modelo con múltiples semillas y devuelve MAE promedio\n",
    "        \"\"\"\n",
    "        mae_scores = []\n",
    "        \n",
    "        for seed in self.seeds:\n",
    "            # Configurar semilla\n",
    "            params_seed = params.copy()\n",
    "            params_seed['random_state'] = seed\n",
    "            params_seed['bagging_seed'] = seed\n",
    "            params_seed['feature_fraction_seed'] = seed\n",
    "            \n",
    "            # Crear datasets de LightGBM\n",
    "            train_set = lgb.Dataset(X_train, label=y_train, weight=weights_train)\n",
    "            \n",
    "            # Entrenar modelo\n",
    "            model = lgb.train(\n",
    "                params_seed,\n",
    "                train_set,\n",
    "                valid_sets=[train_set],\n",
    "                callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "            )\n",
    "            \n",
    "            # Predecir y calcular MAE\n",
    "            y_pred = model.predict(X_val)\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            mae_scores.append(mae)\n",
    "        \n",
    "        # Retornar MAE promedio de todas las semillas\n",
    "        return np.mean(mae_scores)\n",
    "    \n",
    "    def _objective(self, trial) -> float:\n",
    "        \"\"\"\n",
    "        Función objetivo para Optuna\n",
    "        \"\"\"\n",
    "        # Obtener hiperparámetros\n",
    "        params = self._xgb_to_lgb_params(trial)\n",
    "        \n",
    "        # Preparar datos para CV temporal\n",
    "        X_train = self.train_data[self.feature_cols]\n",
    "        y_train = self.train_data[self.target_col]\n",
    "        weights_train = self.train_data[self.weight_col]\n",
    "        \n",
    "        # Validación cruzada temporal\n",
    "        tscv = TimeSeriesSplit(n_splits=self.n_folds)\n",
    "        cv_scores = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train)):\n",
    "            # Dividir datos del fold\n",
    "            X_fold_train = X_train.iloc[train_idx]\n",
    "            y_fold_train = y_train.iloc[train_idx]\n",
    "            weights_fold_train = weights_train.iloc[train_idx]\n",
    "            \n",
    "            X_fold_val = X_train.iloc[val_idx]\n",
    "            y_fold_val = y_train.iloc[val_idx]\n",
    "            \n",
    "            # Entrenar con ensemble de semillas\n",
    "            mae_score = self._train_with_seeds(\n",
    "                params, X_fold_train, y_fold_train, \n",
    "                X_fold_val, y_fold_val, weights_fold_train\n",
    "            )\n",
    "            \n",
    "            cv_scores.append(mae_score)\n",
    "            \n",
    "            # Reporte de progreso\n",
    "            print(f\"Trial {trial.number}, Fold {fold+1}/{self.n_folds}, MAE: {mae_score:.4f}\")\n",
    "        \n",
    "        # MAE promedio de todos los folds\n",
    "        mean_cv_score = np.mean(cv_scores)\n",
    "        \n",
    "        print(f\"Trial {trial.number} completado. MAE CV promedio: {mean_cv_score:.4f}\")\n",
    "        \n",
    "        return mean_cv_score\n",
    "    \n",
    "    def optimize(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Ejecuta la optimización bayesiana\n",
    "        \"\"\"\n",
    "        print(\"Iniciando optimización bayesiana con Optuna...\")\n",
    "        print(f\"Número de trials: {self.n_trials}\")\n",
    "        print(f\"Número de folds: {self.n_folds}\")\n",
    "        print(f\"Semillas para ensemble: {self.seeds}\")\n",
    "        \n",
    "        # Crear estudio de Optuna\n",
    "        study = optuna.create_study(direction='minimize', \n",
    "                                  sampler=optuna.samplers.TPESampler(seed=42))\n",
    "        \n",
    "        # Ejecutar optimización\n",
    "        study.optimize(self._objective, n_trials=self.n_trials)\n",
    "        \n",
    "        # Guardar mejores resultados\n",
    "        self.best_params = study.best_params\n",
    "        self.best_score = study.best_value\n",
    "        \n",
    "        print(f\"\\nOptimización completada!\")\n",
    "        print(f\"Mejor MAE: {self.best_score:.4f}\")\n",
    "        print(f\"Mejores parámetros: {self.best_params}\")\n",
    "        \n",
    "        return {\n",
    "            'best_params': self.best_params,\n",
    "            'best_score': self.best_score,\n",
    "            'study': study\n",
    "        }\n",
    "    \n",
    "    def train_final_model(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Entrena el modelo final con los mejores parámetros en datos de validación\n",
    "        \"\"\"\n",
    "        if self.best_params is None:\n",
    "            raise ValueError(\"Debe ejecutar optimize() primero\")\n",
    "        \n",
    "        print(\"\\nEntrenando modelo final en datos de validación...\")\n",
    "        \n",
    "        # Convertir parámetros para LightGBM\n",
    "        final_params = self._xgb_to_lgb_params_final(self.best_params)\n",
    "        \n",
    "        # Datos de validación\n",
    "        X_val = self.val_data[self.feature_cols]\n",
    "        y_val = self.val_data[self.target_col]\n",
    "        weights_val = self.val_data[self.weight_col]\n",
    "        \n",
    "        # Entrenar con ensemble de semillas\n",
    "        models = []\n",
    "        predictions = []\n",
    "        \n",
    "        for seed in self.seeds:\n",
    "            params_seed = final_params.copy()\n",
    "            params_seed['random_state'] = seed\n",
    "            params_seed['bagging_seed'] = seed\n",
    "            params_seed['feature_fraction_seed'] = seed\n",
    "            \n",
    "            # Crear dataset\n",
    "            val_set = lgb.Dataset(X_val, label=y_val, weight=weights_val)\n",
    "            \n",
    "            # Entrenar\n",
    "            model = lgb.train(\n",
    "                params_seed,\n",
    "                val_set,\n",
    "                valid_sets=[val_set],\n",
    "                callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "            )\n",
    "            \n",
    "            models.append(model)\n",
    "            predictions.append(model.predict(X_val))\n",
    "        \n",
    "        # Predicción ensemble (promedio)\n",
    "        ensemble_pred = np.mean(predictions, axis=0)\n",
    "        final_mae = mean_absolute_error(y_val, ensemble_pred)\n",
    "        \n",
    "        print(f\"MAE final en validación: {final_mae:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'models': models,\n",
    "            'ensemble_prediction': ensemble_pred,\n",
    "            'final_mae': final_mae,\n",
    "            'individual_predictions': predictions\n",
    "        }\n",
    "    \n",
    "    def _xgb_to_lgb_params_final(self, best_params: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Convierte los mejores parámetros para entrenamiento final\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            'max_depth': best_params['max_depth'],\n",
    "            'learning_rate': best_params['learning_rate'],\n",
    "            'num_iterations': best_params['n_estimators'],\n",
    "            'min_child_samples': best_params['min_child_weight'] * 5,\n",
    "            'bagging_fraction': best_params['subsample'],\n",
    "            'bagging_freq': 1,\n",
    "            'feature_fraction': best_params['colsample_bytree'],\n",
    "            'reg_alpha': best_params['reg_alpha'],\n",
    "            'reg_lambda': best_params['reg_lambda'],\n",
    "            'objective': 'regression',\n",
    "            'metric': 'mae',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'verbosity': -1,\n",
    "            'force_col_wise': True,\n",
    "            'deterministic': True\n",
    "        }\n",
    "        \n",
    "        return params\n",
    "\n",
    "# Ejemplo de uso\n",
    "def example_usage():\n",
    "    \"\"\"\n",
    "    Ejemplo de cómo usar el optimizador\n",
    "    \"\"\"\n",
    "    # Crear datos de ejemplo\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range('2020-01-01', '2023-12-31', freq='D')\n",
    "    n_samples = len(dates)\n",
    "    \n",
    "    # Datos sintéticos de series de tiempo\n",
    "    df = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'feature1': np.random.randn(n_samples),\n",
    "        'feature2': np.random.randn(n_samples),\n",
    "        'feature3': np.random.randn(n_samples),\n",
    "        'target': np.random.randn(n_samples) * 10 + 100,\n",
    "        'tn': np.random.uniform(0.5, 2.0, n_samples)  # Pesos\n",
    "    })\n",
    "    \n",
    "    # Definir columnas\n",
    "    feature_cols = ['feature1', 'feature2', 'feature3']\n",
    "    target_col = 'target'\n",
    "    date_col = 'date'\n",
    "    \n",
    "    # Configurar meses\n",
    "    train_months = [1, 2, 3, 4, 5, 6, 7, 8, 9]  # Enero a Septiembre\n",
    "    val_months = [10, 11, 12]  # Octubre a Diciembre\n",
    "    \n",
    "    # Crear optimizador\n",
    "    optimizer = LightGBMTimeSeriesOptimizer(\n",
    "        df=df,\n",
    "        target_col=target_col,\n",
    "        feature_cols=feature_cols,\n",
    "        date_col=date_col,\n",
    "        weight_col='tn',\n",
    "        train_months=train_months,\n",
    "        val_months=val_months,\n",
    "        n_folds=5,\n",
    "        n_trials=20,\n",
    "        seeds=[42, 123, 456, 789, 999]\n",
    "    )\n",
    "    \n",
    "    # Ejecutar optimización\n",
    "    results = optimizer.optimize()\n",
    "    \n",
    "    # Entrenar modelo final\n",
    "    final_results = optimizer.train_final_model()\n",
    "    \n",
    "    return optimizer, results, final_results\n",
    "\n",
    "# Descomenta la siguiente línea para ejecutar el ejemplo\n",
    "# optimizer, results, final_results = example_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a4830b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728545f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c37fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0c60af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4ddd570",
   "metadata": {},
   "source": [
    "#### Opcion 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d640d58e",
   "metadata": {},
   "source": [
    "##### 📦 1. Preprocesamiento y splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6f86c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento inicial\n",
    "df = df.copy()\n",
    "df = df.drop(columns=['periodo_dt', 'cat1', 'cat2', 'cat3', 'brand'])\n",
    "df = df.drop(columns=['nacimiento_producto', 'muerte_producto'])\n",
    "df['target'].fillna(0, inplace=True)\n",
    "# df['tn'] = df['tn'].clip(lower=1e-5)  # Evitar sample_weight=0\n",
    "scaler = MinMaxScaler(feature_range=(1, 100))\n",
    "df['tn_scaled'] = scaler.fit_transform(df[['tn']])\n",
    "\n",
    "# Definición de features\n",
    "features = [col for col in df.columns if col not in ['target']]\n",
    "\n",
    "# Splits temporales\n",
    "df_train = df[df['periodo'].between(201701, 201708)].copy()\n",
    "df_val = df[df['periodo'].between(201909, 201910)].copy()\n",
    "df_test = df[df['periodo'].between(201911, 201912)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6a4acb",
   "metadata": {},
   "source": [
    "##### 🧪 2. Optuna + TimeSeriesSplit sobre df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "2c7ac6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 11:38:57,416] A new study created in memory with name: no-name-a099987d-26bc-4ced-9ee2-0e1642a622d0\n",
      "[I 2025-06-25 11:38:57,550] Trial 0 finished with value: 0.023565999205846868 and parameters: {'max_depth': 5, 'learning_rate': 0.19184959844935182, 'n_estimators': 917, 'min_child_weight': 4, 'subsample': 0.9775237974623521, 'colsample_bytree': 0.8681530804034128, 'reg_alpha': 0.9305362158527875, 'reg_lambda': 0.4848941345217699}. Best is trial 0 with value: 0.023565999205846868.\n",
      "[I 2025-06-25 11:38:57,572] Trial 1 finished with value: 0.023565999205846868 and parameters: {'max_depth': 5, 'learning_rate': 0.17828713801239499, 'n_estimators': 991, 'min_child_weight': 4, 'subsample': 0.7882264942714594, 'colsample_bytree': 0.8511373474761001, 'reg_alpha': 0.6508106177547751, 'reg_lambda': 0.36346305698477976}. Best is trial 0 with value: 0.023565999205846868.\n",
      "[I 2025-06-25 11:38:57,617] Trial 2 finished with value: 0.023565999205846868 and parameters: {'max_depth': 12, 'learning_rate': 0.1937814160931496, 'n_estimators': 213, 'min_child_weight': 1, 'subsample': 0.5121679577952801, 'colsample_bytree': 0.5504729148677188, 'reg_alpha': 0.5756346509398128, 'reg_lambda': 0.9316386865338435}. Best is trial 0 with value: 0.023565999205846868.\n",
      "[I 2025-06-25 11:38:57,654] Trial 3 finished with value: 0.023565999205846868 and parameters: {'max_depth': 17, 'learning_rate': 0.058772268932208965, 'n_estimators': 806, 'min_child_weight': 3, 'subsample': 0.7686803502166191, 'colsample_bytree': 0.7723956638426793, 'reg_alpha': 0.09684983172913941, 'reg_lambda': 0.7642284134029405}. Best is trial 0 with value: 0.023565999205846868.\n",
      "[I 2025-06-25 11:38:57,685] Trial 4 finished with value: 0.023565999205846868 and parameters: {'max_depth': 18, 'learning_rate': 0.06686170319953896, 'n_estimators': 355, 'min_child_weight': 7, 'subsample': 0.7205769069594667, 'colsample_bytree': 0.9544362869851983, 'reg_alpha': 0.8011323096967516, 'reg_lambda': 0.3063284966865557}. Best is trial 0 with value: 0.023565999205846868.\n",
      "[I 2025-06-25 11:38:57,703] Trial 5 finished with value: 0.023565999205846868 and parameters: {'max_depth': 19, 'learning_rate': 0.042598032854045266, 'n_estimators': 952, 'min_child_weight': 5, 'subsample': 0.6421702485119127, 'colsample_bytree': 0.5056783854391422, 'reg_alpha': 0.108661226962397, 'reg_lambda': 0.9803078687905576}. Best is trial 0 with value: 0.023565999205846868.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0212961\tvalid_0's l2: 0.000472316\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0203914\tvalid_0's l2: 0.00041581\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.04567\tvalid_0's l2: 0.00286353\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0175\tvalid_0's l2: 0.000340138\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0129725\tvalid_0's l2: 0.000168285\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0212961\tvalid_0's l2: 0.000472316\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0203914\tvalid_0's l2: 0.00041581\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.04567\tvalid_0's l2: 0.00286353\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0175\tvalid_0's l2: 0.000340138\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0129725\tvalid_0's l2: 0.000168285\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0212961\tvalid_0's l2: 0.000472316\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0203914\tvalid_0's l2: 0.00041581\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.04567\tvalid_0's l2: 0.00286353\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0175\tvalid_0's l2: 0.000340138\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0129725\tvalid_0's l2: 0.000168285\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0212961\tvalid_0's l2: 0.000472316\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0203914\tvalid_0's l2: 0.00041581\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.04567\tvalid_0's l2: 0.00286353\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0175\tvalid_0's l2: 0.000340138\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0129725\tvalid_0's l2: 0.000168285\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0212961\tvalid_0's l2: 0.000472316\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0203914\tvalid_0's l2: 0.00041581\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.04567\tvalid_0's l2: 0.00286353\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0175\tvalid_0's l2: 0.000340138\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0129725\tvalid_0's l2: 0.000168285\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0212961\tvalid_0's l2: 0.000472316\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0203914\tvalid_0's l2: 0.00041581\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.04567\tvalid_0's l2: 0.00286353\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0175\tvalid_0's l2: 0.000340138\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0129725\tvalid_0's l2: 0.000168285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 11:38:57,734] Trial 6 finished with value: 0.023565999205846868 and parameters: {'max_depth': 14, 'learning_rate': 0.291832490553457, 'n_estimators': 171, 'min_child_weight': 3, 'subsample': 0.7013864862723639, 'colsample_bytree': 0.9378377548299373, 'reg_alpha': 0.5320524250495539, 'reg_lambda': 0.37655508255668557}. Best is trial 0 with value: 0.023565999205846868.\n",
      "[I 2025-06-25 11:38:57,767] Trial 7 finished with value: 0.023565999205846868 and parameters: {'max_depth': 13, 'learning_rate': 0.23478928916288078, 'n_estimators': 634, 'min_child_weight': 8, 'subsample': 0.8063982698706784, 'colsample_bytree': 0.5348601682436561, 'reg_alpha': 0.37003580770203626, 'reg_lambda': 0.6263567974714949}. Best is trial 0 with value: 0.023565999205846868.\n",
      "[I 2025-06-25 11:38:57,800] Trial 8 finished with value: 0.023565999205846868 and parameters: {'max_depth': 8, 'learning_rate': 0.12216602858102664, 'n_estimators': 885, 'min_child_weight': 2, 'subsample': 0.6045030231272468, 'colsample_bytree': 0.9488572425249172, 'reg_alpha': 0.4333480615762091, 'reg_lambda': 0.8311296580267515}. Best is trial 0 with value: 0.023565999205846868.\n",
      "[I 2025-06-25 11:38:57,816] Trial 9 finished with value: 0.023565999205846868 and parameters: {'max_depth': 16, 'learning_rate': 0.23540703769916738, 'n_estimators': 407, 'min_child_weight': 3, 'subsample': 0.5874611589129333, 'colsample_bytree': 0.5917201753944747, 'reg_alpha': 0.1546568589603058, 'reg_lambda': 0.3758030569407852}. Best is trial 0 with value: 0.023565999205846868.\n",
      "[I 2025-06-25 11:38:57,868] Trial 10 finished with value: 0.023565999205846868 and parameters: {'max_depth': 4, 'learning_rate': 0.11893547546524755, 'n_estimators': 699, 'min_child_weight': 6, 'subsample': 0.9806539227525901, 'colsample_bytree': 0.6702853267700801, 'reg_alpha': 0.9885928236205318, 'reg_lambda': 0.004078573067335167}. Best is trial 0 with value: 0.023565999205846868.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0212961\tvalid_0's l2: 0.000472316\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0203914\tvalid_0's l2: 0.00041581\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.04567\tvalid_0's l2: 0.00286353\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0175\tvalid_0's l2: 0.000340138\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0129725\tvalid_0's l2: 0.000168285\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0212961\tvalid_0's l2: 0.000472316\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0203914\tvalid_0's l2: 0.00041581\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.04567\tvalid_0's l2: 0.00286353\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0175\tvalid_0's l2: 0.000340138\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0129725\tvalid_0's l2: 0.000168285\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0212961\tvalid_0's l2: 0.000472316\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0203914\tvalid_0's l2: 0.00041581\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.04567\tvalid_0's l2: 0.00286353\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0175\tvalid_0's l2: 0.000340138\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0129725\tvalid_0's l2: 0.000168285\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0212961\tvalid_0's l2: 0.000472316\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0203914\tvalid_0's l2: 0.00041581\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.04567\tvalid_0's l2: 0.00286353\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0175\tvalid_0's l2: 0.000340138\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0129725\tvalid_0's l2: 0.000168285\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0212961\tvalid_0's l2: 0.000472316\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0203914\tvalid_0's l2: 0.00041581\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.04567\tvalid_0's l2: 0.00286353\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0175\tvalid_0's l2: 0.000340138\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0129725\tvalid_0's l2: 0.000168285\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0212961\tvalid_0's l2: 0.000472316\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0203914\tvalid_0's l2: 0.00041581\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.04567\tvalid_0's l2: 0.00286353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 11:38:57,950] Trial 11 finished with value: 0.023565999205846868 and parameters: {'max_depth': 3, 'learning_rate': 0.17282387538863508, 'n_estimators': 993, 'min_child_weight': 10, 'subsample': 0.9967473520597707, 'colsample_bytree': 0.8196958971092697, 'reg_alpha': 0.7537414731724487, 'reg_lambda': 0.14137523144409758}. Best is trial 0 with value: 0.023565999205846868.\n",
      "[I 2025-06-25 11:38:58,000] Trial 12 finished with value: 0.023565999205846868 and parameters: {'max_depth': 8, 'learning_rate': 0.2147831096671093, 'n_estimators': 758, 'min_child_weight': 4, 'subsample': 0.8534070651725059, 'colsample_bytree': 0.871938864318763, 'reg_alpha': 0.9980929707052575, 'reg_lambda': 0.5146881118445797}. Best is trial 0 with value: 0.023565999205846868.\n",
      "[I 2025-06-25 11:38:58,069] Trial 13 finished with value: 0.023565999205846868 and parameters: {'max_depth': 8, 'learning_rate': 0.1508663607676124, 'n_estimators': 551, 'min_child_weight': 5, 'subsample': 0.8937739199989612, 'colsample_bytree': 0.8510518449142153, 'reg_alpha': 0.7559652733189751, 'reg_lambda': 0.572765504876636}. Best is trial 0 with value: 0.023565999205846868.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0175\tvalid_0's l2: 0.000340138\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0129725\tvalid_0's l2: 0.000168285\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0212961\tvalid_0's l2: 0.000472316\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0203914\tvalid_0's l2: 0.00041581\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.04567\tvalid_0's l2: 0.00286353\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0175\tvalid_0's l2: 0.000340138\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0129725\tvalid_0's l2: 0.000168285\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0212961\tvalid_0's l2: 0.000472316\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0203914\tvalid_0's l2: 0.00041581\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.04567\tvalid_0's l2: 0.00286353\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0175\tvalid_0's l2: 0.000340138\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0129725\tvalid_0's l2: 0.000168285\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0212961\tvalid_0's l2: 0.000472316\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0203914\tvalid_0's l2: 0.00041581\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.04567\tvalid_0's l2: 0.00286353\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0175\tvalid_0's l2: 0.000340138\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0129725\tvalid_0's l2: 0.000168285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 11:38:58,135] Trial 14 finished with value: 0.023565999205846868 and parameters: {'max_depth': 6, 'learning_rate': 0.2789989590080229, 'n_estimators': 867, 'min_child_weight': 1, 'subsample': 0.9165194315784034, 'colsample_bytree': 0.6987068705412178, 'reg_alpha': 0.8784864862956928, 'reg_lambda': 0.21892227113594726}. Best is trial 0 with value: 0.023565999205846868.\n",
      "[I 2025-06-25 11:38:58,212] Trial 15 finished with value: 0.023565999205846868 and parameters: {'max_depth': 5, 'learning_rate': 0.12778535542934394, 'n_estimators': 995, 'min_child_weight': 6, 'subsample': 0.8310485452793416, 'colsample_bytree': 0.9994940608928367, 'reg_alpha': 0.6427784978428781, 'reg_lambda': 0.6782618203622928}. Best is trial 0 with value: 0.023565999205846868.\n",
      "[I 2025-06-25 11:38:58,277] Trial 16 finished with value: 0.023565999205846868 and parameters: {'max_depth': 9, 'learning_rate': 0.08911607089843038, 'n_estimators': 860, 'min_child_weight': 4, 'subsample': 0.92081169316734, 'colsample_bytree': 0.7827881944384752, 'reg_alpha': 0.33312795545779483, 'reg_lambda': 0.4362293840064977}. Best is trial 0 with value: 0.023565999205846868.\n",
      "[I 2025-06-25 11:38:58,326] Trial 17 finished with value: 0.023565999205846868 and parameters: {'max_depth': 10, 'learning_rate': 0.1732169584823236, 'n_estimators': 644, 'min_child_weight': 8, 'subsample': 0.7695128688143772, 'colsample_bytree': 0.8747202665755461, 'reg_alpha': 0.6645648852606417, 'reg_lambda': 0.2342557828582198}. Best is trial 0 with value: 0.023565999205846868.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0212961\tvalid_0's l2: 0.000472316\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0203914\tvalid_0's l2: 0.00041581\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.04567\tvalid_0's l2: 0.00286353\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0175\tvalid_0's l2: 0.000340138\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0129725\tvalid_0's l2: 0.000168285\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0212961\tvalid_0's l2: 0.000472316\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0203914\tvalid_0's l2: 0.00041581\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.04567\tvalid_0's l2: 0.00286353\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0175\tvalid_0's l2: 0.000340138\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0129725\tvalid_0's l2: 0.000168285\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0212961\tvalid_0's l2: 0.000472316\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0203914\tvalid_0's l2: 0.00041581\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.04567\tvalid_0's l2: 0.00286353\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0175\tvalid_0's l2: 0.000340138\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0129725\tvalid_0's l2: 0.000168285\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0212961\tvalid_0's l2: 0.000472316\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0203914\tvalid_0's l2: 0.00041581\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.04567\tvalid_0's l2: 0.00286353\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 11:38:58,367] Trial 18 finished with value: 0.023565999205846868 and parameters: {'max_depth': 6, 'learning_rate': 0.2582032705366626, 'n_estimators': 465, 'min_child_weight': 4, 'subsample': 0.6733154544619859, 'colsample_bytree': 0.6670986140324329, 'reg_alpha': 0.8915207403257668, 'reg_lambda': 0.48741185583731456}. Best is trial 0 with value: 0.023565999205846868.\n",
      "[I 2025-06-25 11:38:58,434] Trial 19 finished with value: 0.023565999205846868 and parameters: {'max_depth': 3, 'learning_rate': 0.2095338883041601, 'n_estimators': 771, 'min_child_weight': 2, 'subsample': 0.9526802464958788, 'colsample_bytree': 0.7287986479321819, 'reg_alpha': 0.269860896900032, 'reg_lambda': 0.1127497034411079}. Best is trial 0 with value: 0.023565999205846868.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0175\tvalid_0's l2: 0.000340138\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0129725\tvalid_0's l2: 0.000168285\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0212961\tvalid_0's l2: 0.000472316\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0203914\tvalid_0's l2: 0.00041581\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.04567\tvalid_0's l2: 0.00286353\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0175\tvalid_0's l2: 0.000340138\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.0129725\tvalid_0's l2: 0.000168285\n",
      "Mejores hiperparámetros:  {'max_depth': 5, 'learning_rate': 0.19184959844935182, 'n_estimators': 917, 'min_child_weight': 4, 'subsample': 0.9775237974623521, 'colsample_bytree': 0.8681530804034128, 'reg_alpha': 0.9305362158527875, 'reg_lambda': 0.4848941345217699}\n",
      "Mejor MAE:  0.023565999205846868\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import early_stopping, log_evaluation\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        'objective': 'regression',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    X = df_train.drop(columns=['target'])\n",
    "    y = df_train['target']\n",
    "    # sw = df_train['tn']\n",
    "    # sw = df_train['tn'].clip(lower=1e-5)\n",
    "    sw = df_train['tn_scaled']\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5, max_train_size=24 , test_size=2 )\n",
    "    maes = []\n",
    "    for train_idx, val_idx in tscv.split(X):\n",
    "        X_train_fold = X.iloc[train_idx]\n",
    "        y_train_fold = y.iloc[train_idx]\n",
    "        sw_train_fold = sw.iloc[train_idx]\n",
    "\n",
    "        X_val_fold = X.iloc[val_idx]\n",
    "        y_val_fold = y.iloc[val_idx]\n",
    "\n",
    "        model = lgb.LGBMRegressor(**params)\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            sample_weight=sw_train_fold,\n",
    "            eval_set=[(X_val_fold, y_val_fold)],\n",
    "            eval_metric=\"mae\",\n",
    "            callbacks=[\n",
    "                early_stopping(stopping_rounds=50),\n",
    "                log_evaluation(0)  # cambiar a 100 si querés ver progreso cada 100 iteraciones\n",
    "            ]\n",
    "        )\n",
    "        pred = model.predict(X_val_fold)\n",
    "        maes.append(mean_absolute_error(y_val_fold, pred))\n",
    "\n",
    "    return np.mean(maes)\n",
    "\n",
    "\n",
    "\n",
    "# Crear un estudio de Optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "\n",
    "best_params = study.best_params\n",
    "best_params.update({\n",
    "    'objective': 'regression',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'random_state': 42\n",
    "})\n",
    "\n",
    "# Imprimir los mejores hiperparámetros\n",
    "print(\"Mejores hiperparámetros: \", study.best_params)\n",
    "print(\"Mejor MAE: \", study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcf3c78",
   "metadata": {},
   "source": [
    "##### ✅ 3. Validación externa sobre df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "4e38541a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE en validación externa (201909–201910): 24.3070\n"
     ]
    }
   ],
   "source": [
    "model_val = lgb.LGBMRegressor(**best_params)\n",
    "model_val.fit(\n",
    "    df_train[features], df_train['target'],\n",
    "    sample_weight=df_train['tn_scaled']\n",
    ")\n",
    "\n",
    "y_val_pred = model_val.predict(df_val[features])\n",
    "mae_val = mean_absolute_error(df_val['target'], y_val_pred)\n",
    "print(f\"MAE en validación externa (201909–201910): {mae_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bf9c01",
   "metadata": {},
   "source": [
    "##### 🧪 4. Entrenamiento final + evaluación en df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "61a5460b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE en test final (201911–201912): 28.7273\n"
     ]
    }
   ],
   "source": [
    "df_fit = pd.concat([df_train, df_val])\n",
    "\n",
    "model_final = lgb.LGBMRegressor(**best_params)\n",
    "model_final.fit(\n",
    "    df_fit[features], df_fit['target'],\n",
    "    sample_weight=df_fit['tn_scaled']\n",
    ")\n",
    "\n",
    "y_test_pred = model_final.predict(df_test[features])\n",
    "mae_test = mean_absolute_error(df_test['target'], y_test_pred)\n",
    "print(f\"MAE en test final (201911–201912): {mae_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ffbc2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7264bcde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c72e25f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f98f76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24fe993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbe8d989",
   "metadata": {},
   "source": [
    "##### Opcion 3: Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6d80afc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 11:58:22,658] A new study created in memory with name: no-name-5184261e-88d5-49ba-9f4f-cb12bb80ccb8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 11:58:24,456] Trial 0 finished with value: 21.337893405527847 and parameters: {'max_depth': 6, 'learning_rate': 0.15834926051879664, 'n_estimators': 648, 'min_child_weight': 3, 'subsample': 0.516218539486245, 'colsample_bytree': 0.6924709348175548, 'reg_alpha': 0.7062128345596093, 'reg_lambda': 0.2911773055971826}. Best is trial 0 with value: 21.337893405527847.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's l1: 21.3379\tvalid_0's l2: 1059.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 11:58:24,809] Trial 1 finished with value: 22.622034208448287 and parameters: {'max_depth': 14, 'learning_rate': 0.05590818037620785, 'n_estimators': 722, 'min_child_weight': 6, 'subsample': 0.5000263212589382, 'colsample_bytree': 0.9110079241235117, 'reg_alpha': 0.7551105680087553, 'reg_lambda': 0.4283567784865431}. Best is trial 0 with value: 21.337893405527847.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[63]\tvalid_0's l1: 22.622\tvalid_0's l2: 1100.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 11:58:25,007] Trial 2 finished with value: 22.694048566215418 and parameters: {'max_depth': 19, 'learning_rate': 0.12077428742798069, 'n_estimators': 516, 'min_child_weight': 8, 'subsample': 0.7777241896337728, 'colsample_bytree': 0.6130645606799522, 'reg_alpha': 0.7842778643616982, 'reg_lambda': 0.11959553041855808}. Best is trial 0 with value: 21.337893405527847.\n",
      "[I 2025-06-25 11:58:25,125] Trial 3 finished with value: 23.938766748017535 and parameters: {'max_depth': 20, 'learning_rate': 0.08341289887877204, 'n_estimators': 405, 'min_child_weight': 5, 'subsample': 0.593854169986068, 'colsample_bytree': 0.5955136561216656, 'reg_alpha': 0.9467153401232999, 'reg_lambda': 0.05866239110818161}. Best is trial 0 with value: 21.337893405527847.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[28]\tvalid_0's l1: 22.694\tvalid_0's l2: 1164.16\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid_0's l1: 23.9388\tvalid_0's l2: 1216.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 11:58:25,372] Trial 4 finished with value: 24.151472287989442 and parameters: {'max_depth': 18, 'learning_rate': 0.043151358556438035, 'n_estimators': 315, 'min_child_weight': 2, 'subsample': 0.896366741582368, 'colsample_bytree': 0.5553686091574255, 'reg_alpha': 0.4519946606991544, 'reg_lambda': 0.9424571597391525}. Best is trial 0 with value: 21.337893405527847.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[81]\tvalid_0's l1: 24.1515\tvalid_0's l2: 1223.58\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 11:58:25,527] Trial 5 finished with value: 22.59702589691472 and parameters: {'max_depth': 10, 'learning_rate': 0.06635638865142272, 'n_estimators': 589, 'min_child_weight': 2, 'subsample': 0.9615574644848668, 'colsample_bytree': 0.5244442954815729, 'reg_alpha': 0.9326391859115322, 'reg_lambda': 0.7248702933226369}. Best is trial 0 with value: 21.337893405527847.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[61]\tvalid_0's l1: 22.597\tvalid_0's l2: 1282.44\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[81]\tvalid_0's l1: 21.2486\tvalid_0's l2: 999.526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 11:58:25,844] Trial 6 finished with value: 21.248551619932606 and parameters: {'max_depth': 5, 'learning_rate': 0.04091476197959223, 'n_estimators': 817, 'min_child_weight': 10, 'subsample': 0.6665988221721584, 'colsample_bytree': 0.7552536762344291, 'reg_alpha': 0.9600786458501125, 'reg_lambda': 0.6362925848465053}. Best is trial 6 with value: 21.248551619932606.\n",
      "[I 2025-06-25 11:58:25,981] Trial 7 finished with value: 23.039567168091597 and parameters: {'max_depth': 11, 'learning_rate': 0.050869716136326626, 'n_estimators': 528, 'min_child_weight': 5, 'subsample': 0.8504440907209697, 'colsample_bytree': 0.7470040735728563, 'reg_alpha': 0.3436176621616923, 'reg_lambda': 0.09857854310394532}. Best is trial 6 with value: 21.248551619932606.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[69]\tvalid_0's l1: 23.0396\tvalid_0's l2: 1132.49\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 11:58:26,093] Trial 8 finished with value: 22.76003926707006 and parameters: {'max_depth': 18, 'learning_rate': 0.06903119606398696, 'n_estimators': 679, 'min_child_weight': 9, 'subsample': 0.5157085287236253, 'colsample_bytree': 0.7914621361631503, 'reg_alpha': 0.3923123778526809, 'reg_lambda': 0.3415232317348281}. Best is trial 6 with value: 21.248551619932606.\n",
      "[I 2025-06-25 11:58:26,227] Trial 9 finished with value: 20.875120105299597 and parameters: {'max_depth': 7, 'learning_rate': 0.056050209806397154, 'n_estimators': 327, 'min_child_weight': 1, 'subsample': 0.6911971429353836, 'colsample_bytree': 0.8488682255014274, 'reg_alpha': 0.9427499056846147, 'reg_lambda': 0.38319237940819695}. Best is trial 9 with value: 20.875120105299597.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[50]\tvalid_0's l1: 22.76\tvalid_0's l2: 1135.32\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[62]\tvalid_0's l1: 20.8751\tvalid_0's l2: 1041.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 11:58:26,481] Trial 10 finished with value: 26.46782734587926 and parameters: {'max_depth': 3, 'learning_rate': 0.24693227885184593, 'n_estimators': 100, 'min_child_weight': 1, 'subsample': 0.7258937810964774, 'colsample_bytree': 0.9771125431671868, 'reg_alpha': 0.11513165978842421, 'reg_lambda': 0.5961669241382725}. Best is trial 9 with value: 20.875120105299597.\n",
      "[I 2025-06-25 11:58:26,588] Trial 11 finished with value: 20.534622090801463 and parameters: {'max_depth': 7, 'learning_rate': 0.1653231025548945, 'n_estimators': 935, 'min_child_weight': 10, 'subsample': 0.6735020730512913, 'colsample_bytree': 0.8488226200830606, 'reg_alpha': 0.9927590613137306, 'reg_lambda': 0.6445932594664636}. Best is trial 11 with value: 20.534622090801463.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's l1: 26.4678\tvalid_0's l2: 1309.8\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[21]\tvalid_0's l1: 20.5346\tvalid_0's l2: 1048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 11:58:26,722] Trial 12 finished with value: 21.906310839682426 and parameters: {'max_depth': 8, 'learning_rate': 0.20177943708713147, 'n_estimators': 997, 'min_child_weight': 8, 'subsample': 0.662432869057701, 'colsample_bytree': 0.864901900347429, 'reg_alpha': 0.5811311538101733, 'reg_lambda': 0.841069464563299}. Best is trial 11 with value: 20.534622090801463.\n",
      "[I 2025-06-25 11:58:26,903] Trial 13 finished with value: 21.604088490416704 and parameters: {'max_depth': 8, 'learning_rate': 0.15590322900309656, 'n_estimators': 229, 'min_child_weight': 7, 'subsample': 0.7591610313631091, 'colsample_bytree': 0.8593848977560138, 'reg_alpha': 0.8388144143543763, 'reg_lambda': 0.5314507833487296}. Best is trial 11 with value: 20.534622090801463.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's l1: 21.9063\tvalid_0's l2: 1071.16\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[21]\tvalid_0's l1: 21.6041\tvalid_0's l2: 1082.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 11:58:27,076] Trial 14 finished with value: 22.969488336731843 and parameters: {'max_depth': 14, 'learning_rate': 0.2055664910778136, 'n_estimators': 972, 'min_child_weight': 4, 'subsample': 0.6249565486157065, 'colsample_bytree': 0.991775949988527, 'reg_alpha': 0.6529546062524286, 'reg_lambda': 0.28431765032534406}. Best is trial 11 with value: 20.534622090801463.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's l1: 22.9695\tvalid_0's l2: 1095.71\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 11:58:27,275] Trial 15 finished with value: 26.887013238814657 and parameters: {'max_depth': 3, 'learning_rate': 0.2846678688470163, 'n_estimators': 855, 'min_child_weight': 10, 'subsample': 0.8195595514075245, 'colsample_bytree': 0.8201392127074557, 'reg_alpha': 0.18113970222039133, 'reg_lambda': 0.7533053800230842}. Best is trial 11 with value: 20.534622090801463.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's l1: 26.887\tvalid_0's l2: 1314.39\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[296]\tvalid_0's l1: 22.068\tvalid_0's l2: 1050.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 11:58:27,981] Trial 16 finished with value: 22.06800041993223 and parameters: {'max_depth': 8, 'learning_rate': 0.011986895944879034, 'n_estimators': 402, 'min_child_weight': 1, 'subsample': 0.692978349416347, 'colsample_bytree': 0.9210311169318095, 'reg_alpha': 0.9955328929651114, 'reg_lambda': 0.443778064828862}. Best is trial 11 with value: 20.534622090801463.\n",
      "[I 2025-06-25 11:58:28,144] Trial 17 finished with value: 19.088252389455022 and parameters: {'max_depth': 6, 'learning_rate': 0.1230241685242418, 'n_estimators': 189, 'min_child_weight': 6, 'subsample': 0.5785037107415656, 'colsample_bytree': 0.6903070427322062, 'reg_alpha': 0.8369517198553408, 'reg_lambda': 0.975887153777886}. Best is trial 17 with value: 19.088252389455022.\n",
      "[I 2025-06-25 11:58:28,241] Trial 18 finished with value: 20.272202419417475 and parameters: {'max_depth': 5, 'learning_rate': 0.1274697788991556, 'n_estimators': 124, 'min_child_weight': 7, 'subsample': 0.5833369426951021, 'colsample_bytree': 0.6810771820803103, 'reg_alpha': 0.5709202053177277, 'reg_lambda': 0.9793526200243753}. Best is trial 17 with value: 19.088252389455022.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[29]\tvalid_0's l1: 19.0883\tvalid_0's l2: 994.945\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[27]\tvalid_0's l1: 20.2722\tvalid_0's l2: 1029.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 11:58:28,360] Trial 19 finished with value: 22.534909058681365 and parameters: {'max_depth': 5, 'learning_rate': 0.1138229223194056, 'n_estimators': 122, 'min_child_weight': 6, 'subsample': 0.5672560038911707, 'colsample_bytree': 0.6586768269974882, 'reg_alpha': 0.5419279835265003, 'reg_lambda': 0.9646524655671687}. Best is trial 17 with value: 19.088252389455022.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[27]\tvalid_0's l1: 22.5349\tvalid_0's l2: 1052.31\n",
      "\n",
      "Mejores hiperparametros encontrados:\n",
      "{'max_depth': 6, 'learning_rate': 0.1230241685242418, 'n_estimators': 189, 'min_child_weight': 6, 'subsample': 0.5785037107415656, 'colsample_bytree': 0.6903070427322062, 'reg_alpha': 0.8369517198553408, 'reg_lambda': 0.975887153777886, 'boosting_type': 'gbdt', 'objective': 'regression', 'random_state': 42}\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[29]\tvalid_0's l1: 19.0883\tvalid_0's l2: 994.945\n",
      "MAE en validacion manual ([201909, 201910]): 19.0883\n",
      "\n",
      "MAE en test final ([201911, 201912]): 46.1894\n"
     ]
    }
   ],
   "source": [
    "# === IMPORTS ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "\n",
    "# === FUNCION CUSTOM_CV ===\n",
    "def custom_cv(df, periodos_train, periodos_val, features, params, verbose=False):\n",
    "    df_train = df[df['periodo'].isin(periodos_train)].copy()\n",
    "    df_val = df[df['periodo'].isin(periodos_val)].copy()\n",
    "\n",
    "    df_train['tn'] = df_train['tn'].clip(lower=1e-5)  # Evitar sample_weight = 0\n",
    "\n",
    "    X_train = df_train[features]\n",
    "    y_train = df_train['target']\n",
    "    sw_train = df_train['tn']\n",
    "\n",
    "    X_val = df_val[features]\n",
    "    y_val = df_val['target']\n",
    "\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        sample_weight=sw_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric=\"mae\",\n",
    "        callbacks=[\n",
    "            early_stopping(stopping_rounds=50),\n",
    "            log_evaluation(100 if verbose else 0)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pred = model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, pred)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"MAE en validacion manual ({periodos_val}): {mae:.4f}\")\n",
    "\n",
    "    return mae, model\n",
    "\n",
    "# === CONFIGURACION INICIAL ===\n",
    "# Asumiendo que df ya esta cargado y tiene columnas: 'periodo', 'target', 'tn', y features\n",
    "features = [col for col in df.columns if col not in ['target']]\n",
    "# features = [col for col in df.columns if col not in ['target', 'tn', 'periodo']]\n",
    "\n",
    "periodos_train  = [201701, 201702, 201703, 201704, 201705, 201706, 201707, 201708, 201709, 201710, 201711, 201712,\n",
    "                  201801, 201802, 201803, 201804, 201805, 201806, 201807, 201808, 201809, 201810, 201811, 201812,\n",
    "                  201901, 201902, 201903, 201904, 201905, 201906, 201907, 201908]\n",
    "periodos_val    = [201909, 201910]\n",
    "periodos_test   = [201911, 201912]\n",
    "\n",
    "# === OPTUNA OBJECTIVE ===\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    mae, _ = custom_cv(df, periodos_train, periodos_val, features, params, verbose=False)\n",
    "    return mae\n",
    "\n",
    "# === EJECUTAR OPTUNA ===\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "best_params = study.best_params\n",
    "best_params.update({\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'random_state': 42\n",
    "})\n",
    "\n",
    "print(\"\\nMejores hiperparametros encontrados:\")\n",
    "print(best_params)\n",
    "\n",
    "# === ENTRENAR CON LOS MEJORES PARAMETROS ===\n",
    "mae_val, best_model = custom_cv(df, periodos_train, periodos_val, features, best_params, verbose=True)\n",
    "\n",
    "# === EVALUACION EN TEST FINAL ===\n",
    "df_test = df[df['periodo'].isin(periodos_test)].copy()\n",
    "X_test = df_test[features]\n",
    "y_test = df_test['target']\n",
    "\n",
    "pred_test = best_model.predict(X_test)\n",
    "mae_test = mean_absolute_error(y_test, pred_test)\n",
    "print(f\"\\nMAE en test final ({periodos_test}): {mae_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3381944b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d647850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3be9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d055d31",
   "metadata": {},
   "source": [
    "#### Opcion 3: Chatgpt+Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "f8eba7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(df, periodos_train, periodos_val, features, params, \n",
    "                   seeds=[42, 123, 456], use_cv=True, n_splits=5, verbose=False):\n",
    "    \"\"\"\n",
    "    Evalúa un modelo con múltiples semillas. Usa CV temporal interna si use_cv=True.\n",
    "    \"\"\"\n",
    "    df_train = df[df['periodo'].isin(periodos_train)].copy()\n",
    "    df_val = df[df['periodo'].isin(periodos_val)].copy()\n",
    "    \n",
    "    # Validaciones básicas\n",
    "    if df_train.empty or df_val.empty:\n",
    "        raise ValueError(\"Train o Val está vacío.\")\n",
    "\n",
    "    # Peso\n",
    "    # df_train['tn'] = pd.to_numeric(df_train['tn'], errors='coerce').fillna(1e-5).clip(lower=1e-5)\n",
    "\n",
    "    X_train = df_train[features]\n",
    "    y_train = df_train['target']\n",
    "    sw_train = df_train['tn_scaled']\n",
    "\n",
    "    X_val = df_val[features]\n",
    "    y_val = df_val['target']\n",
    "\n",
    "    if use_cv:\n",
    "        cv_maes = []\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train)):\n",
    "            fold_maes = []\n",
    "\n",
    "            for seed in seeds:\n",
    "                model = lgb.LGBMRegressor(**params, random_state=seed)\n",
    "                model.fit(\n",
    "                    X_train.iloc[train_idx], y_train.iloc[train_idx],\n",
    "                    sample_weight=sw_train.iloc[train_idx],\n",
    "                    eval_set=[(X_train.iloc[val_idx], y_train.iloc[val_idx])],\n",
    "                    eval_metric=\"mae\",\n",
    "                    callbacks=[lgb.early_stopping(30), lgb.log_evaluation(0)]\n",
    "                )\n",
    "                pred = model.predict(X_train.iloc[val_idx])\n",
    "                fold_maes.append(mean_absolute_error(y_train.iloc[val_idx], pred))\n",
    "\n",
    "            cv_maes.append(np.mean(fold_maes))\n",
    "        \n",
    "        mae_cv = np.mean(cv_maes)\n",
    "        if verbose:\n",
    "            print(f\"CV MAE: {mae_cv:.4f}\")\n",
    "    else:\n",
    "        mae_cv = None\n",
    "\n",
    "    # Entrenamiento final con todo el set de train\n",
    "    final_preds = []\n",
    "    models = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        model = lgb.LGBMRegressor(**params, random_state=seed)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            sample_weight=sw_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric=\"mae\",\n",
    "            callbacks=[lgb.early_stopping(30), lgb.log_evaluation(0)]\n",
    "        )\n",
    "        pred = model.predict(X_val)\n",
    "        final_preds.append(pred)\n",
    "        models.append(model)\n",
    "\n",
    "    ensemble_pred = np.mean(final_preds, axis=0)\n",
    "    mae_val = mean_absolute_error(y_val, ensemble_pred)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Ensemble MAE Val: {mae_val:.4f}\")\n",
    "\n",
    "    return mae_cv if use_cv else mae_val, models, ensemble_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "585f68b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_objective(df, periodos_train, periodos_val, features, use_cv=True):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            'min_child_samples': trial.suggest_int('min_child_weight', 1, 10) * 5,\n",
    "            'bagging_fraction': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'bagging_freq': 1,\n",
    "            'feature_fraction': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'regression',\n",
    "            'metric': 'mae',\n",
    "            'verbosity': -1,\n",
    "            'deterministic': True,\n",
    "            'force_col_wise': True\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            mae, _, _ = evaluate_model(df, periodos_train, periodos_val, features, params, use_cv=use_cv)\n",
    "            return mae\n",
    "        except Exception as e:\n",
    "            print(f\"Trial error: {e}\")\n",
    "            return float(\"inf\")\n",
    "    return objective\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "1675b70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bayesian_optimization(df, periodos_train, periodos_val, periodos_test,\n",
    "                              n_trials=20, use_cv=True):\n",
    "    features = [col for col in df.columns if col not in ['target', 'tn', 'periodo']]\n",
    "    print(f\"Features: {features}\")\n",
    "\n",
    "    objective = create_objective(df, periodos_train, periodos_val, features, use_cv=use_cv)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_params.update({\n",
    "        'min_child_samples': best_params.pop('min_child_weight') * 5,\n",
    "        'bagging_fraction': best_params.pop('subsample'),\n",
    "        'bagging_freq': 1,\n",
    "        'feature_fraction': best_params.pop('colsample_bytree'),\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'metric': 'mae',\n",
    "        'verbosity': -1,\n",
    "        'deterministic': True,\n",
    "        'force_col_wise': True\n",
    "    })\n",
    "\n",
    "    print(\"\\n🏆 Mejores parámetros:\")\n",
    "    for k, v in best_params.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "    # Entrenamiento final\n",
    "    print(\"\\nEntrenando modelo final...\")\n",
    "    mae_val, models, _ = evaluate_model(df, periodos_train, periodos_val, features, best_params, use_cv=False, verbose=True)\n",
    "\n",
    "    # Evaluación en test\n",
    "    df_test = df[df['periodo'].isin(periodos_test)].copy()\n",
    "    if df_test.empty:\n",
    "        print(\"No hay datos de test\")\n",
    "        return study, models, None\n",
    "\n",
    "    X_test = df_test[features]\n",
    "    y_test = df_test['target']\n",
    "\n",
    "    preds = [model.predict(X_test) for model in models]\n",
    "    ensemble_pred = np.mean(preds, axis=0)\n",
    "    mae_test = mean_absolute_error(y_test, ensemble_pred)\n",
    "\n",
    "    print(f\"\\n🎯 MAE Test Ensemble: {mae_test:.4f}\")\n",
    "\n",
    "    return study, models, {\n",
    "        'mae_val': mae_val,\n",
    "        'mae_test': mae_test,\n",
    "        'ensemble_pred': ensemble_pred,\n",
    "        'best_params': best_params\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "faa37985",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 13:04:06,339] A new study created in memory with name: no-name-997df99b-b5aa-493f-b755-c7a78fe3b7e2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['product_id', 'mes_n', 'total_meses', 'producto_nuevo', 'ciclo_de_vida_inicial', 'sku_size', 'stock_final', 'plan_precios_cuidados', 'cust_request_qty', 'cust_request_tn', 'tn_scaled']\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's l1: 24.9594\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[62]\tvalid_0's l1: 23.2264\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[94]\tvalid_0's l1: 27.8716\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's l1: 11.0641\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's l1: 10.515\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid_0's l1: 17.5765\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[35]\tvalid_0's l1: 9.32417\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's l1: 9.23341\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[27]\tvalid_0's l1: 8.9269\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid_0's l1: 6.03951\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[44]\tvalid_0's l1: 4.91472\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[55]\tvalid_0's l1: 5.78883\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[27]\tvalid_0's l1: 4.44271\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[42]\tvalid_0's l1: 3.61037\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[33]\tvalid_0's l1: 4.12355\n",
      "Training until validation scores don't improve for 30 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 13:04:07,309] Trial 0 finished with value: 11.441143919471127 and parameters: {'max_depth': 9, 'learning_rate': 0.2536999076681772, 'n_estimators': 759, 'min_child_weight': 6, 'subsample': 0.5780093202212182, 'colsample_bytree': 0.5779972601681014, 'reg_alpha': 0.05808361216819946, 'reg_lambda': 0.8661761457749352}. Best is trial 0 with value: 11.441143919471127.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[14]\tvalid_0's l1: 13.5267\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's l1: 12.4205\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's l1: 12.8233\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[116]\tvalid_0's l1: 27.5122\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[78]\tvalid_0's l1: 28.4422\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[97]\tvalid_0's l1: 28.6919\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[67]\tvalid_0's l1: 10.1431\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[77]\tvalid_0's l1: 10.5484\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[77]\tvalid_0's l1: 13.509\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[101]\tvalid_0's l1: 8.50736\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[95]\tvalid_0's l1: 8.24292\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[103]\tvalid_0's l1: 8.13775\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's l1: 4.91508\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[76]\tvalid_0's l1: 5.26074\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[86]\tvalid_0's l1: 5.07235\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[96]\tvalid_0's l1: 4.02688\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[116]\tvalid_0's l1: 3.88551\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[113]\tvalid_0's l1: 3.57024\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid_0's l1: 12.772\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[39]\tvalid_0's l1: 12.2706\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[38]\tvalid_0's l1: 12.9969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 13:04:08,727] Trial 1 finished with value: 11.364376391313705 and parameters: {'max_depth': 13, 'learning_rate': 0.11114989443094977, 'n_estimators': 118, 'min_child_weight': 10, 'subsample': 0.9162213204002109, 'colsample_bytree': 0.6061695553391381, 'reg_alpha': 0.18182496720710062, 'reg_lambda': 0.18340450985343382}. Best is trial 1 with value: 11.364376391313705.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[206]\tvalid_0's l1: 30.1202\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[343]\tvalid_0's l1: 28.0924\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[485]\tvalid_0's l1: 27.5865\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[199]\tvalid_0's l1: 11.5226\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[138]\tvalid_0's l1: 12.0309\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[180]\tvalid_0's l1: 13.4905\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[205]\tvalid_0's l1: 7.79668\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[139]\tvalid_0's l1: 9.32236\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[212]\tvalid_0's l1: 8.5202\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[206]\tvalid_0's l1: 5.81177\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[179]\tvalid_0's l1: 5.67078\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[206]\tvalid_0's l1: 5.66913\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[237]\tvalid_0's l1: 3.59328\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[268]\tvalid_0's l1: 4.00619\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[239]\tvalid_0's l1: 4.11933\n",
      "Training until validation scores don't improve for 30 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 13:04:11,172] Trial 2 finished with value: 11.8235233651605 and parameters: {'max_depth': 8, 'learning_rate': 0.05958389350068958, 'n_estimators': 489, 'min_child_weight': 3, 'subsample': 0.8059264473611898, 'colsample_bytree': 0.569746930326021, 'reg_alpha': 0.29214464853521815, 'reg_lambda': 0.3663618432936917}. Best is trial 1 with value: 11.364376391313705.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[83]\tvalid_0's l1: 13.4308\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[80]\tvalid_0's l1: 12.8086\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[87]\tvalid_0's l1: 13.4938\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[59]\tvalid_0's l1: 26.8926\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[108]\tvalid_0's l1: 26.478\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[117]\tvalid_0's l1: 23.8648\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[65]\tvalid_0's l1: 11.1371\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[68]\tvalid_0's l1: 10.9186\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[75]\tvalid_0's l1: 14.9185\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[46]\tvalid_0's l1: 8.90435\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[82]\tvalid_0's l1: 8.10315\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[106]\tvalid_0's l1: 9.30107\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[86]\tvalid_0's l1: 6.14236\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[56]\tvalid_0's l1: 5.36945\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[49]\tvalid_0's l1: 5.50346\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[183]\tvalid_0's l1: 3.83372\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[85]\tvalid_0's l1: 3.69812\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[174]\tvalid_0's l1: 3.71212\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[29]\tvalid_0's l1: 13.6211\n",
      "Training until validation scores don't improve for 30 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 13:04:12,477] Trial 3 finished with value: 11.251813455341713 and parameters: {'max_depth': 11, 'learning_rate': 0.14447746112718687, 'n_estimators': 279, 'min_child_weight': 6, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989, 'reg_alpha': 0.6075448519014384, 'reg_lambda': 0.17052412368729153}. Best is trial 3 with value: 11.251813455341713.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[32]\tvalid_0's l1: 12.8416\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[27]\tvalid_0's l1: 13.2925\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[241]\tvalid_0's l1: 28.1253\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[59]\tvalid_0's l1: 39.0005\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[93]\tvalid_0's l1: 32.4649\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[73]\tvalid_0's l1: 14.3215\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[35]\tvalid_0's l1: 13.7952\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[75]\tvalid_0's l1: 18.8318\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[84]\tvalid_0's l1: 12.3477\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[44]\tvalid_0's l1: 10.8415\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[144]\tvalid_0's l1: 11.5403\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[84]\tvalid_0's l1: 8.44934\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[39]\tvalid_0's l1: 9.01358\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid_0's l1: 8.03084\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[44]\tvalid_0's l1: 8.79957\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[30]\tvalid_0's l1: 5.43868\n",
      "Training until validation scores don't improve for 30 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 13:04:13,172] Trial 4 finished with value: 15.24517198408094 and parameters: {'max_depth': 4, 'learning_rate': 0.2521267904777921, 'n_estimators': 970, 'min_child_weight': 9, 'subsample': 0.6523068845866853, 'colsample_bytree': 0.5488360570031919, 'reg_alpha': 0.6842330265121569, 'reg_lambda': 0.4401524937396013}. Best is trial 3 with value: 11.251813455341713.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[31]\tvalid_0's l1: 7.67691\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's l1: 13.753\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's l1: 13.2544\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's l1: 12.9692\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[130]\tvalid_0's l1: 44.0566\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[126]\tvalid_0's l1: 42.1054\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[130]\tvalid_0's l1: 41.5582\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[130]\tvalid_0's l1: 13.9494\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[130]\tvalid_0's l1: 14.4493\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[130]\tvalid_0's l1: 13.7688\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[130]\tvalid_0's l1: 11.741\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[130]\tvalid_0's l1: 11.0007\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[130]\tvalid_0's l1: 11.4581\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[130]\tvalid_0's l1: 7.45796\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[129]\tvalid_0's l1: 7.40531\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[130]\tvalid_0's l1: 7.42006\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[130]\tvalid_0's l1: 6.1105\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[126]\tvalid_0's l1: 6.12647\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[130]\tvalid_0's l1: 5.7745\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[84]\tvalid_0's l1: 12.8445\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[83]\tvalid_0's l1: 12.7423\n",
      "Training until validation scores don't improve for 30 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 13:04:14,177] Trial 5 finished with value: 16.292157209728394 and parameters: {'max_depth': 5, 'learning_rate': 0.05388108577817234, 'n_estimators': 130, 'min_child_weight': 10, 'subsample': 0.6293899908000085, 'colsample_bytree': 0.831261142176991, 'reg_alpha': 0.31171107608941095, 'reg_lambda': 0.5200680211778108}. Best is trial 3 with value: 11.251813455341713.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[85]\tvalid_0's l1: 12.8424\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[969]\tvalid_0's l1: 28.5238\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[973]\tvalid_0's l1: 28.8973\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[973]\tvalid_0's l1: 28.6948\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[497]\tvalid_0's l1: 11.5641\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[528]\tvalid_0's l1: 11.4202\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[488]\tvalid_0's l1: 11.4516\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[924]\tvalid_0's l1: 8.3106\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[730]\tvalid_0's l1: 8.52737\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[582]\tvalid_0's l1: 8.96884\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[561]\tvalid_0's l1: 5.6531\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[475]\tvalid_0's l1: 6.08746\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[456]\tvalid_0's l1: 6.0589\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[490]\tvalid_0's l1: 4.43511\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[542]\tvalid_0's l1: 4.63658\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[508]\tvalid_0's l1: 4.54721\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[267]\tvalid_0's l1: 13.3828\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[265]\tvalid_0's l1: 13.2158\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[264]\tvalid_0's l1: 13.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 13:04:20,951] Trial 6 finished with value: 11.851787278926569 and parameters: {'max_depth': 12, 'learning_rate': 0.01875220945578641, 'n_estimators': 973, 'min_child_weight': 8, 'subsample': 0.9697494707820946, 'colsample_bytree': 0.9474136752138245, 'reg_alpha': 0.5978999788110851, 'reg_lambda': 0.9218742350231168}. Best is trial 3 with value: 11.251813455341713.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[140]\tvalid_0's l1: 63.7774\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[140]\tvalid_0's l1: 64.9614\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[140]\tvalid_0's l1: 64.3737\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[140]\tvalid_0's l1: 31.4622\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[140]\tvalid_0's l1: 31.5449\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[140]\tvalid_0's l1: 31.7645\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[140]\tvalid_0's l1: 27.0011\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[140]\tvalid_0's l1: 26.9861\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[140]\tvalid_0's l1: 27.7346\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[140]\tvalid_0's l1: 23.1905\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[140]\tvalid_0's l1: 23.4712\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[140]\tvalid_0's l1: 23.5654\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[140]\tvalid_0's l1: 20.2679\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[140]\tvalid_0's l1: 20.0933\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[140]\tvalid_0's l1: 20.0128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 13:04:22,015] Trial 7 finished with value: 33.347137813399044 and parameters: {'max_depth': 4, 'learning_rate': 0.01947558230629543, 'n_estimators': 140, 'min_child_weight': 4, 'subsample': 0.6943386448447411, 'colsample_bytree': 0.6356745158869479, 'reg_alpha': 0.8287375091519293, 'reg_lambda': 0.3567533266935893}. Best is trial 3 with value: 11.251813455341713.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[140]\tvalid_0's l1: 20.5211\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[140]\tvalid_0's l1: 20.3652\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[140]\tvalid_0's l1: 20.5357\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[135]\tvalid_0's l1: 31.2856\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[223]\tvalid_0's l1: 28.4465\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[181]\tvalid_0's l1: 30.132\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's l1: 10.3009\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[161]\tvalid_0's l1: 10.2927\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[226]\tvalid_0's l1: 10.7708\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[209]\tvalid_0's l1: 8.36561\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[184]\tvalid_0's l1: 8.15956\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[222]\tvalid_0's l1: 7.70016\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[207]\tvalid_0's l1: 4.9651\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[169]\tvalid_0's l1: 5.3217\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[184]\tvalid_0's l1: 5.03766\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[132]\tvalid_0's l1: 3.87529\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[145]\tvalid_0's l1: 3.96196\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[181]\tvalid_0's l1: 3.86368\n",
      "Training until validation scores don't improve for 30 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 13:04:23,793] Trial 8 finished with value: 11.498609981296207 and parameters: {'max_depth': 8, 'learning_rate': 0.06333268775321843, 'n_estimators': 226, 'min_child_weight': 9, 'subsample': 0.5372753218398854, 'colsample_bytree': 0.9934434683002586, 'reg_alpha': 0.7722447692966574, 'reg_lambda': 0.1987156815341724}. Best is trial 3 with value: 11.251813455341713.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[88]\tvalid_0's l1: 12.8102\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's l1: 12.6387\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[71]\tvalid_0's l1: 12.62\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[146]\tvalid_0's l1: 33.5197\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[263]\tvalid_0's l1: 31.9093\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[82]\tvalid_0's l1: 40.4699\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[119]\tvalid_0's l1: 17.0081\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[119]\tvalid_0's l1: 16.2926\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[87]\tvalid_0's l1: 15.1857\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[90]\tvalid_0's l1: 13.5838\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[66]\tvalid_0's l1: 14.3067\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[103]\tvalid_0's l1: 12.439\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[183]\tvalid_0's l1: 9.64054\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[105]\tvalid_0's l1: 9.60903\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[201]\tvalid_0's l1: 8.82503\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[60]\tvalid_0's l1: 8.22021\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid_0's l1: 9.59994\n",
      "Training until validation scores don't improve for 30 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 13:04:24,620] Trial 9 finished with value: 16.580070345157477 and parameters: {'max_depth': 3, 'learning_rate': 0.1601531217136121, 'n_estimators': 736, 'min_child_weight': 8, 'subsample': 0.8856351733429728, 'colsample_bytree': 0.5370223258670452, 'reg_alpha': 0.3584657285442726, 'reg_lambda': 0.11586905952512971}. Best is trial 3 with value: 11.251813455341713.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[82]\tvalid_0's l1: 8.09161\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[28]\tvalid_0's l1: 14.3706\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[21]\tvalid_0's l1: 13.858\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid_0's l1: 14.3564\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[377]\tvalid_0's l1: 50.2132\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[377]\tvalid_0's l1: 49.9173\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[377]\tvalid_0's l1: 49.496\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[377]\tvalid_0's l1: 22.2458\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[377]\tvalid_0's l1: 22.1041\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[377]\tvalid_0's l1: 21.7869\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[377]\tvalid_0's l1: 18.4698\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[377]\tvalid_0's l1: 18.3889\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[377]\tvalid_0's l1: 18.4554\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[377]\tvalid_0's l1: 15.9622\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[377]\tvalid_0's l1: 15.8822\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[377]\tvalid_0's l1: 15.9797\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[377]\tvalid_0's l1: 13.438\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[377]\tvalid_0's l1: 13.2804\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[377]\tvalid_0's l1: 13.4129\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[377]\tvalid_0's l1: 15.9657\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[377]\tvalid_0's l1: 15.7229\n",
      "Training until validation scores don't improve for 30 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 13:04:28,838] Trial 10 finished with value: 23.935515748372143 and parameters: {'max_depth': 18, 'learning_rate': 0.010206070557576998, 'n_estimators': 377, 'min_child_weight': 1, 'subsample': 0.7828065453407016, 'colsample_bytree': 0.713406447691657, 'reg_alpha': 0.9597707459454202, 'reg_lambda': 0.011852595611645367}. Best is trial 3 with value: 11.251813455341713.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[377]\tvalid_0's l1: 15.8832\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[63]\tvalid_0's l1: 31.5978\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[115]\tvalid_0's l1: 28.2836\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[98]\tvalid_0's l1: 28.4702\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[85]\tvalid_0's l1: 11.135\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[87]\tvalid_0's l1: 11.8937\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[114]\tvalid_0's l1: 11.4443\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[164]\tvalid_0's l1: 7.8783\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[281]\tvalid_0's l1: 7.57073\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[204]\tvalid_0's l1: 7.75844\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[95]\tvalid_0's l1: 5.81889\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[110]\tvalid_0's l1: 5.479\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[174]\tvalid_0's l1: 5.14462\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[147]\tvalid_0's l1: 4.09207\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[181]\tvalid_0's l1: 3.64802\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[160]\tvalid_0's l1: 3.89927\n",
      "Training until validation scores don't improve for 30 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 13:04:30,753] Trial 11 finished with value: 11.607600247339557 and parameters: {'max_depth': 14, 'learning_rate': 0.11438892897450689, 'n_estimators': 303, 'min_child_weight': 6, 'subsample': 0.8991358530198217, 'colsample_bytree': 0.6793931487548484, 'reg_alpha': 0.06782026776262084, 'reg_lambda': 0.6355171476922921}. Best is trial 3 with value: 11.251813455341713.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's l1: 13.0761\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid_0's l1: 13.0413\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid_0's l1: 13.4545\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[198]\tvalid_0's l1: 27.5422\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[133]\tvalid_0's l1: 27.7686\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[210]\tvalid_0's l1: 25.8262\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[127]\tvalid_0's l1: 11.1001\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[95]\tvalid_0's l1: 10.9942\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[69]\tvalid_0's l1: 15.0738\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[96]\tvalid_0's l1: 8.17927\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[115]\tvalid_0's l1: 9.08518\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[188]\tvalid_0's l1: 8.21443\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[120]\tvalid_0's l1: 5.93234\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[110]\tvalid_0's l1: 5.92037\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[87]\tvalid_0's l1: 6.18989\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[81]\tvalid_0's l1: 3.83635\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's l1: 4.83284\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[191]\tvalid_0's l1: 3.88789\n",
      "Training until validation scores don't improve for 30 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 13:04:32,471] Trial 12 finished with value: 11.625574926327385 and parameters: {'max_depth': 15, 'learning_rate': 0.10781135193503111, 'n_estimators': 415, 'min_child_weight': 4, 'subsample': 0.8606412847038332, 'colsample_bytree': 0.5038788416660758, 'reg_alpha': 0.4687052188878424, 'reg_lambda': 0.2396827879945454}. Best is trial 3 with value: 11.251813455341713.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's l1: 13.5864\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[46]\tvalid_0's l1: 13.1244\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[42]\tvalid_0's l1: 13.7235\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[198]\tvalid_0's l1: 27.8042\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[130]\tvalid_0's l1: 28.7619\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[141]\tvalid_0's l1: 26.9433\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[58]\tvalid_0's l1: 10.7473\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[89]\tvalid_0's l1: 11.6955\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[63]\tvalid_0's l1: 11.41\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[195]\tvalid_0's l1: 8.00473\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[234]\tvalid_0's l1: 7.90246\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[135]\tvalid_0's l1: 8.24802\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[233]\tvalid_0's l1: 5.38793\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[152]\tvalid_0's l1: 5.53922\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[168]\tvalid_0's l1: 5.22474\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[72]\tvalid_0's l1: 4.42249\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[221]\tvalid_0's l1: 3.47574\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[124]\tvalid_0's l1: 4.24936\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[58]\tvalid_0's l1: 13.6147\n",
      "Training until validation scores don't improve for 30 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 13:04:34,528] Trial 13 finished with value: 11.321121543402304 and parameters: {'max_depth': 19, 'learning_rate': 0.12395586710756926, 'n_estimators': 234, 'min_child_weight': 7, 'subsample': 0.9912255787291954, 'colsample_bytree': 0.8304278626461434, 'reg_alpha': 0.4984881742637889, 'reg_lambda': 0.004470039476462406}. Best is trial 3 with value: 11.251813455341713.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[38]\tvalid_0's l1: 13.4053\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid_0's l1: 13.3773\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[135]\tvalid_0's l1: 26.971\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[145]\tvalid_0's l1: 25.2996\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[125]\tvalid_0's l1: 26.7832\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[84]\tvalid_0's l1: 10.827\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[58]\tvalid_0's l1: 11.3514\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[54]\tvalid_0's l1: 11.7106\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[228]\tvalid_0's l1: 7.47715\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[197]\tvalid_0's l1: 7.3522\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[134]\tvalid_0's l1: 7.45663\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[127]\tvalid_0's l1: 5.29236\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[431]\tvalid_0's l1: 4.7498\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[63]\tvalid_0's l1: 5.49684\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[222]\tvalid_0's l1: 3.22938\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[328]\tvalid_0's l1: 3.05436\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[188]\tvalid_0's l1: 3.51472\n",
      "Training until validation scores don't improve for 30 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 13:04:36,786] Trial 14 finished with value: 10.70440832136839 and parameters: {'max_depth': 19, 'learning_rate': 0.1627363004643333, 'n_estimators': 603, 'min_child_weight': 7, 'subsample': 0.9898785366436434, 'colsample_bytree': 0.8633753186051893, 'reg_alpha': 0.4621147584011281, 'reg_lambda': 0.0016620296848777307}. Best is trial 14 with value: 10.70440832136839.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[28]\tvalid_0's l1: 13.5754\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[27]\tvalid_0's l1: 13.4558\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[28]\tvalid_0's l1: 13.0131\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[94]\tvalid_0's l1: 30.6725\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[103]\tvalid_0's l1: 27.2112\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[129]\tvalid_0's l1: 26.3341\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[50]\tvalid_0's l1: 11.4126\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[57]\tvalid_0's l1: 11.3215\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[68]\tvalid_0's l1: 11.2034\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[116]\tvalid_0's l1: 8.37194\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[110]\tvalid_0's l1: 8.24773\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[93]\tvalid_0's l1: 8.42936\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[76]\tvalid_0's l1: 5.63457\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[110]\tvalid_0's l1: 5.86016\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[166]\tvalid_0's l1: 5.3054\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[104]\tvalid_0's l1: 4.39617\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[163]\tvalid_0's l1: 3.81829\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[145]\tvalid_0's l1: 3.67404\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's l1: 14.22\n",
      "Training until validation scores don't improve for 30 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 13:04:38,403] Trial 15 finished with value: 11.459526549395612 and parameters: {'max_depth': 16, 'learning_rate': 0.17685611899284673, 'n_estimators': 620, 'min_child_weight': 5, 'subsample': 0.7304453570550351, 'colsample_bytree': 0.8247110202776926, 'reg_alpha': 0.5895985018457593, 'reg_lambda': 0.08892852137302411}. Best is trial 14 with value: 10.70440832136839.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[28]\tvalid_0's l1: 13.1724\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[23]\tvalid_0's l1: 13.1408\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[132]\tvalid_0's l1: 31.5993\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[252]\tvalid_0's l1: 28.4043\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[255]\tvalid_0's l1: 26.4339\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[179]\tvalid_0's l1: 10.9419\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[147]\tvalid_0's l1: 11.16\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[200]\tvalid_0's l1: 11.4601\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[283]\tvalid_0's l1: 8.36608\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[157]\tvalid_0's l1: 9.26194\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[214]\tvalid_0's l1: 8.63241\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[195]\tvalid_0's l1: 5.52465\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[147]\tvalid_0's l1: 5.52696\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[206]\tvalid_0's l1: 5.33827\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[181]\tvalid_0's l1: 4.2481\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[185]\tvalid_0's l1: 4.10389\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[206]\tvalid_0's l1: 3.79381\n",
      "Training until validation scores don't improve for 30 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 13:04:40,680] Trial 16 finished with value: 11.65303741775477 and parameters: {'max_depth': 10, 'learning_rate': 0.07486123550555085, 'n_estimators': 618, 'min_child_weight': 7, 'subsample': 0.820209331319861, 'colsample_bytree': 0.7626257427625968, 'reg_alpha': 0.3996289077233802, 'reg_lambda': 0.6394684662708778}. Best is trial 14 with value: 10.70440832136839.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[58]\tvalid_0's l1: 13.1856\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[72]\tvalid_0's l1: 12.7183\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[64]\tvalid_0's l1: 13.1655\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[460]\tvalid_0's l1: 29.0425\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[366]\tvalid_0's l1: 29.9218\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[517]\tvalid_0's l1: 27.5345\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[518]\tvalid_0's l1: 10.614\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[520]\tvalid_0's l1: 10.6746\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[448]\tvalid_0's l1: 10.8704\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[356]\tvalid_0's l1: 7.97148\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[350]\tvalid_0's l1: 8.23718\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[530]\tvalid_0's l1: 7.57931\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[530]\tvalid_0's l1: 4.71559\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[529]\tvalid_0's l1: 4.65216\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[493]\tvalid_0's l1: 4.64784\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[409]\tvalid_0's l1: 3.92548\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[422]\tvalid_0's l1: 3.79895\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[526]\tvalid_0's l1: 3.46778\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[171]\tvalid_0's l1: 13.5182\n",
      "Training until validation scores don't improve for 30 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 13:04:46,037] Trial 17 finished with value: 11.176907605555911 and parameters: {'max_depth': 20, 'learning_rate': 0.03624480731083465, 'n_estimators': 530, 'min_child_weight': 2, 'subsample': 0.9479531437001762, 'colsample_bytree': 0.8968318085615454, 'reg_alpha': 0.6151733569935907, 'reg_lambda': 0.2759669838668322}. Best is trial 14 with value: 10.70440832136839.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[170]\tvalid_0's l1: 13.5775\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[154]\tvalid_0's l1: 13.7947\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[797]\tvalid_0's l1: 25.7436\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[798]\tvalid_0's l1: 25.4694\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[497]\tvalid_0's l1: 28.2606\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[346]\tvalid_0's l1: 10.4145\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[798]\tvalid_0's l1: 9.12093\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[562]\tvalid_0's l1: 9.68038\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[784]\tvalid_0's l1: 6.35137\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[561]\tvalid_0's l1: 6.82619\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[557]\tvalid_0's l1: 6.96699\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[497]\tvalid_0's l1: 4.63313\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[689]\tvalid_0's l1: 4.19542\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[490]\tvalid_0's l1: 4.58924\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[380]\tvalid_0's l1: 3.76447\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[551]\tvalid_0's l1: 3.24229\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[561]\tvalid_0's l1: 3.24986\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[213]\tvalid_0's l1: 13.3293\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[179]\tvalid_0's l1: 13.5433\n",
      "Training until validation scores don't improve for 30 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 13:04:52,128] Trial 18 finished with value: 10.167215476048193 and parameters: {'max_depth': 20, 'learning_rate': 0.0352430720598831, 'n_estimators': 799, 'min_child_weight': 1, 'subsample': 0.9478668969549889, 'colsample_bytree': 0.9032781518363165, 'reg_alpha': 0.764927974102249, 'reg_lambda': 0.3025381377025733}. Best is trial 18 with value: 10.167215476048193.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[185]\tvalid_0's l1: 13.4307\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[320]\tvalid_0's l1: 29.7189\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[576]\tvalid_0's l1: 26.2326\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[343]\tvalid_0's l1: 28.5351\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[819]\tvalid_0's l1: 8.6499\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[507]\tvalid_0's l1: 9.67685\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[571]\tvalid_0's l1: 9.52977\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[819]\tvalid_0's l1: 5.9002\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[773]\tvalid_0's l1: 5.9514\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[562]\tvalid_0's l1: 6.45968\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[659]\tvalid_0's l1: 3.9927\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[542]\tvalid_0's l1: 4.17716\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[821]\tvalid_0's l1: 3.70894\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[751]\tvalid_0's l1: 2.78927\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[572]\tvalid_0's l1: 3.0072\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[582]\tvalid_0's l1: 2.99198\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[195]\tvalid_0's l1: 13.2536\n",
      "Training until validation scores don't improve for 30 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 13:04:58,434] Trial 19 finished with value: 10.08810443916594 and parameters: {'max_depth': 17, 'learning_rate': 0.03410098377122444, 'n_estimators': 821, 'min_child_weight': 1, 'subsample': 0.9881730175655369, 'colsample_bytree': 0.9134336554829188, 'reg_alpha': 0.93004343617903, 'reg_lambda': 0.55198605883872}. Best is trial 19 with value: 10.08810443916594.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[220]\tvalid_0's l1: 13.4033\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[184]\tvalid_0's l1: 13.2876\n",
      "\n",
      "🏆 Mejores parámetros:\n",
      "max_depth: 17\n",
      "learning_rate: 0.03410098377122444\n",
      "n_estimators: 821\n",
      "reg_alpha: 0.93004343617903\n",
      "reg_lambda: 0.55198605883872\n",
      "min_child_samples: 5\n",
      "bagging_fraction: 0.9881730175655369\n",
      "bagging_freq: 1\n",
      "feature_fraction: 0.9134336554829188\n",
      "boosting_type: gbdt\n",
      "objective: regression\n",
      "metric: mae\n",
      "verbosity: -1\n",
      "deterministic: True\n",
      "force_col_wise: True\n",
      "\n",
      "Entrenando modelo final...\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[195]\tvalid_0's l1: 13.2536\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[220]\tvalid_0's l1: 13.4033\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[184]\tvalid_0's l1: 13.2876\n",
      "Ensemble MAE Val: 13.3003\n",
      "\n",
      "🎯 MAE Test Ensemble: 10.2768\n",
      "\n",
      "Top 5 mejores trials:\n",
      "1. MAE: 10.0881\n",
      "2. MAE: 10.1672\n",
      "3. MAE: 10.7044\n",
      "4. MAE: 11.1769\n",
      "5. MAE: 11.2518\n"
     ]
    }
   ],
   "source": [
    "# Configurar períodos\n",
    "periodos_train  = [201701, 201702, 201703, 201704, 201705, 201706, 201707, 201708, 201709, 201710, 201711, 201712,\n",
    "                  201801, 201802, 201803, 201804, 201805, 201806, 201807, 201808, 201809, 201810, 201811, 201812,\n",
    "                  201901, 201902, 201903, 201904, 201905, 201906]\n",
    "periodos_val    = [201907, 201908]\n",
    "periodos_test   = [201909, 201910]\n",
    "\n",
    "# Ejecutar optimización\n",
    "study, models, results = run_bayesian_optimization(\n",
    "    df=df,\n",
    "    periodos_train=periodos_train,\n",
    "    periodos_val=periodos_val,\n",
    "    periodos_test=periodos_test,\n",
    "    n_trials=20,\n",
    "    use_cv=True  # True = validación cruzada temporal dentro de train\n",
    ")\n",
    "\n",
    "# Análisis adicional\n",
    "print(\"\\nTop 5 mejores trials:\")\n",
    "top_trials = study.trials_dataframe().nsmallest(5, 'value')\n",
    "for i, (_, row) in enumerate(top_trials.iterrows(), 1):\n",
    "    print(f\"{i}. MAE: {row['value']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
