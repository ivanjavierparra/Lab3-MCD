{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbae8eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\.conda\\envs\\py311lab3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Importing plotly failed. Interactive plots will not work.\n",
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib\n",
    "import gc\n",
    "import sys\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from typing import List, Dict\n",
    "sys.path.append('./scripts')  \n",
    "import preprocesamiento\n",
    "import feature_engineering\n",
    "import model_autogluon\n",
    "importlib.reload(preprocesamiento)\n",
    "importlib.reload(model_autogluon)\n",
    "importlib.reload(feature_engineering)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14763c97",
   "metadata": {},
   "source": [
    "# Experimento 6: \n",
    "- Autogluon: Error-Driven Ensemble Weighting\n",
    "- NO FUNCIONO\n",
    "- Kaggle =  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70070d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ErrorDrivenEnsemble:\n",
    "#     def __init__(self, val_window_size: int = 24):\n",
    "#         \"\"\"\n",
    "#         Inicializa el ensamblador con el tamaño de la ventana de validación\n",
    "#         :param val_window_size: Número de períodos a usar para validación (por defecto 24 meses)\n",
    "#         \"\"\"\n",
    "#         self.val_window_size = val_window_size\n",
    "#         self.predictors = []\n",
    "#         self.weights = []\n",
    "#         self.best_combination = None\n",
    "\n",
    "#     def add_predictor(self, predictor_config: Dict):\n",
    "#         \"\"\"\n",
    "#         Agrega una configuración de predictor al ensamblaje\n",
    "#         :param predictor_config: Dict con configuración. Ejemplo:\n",
    "#             {\n",
    "#                 'name': 'deepar_12w',\n",
    "#                 'train_kwargs': {\n",
    "#                     'num_val_windows': 12,\n",
    "#                     'hyperparameters': {'DeepAR': {...}}\n",
    "#                 }\n",
    "#             }\n",
    "#         \"\"\"\n",
    "#         self.predictors.append(predictor_config)\n",
    "\n",
    "#     def _prepare_validation_data(self, data: TimeSeriesDataFrame):\n",
    "#         \"\"\"Prepara los datos de entrenamiento y validación\"\"\"\n",
    "#         train_data = data.iloc[:-self.val_window_size]\n",
    "#         val_data = data.iloc[-self.val_window_size:]\n",
    "#         return train_data, val_data\n",
    "\n",
    "#     def _train_predictor(self, train_data: TimeSeriesDataFrame, config: Dict):\n",
    "#         \"\"\"Entrena un predictor individual\"\"\"\n",
    "#         predictor = TimeSeriesPredictor(\n",
    "#             target='target',\n",
    "#             prediction_length=2,\n",
    "#             freq=\"M\",\n",
    "#             eval_metric=\"MAPE\",\n",
    "#             quantile_levels=[0.1, 0.5, 0.9]\n",
    "#         )\n",
    "        \n",
    "#         predictor.fit(\n",
    "#             train_data=train_data,\n",
    "#             **config['train_kwargs']\n",
    "#         )\n",
    "#         return predictor\n",
    "\n",
    "#     def _evaluate_predictor(self, predictor, val_data: TimeSeriesDataFrame):\n",
    "#         \"\"\"Evalúa un predictor en los datos de validación\"\"\"\n",
    "#         preds = predictor.predict(val_data)\n",
    "#         val_targets = val_data['target']\n",
    "        \n",
    "#         # Alinear índices para comparación\n",
    "#         aligned_preds = preds.reset_index().merge(\n",
    "#             val_data.reset_index()[['item_id', 'timestamp']],\n",
    "#             on=['item_id', 'timestamp']\n",
    "#         ).set_index(['item_id', 'timestamp'])\n",
    "        \n",
    "#         mae = mean_absolute_error(val_targets, aligned_preds['mean'])\n",
    "#         return mae, preds\n",
    "\n",
    "#     def optimize_weights(self, data: TimeSeriesDataFrame):\n",
    "#         \"\"\"\n",
    "#         Entrena y evalúa todos los predictores, calculando los pesos óptimos\n",
    "#         :param data: Datos completos (TimeSeriesDataFrame)\n",
    "#         :return: Dict con resultados de optimización\n",
    "#         \"\"\"\n",
    "#         train_data, val_data = self._prepare_validation_data(data)\n",
    "#         results = []\n",
    "        \n",
    "#         # Entrenar y evaluar cada predictor\n",
    "#         for config in self.predictors:\n",
    "#             print(f\"\\nEntrenando {config['name']}...\")\n",
    "#             try:\n",
    "#                 predictor = self._train_predictor(train_data, config)\n",
    "#                 mae, preds = self._evaluate_predictor(predictor, val_data)\n",
    "                \n",
    "#                 results.append({\n",
    "#                     'name': config['name'],\n",
    "#                     'predictor': predictor,\n",
    "#                     'mae': mae,\n",
    "#                     'val_predictions': preds\n",
    "#                 })\n",
    "#                 print(f\"{config['name']} - MAE: {mae:.2f}\")\n",
    "                \n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error entrenando {config['name']}: {str(e)}\")\n",
    "#                 continue\n",
    "        \n",
    "#         if not results:\n",
    "#             raise ValueError(\"Ningún predictor se entrenó exitosamente\")\n",
    "        \n",
    "#         # Calcular pesos (inversamente proporcional al error)\n",
    "#         maes = np.array([r['mae'] for r in results])\n",
    "#         self.weights = (1 / maes) / (1 / maes).sum()\n",
    "        \n",
    "#         # Guardar resultados\n",
    "#         self.results = results\n",
    "#         self.best_combination = {\n",
    "#             'names': [r['name'] for r in results],\n",
    "#             'weights': self.weights.tolist(),\n",
    "#             'mean_mae': maes.mean(),\n",
    "#             'best_mae': maes.min()\n",
    "#         }\n",
    "        \n",
    "#         return self.best_combination\n",
    "\n",
    "#     def predict(self, data: TimeSeriesDataFrame):\n",
    "#         \"\"\"\n",
    "#         Genera predicciones ensambladas\n",
    "#         :param data: Datos para predecir\n",
    "#         :return: TimeSeriesDataFrame con predicciones ensambladas\n",
    "#         \"\"\"\n",
    "#         if not self.results:\n",
    "#             raise ValueError(\"Debes llamar a optimize_weights() primero\")\n",
    "        \n",
    "#         all_preds = []\n",
    "#         for weight, result in zip(self.weights, self.results):\n",
    "#             preds = result['predictor'].predict(data)\n",
    "#             preds['mean'] = preds['mean'] * weight\n",
    "#             all_preds.append(preds[['mean']])\n",
    "        \n",
    "#         # Combinar predicciones\n",
    "#         ensemble_pred = sum(all_preds)\n",
    "#         ensemble_pred['0.1'] = all_preds[0]['0.1']  # Mantener intervalos del primer predictor\n",
    "#         ensemble_pred['0.9'] = all_preds[0]['0.9']\n",
    "        \n",
    "#         return ensemble_pred\n",
    "\n",
    "# # ---------------------------------------------------------------\n",
    "# # EJEMPLO DE USO COMPLETO\n",
    "# # ---------------------------------------------------------------\n",
    "\n",
    "# def main():\n",
    "#     # 1. Cargar y preparar datos (usando tu código existente)\n",
    "#     df = pd.read_csv(\"./datasets/periodo_x_producto_con_target.csv\", sep=',', encoding='utf-8')\n",
    "#     dfg = df.groupby(['periodo', 'product_id']).agg({'tn': 'sum'}).reset_index()\n",
    "#     dfg['timestamp'] = pd.to_datetime(dfg['periodo'].astype(str), format='%Y%m')\n",
    "#     dfg.rename(columns={'tn': 'target', 'product_id': 'item_id'}, inplace=True)\n",
    "    \n",
    "#     # Filtrar productos\n",
    "#     productos_ok = pd.read_csv('../../data/raw/product_id_apredecir201912.csv', sep=',')\n",
    "#     dfg = dfg[dfg['item_id'].isin(productos_ok['product_id'].unique())]\n",
    "    \n",
    "#     # Convertir a TimeSeriesDataFrame\n",
    "#     data = TimeSeriesDataFrame.from_data_frame(\n",
    "#         dfg,\n",
    "#         id_column=\"item_id\",\n",
    "#         timestamp_column=\"timestamp\"\n",
    "#     )\n",
    "\n",
    "#     # 2. Configurar el ensamblador\n",
    "#     ensemble = ErrorDrivenEnsemble(val_window_size=24)\n",
    "    \n",
    "#     # Agregar diferentes configuraciones de modelos\n",
    "#     ensemble.add_predictor({\n",
    "#         'name': 'fast_training',\n",
    "#         'train_kwargs': {\n",
    "#             'presets': 'fast_training',\n",
    "#             'num_val_windows': 3\n",
    "#         }\n",
    "#     })\n",
    "    \n",
    "#     ensemble.add_predictor({\n",
    "#         'name': 'deepar_12w',\n",
    "#         'train_kwargs': {\n",
    "#             'num_val_windows': 12,\n",
    "#             'hyperparameters': {\n",
    "#                 'DeepAR': {\n",
    "#                     'epochs': 50,\n",
    "#                     'num_cells': 64,\n",
    "#                     'context_length': 24\n",
    "#                 }\n",
    "#             }\n",
    "#         }\n",
    "#     })\n",
    "    \n",
    "#     ensemble.add_predictor({\n",
    "#         'name': 'multi_window',\n",
    "#         'train_kwargs': {\n",
    "#             'num_val_windows': 6,\n",
    "#             'hyperparameters': {\n",
    "#                 'SimpleFeedForward': {'epochs': 30},\n",
    "#                 'ARIMA': {},\n",
    "#                 'ETS': {}\n",
    "#             }\n",
    "#         }\n",
    "#     })\n",
    "\n",
    "#     # 3. Optimizar pesos\n",
    "#     optimization_result = ensemble.optimize_weights(data)\n",
    "#     print(\"\\nResultado de optimización:\")\n",
    "#     print(pd.DataFrame(optimization_result))\n",
    "\n",
    "#     # 4. Generar predicciones finales\n",
    "#     final_predictions = ensemble.predict(data)\n",
    "    \n",
    "#     # Filtrar para febrero 2020 (como en tu ejemplo)\n",
    "#     preds_202002 = final_predictions.reset_index()\n",
    "#     preds_202002 = preds_202002[preds_202002['timestamp'] == '2020-02-29']\n",
    "#     final_result = preds_202002[['item_id', 'mean']].rename(columns={\n",
    "#         'item_id': 'product_id',\n",
    "#         'mean': 'pred_ensemble'\n",
    "#     })\n",
    "    \n",
    "#     # Guardar resultados\n",
    "#     output_path = \"./outputs/predicciones_exp_06v3_autogluon.csv\"\n",
    "#     final_result.to_csv(output_path, index=False)\n",
    "#     print(f\"\\nPredicciones ensambladas guardadas en {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "405c77e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "# from typing import List, Dict\n",
    "# from itertools import combinations\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# class ErrorDrivenEnsemble:\n",
    "#     def __init__(self, val_window_size: int = 24):\n",
    "#         \"\"\"\n",
    "#         Inicializa el ensamblador con el tamaño de la ventana de validación\n",
    "#         :param val_window_size: Número de períodos a usar para validación (por defecto 24 meses)\n",
    "#         \"\"\"\n",
    "#         self.val_window_size = val_window_size\n",
    "#         self.predictors = []\n",
    "#         self.weights = []\n",
    "#         self.best_combination = None\n",
    "#         self.results = None\n",
    "\n",
    "#     def add_predictor(self, predictor_config: Dict):\n",
    "#         \"\"\"\n",
    "#         Agrega una configuración de predictor al ensamblaje\n",
    "#         \"\"\"\n",
    "#         self.predictors.append(predictor_config)\n",
    "\n",
    "#     def _prepare_validation_data(self, data: TimeSeriesDataFrame):\n",
    "#         \"\"\"Prepara los datos de entrenamiento y validación con validaciones\"\"\"\n",
    "#         if len(data) < self.val_window_size:\n",
    "#             raise ValueError(f\"No hay suficientes datos. Necesitas al menos {self.val_window_size} observaciones\")\n",
    "            \n",
    "#         train_data = data.iloc[:-self.val_window_size]\n",
    "#         val_data = data.iloc[-self.val_window_size:]\n",
    "        \n",
    "#         if val_data['target'].isnull().any():\n",
    "#             raise ValueError(\"El conjunto de validación contiene valores NaN en el target\")\n",
    "            \n",
    "#         return train_data, val_data\n",
    "\n",
    "#     def _train_predictor(self, train_data: TimeSeriesDataFrame, config: Dict):\n",
    "#         \"\"\"Entrena un predictor individual con manejo de errores\"\"\"\n",
    "#         try:\n",
    "#             predictor = TimeSeriesPredictor(\n",
    "#                 target='target',\n",
    "#                 prediction_length=1,  # Cambiado a 1 para predecir mes+2\n",
    "#                 freq=\"M\",\n",
    "#                 eval_metric=\"MAPE\",\n",
    "#                 quantile_levels=[0.1, 0.5, 0.9]\n",
    "#             )\n",
    "            \n",
    "#             predictor.fit(\n",
    "#                 train_data=train_data,\n",
    "#                 **config['train_kwargs']\n",
    "#             )\n",
    "#             return predictor\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error durante entrenamiento: {str(e)}\")\n",
    "#             return None\n",
    "\n",
    "#     def _evaluate_predictor(self, predictor, val_data: TimeSeriesDataFrame):\n",
    "#         \"\"\"Versión robusta del evaluador\"\"\"\n",
    "#         if predictor is None:\n",
    "#             return float('inf'), None\n",
    "            \n",
    "#         try:\n",
    "#             preds = predictor.predict(val_data)\n",
    "            \n",
    "#             if len(preds) == 0:\n",
    "#                 print(\"⚠️ El predictor no generó predicciones\")\n",
    "#                 return float('inf'), None\n",
    "                \n",
    "#             # Asegurar alineación de índices\n",
    "#             val_targets = val_data['target']\n",
    "#             aligned_preds = preds['mean'].reindex(val_targets.index)\n",
    "            \n",
    "#             if aligned_preds.isnull().any():\n",
    "#                 print(\"⚠️ Algunas predicciones no están disponibles para evaluación\")\n",
    "#                 return float('inf'), preds\n",
    "                \n",
    "#             mae = mean_absolute_error(val_targets, aligned_preds)\n",
    "#             return mae, preds\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error durante evaluación: {str(e)}\")\n",
    "#             return float('inf'), None\n",
    "\n",
    "#     def optimize_weights(self, data: TimeSeriesDataFrame, find_best_subset: bool = True, max_models: int = 3):\n",
    "#         \"\"\"Versión mejorada con manejo robusto de errores\"\"\"\n",
    "#         train_data, val_data = self._prepare_validation_data(data)\n",
    "#         results = []\n",
    "        \n",
    "#         for config in self.predictors:\n",
    "#             print(f\"\\nEntrenando {config['name']}...\")\n",
    "#             predictor = self._train_predictor(train_data, config)\n",
    "#             mae, preds = self._evaluate_predictor(predictor, val_data)\n",
    "            \n",
    "#             if preds is not None:\n",
    "#                 results.append({\n",
    "#                     'name': config['name'],\n",
    "#                     'predictor': predictor,\n",
    "#                     'mae': mae,\n",
    "#                     'val_predictions': preds\n",
    "#                 })\n",
    "#                 print(f\"{config['name']} - MAE: {mae:.2f}\")\n",
    "        \n",
    "#         if not results:\n",
    "#             raise ValueError(\"Ningún predictor se entrenó exitosamente\")\n",
    "        \n",
    "#         self.results = results\n",
    "        \n",
    "#         if find_best_subset and len(results) > 1:\n",
    "#             self.best_combination = self.find_best_subset(max_models)\n",
    "#             selected_indices = [i for i, r in enumerate(results) if r['name'] in self.best_combination['models']]\n",
    "#             self.results = [results[i] for i in selected_indices]\n",
    "#             self.weights = self.best_combination['weights']\n",
    "#         else:\n",
    "#             maes = np.array([r['mae'] for r in results])\n",
    "#             self.weights = (1 / maes) / (1 / maes).sum()\n",
    "#             self.best_combination = {\n",
    "#                 'names': [r['name'] for r in results],\n",
    "#                 'weights': self.weights.tolist(),\n",
    "#                 'mean_mae': (maes * self.weights).sum(),\n",
    "#                 'best_mae': maes.min()\n",
    "#             }\n",
    "        \n",
    "#         return self.best_combination\n",
    "\n",
    "#     def find_best_subset(self, max_models=3):\n",
    "#         \"\"\"Encuentra la mejor combinación de modelos\"\"\"\n",
    "#         model_names = [r['name'] for r in self.results]\n",
    "#         best_score = float('inf')\n",
    "#         best_combo = None\n",
    "        \n",
    "#         for k in range(1, min(max_models, len(model_names)) + 1):\n",
    "#             for combo in combinations(range(len(model_names)), k):\n",
    "#                 maes = [self.results[i]['mae'] for i in combo]\n",
    "#                 weights = (1 / np.array(maes)) / (1 / np.array(maes)).sum()\n",
    "#                 weighted_mae = (np.array(maes) * weights).sum()\n",
    "                \n",
    "#                 if weighted_mae < best_score:\n",
    "#                     best_score = weighted_mae\n",
    "#                     best_combo = {\n",
    "#                         'models': [model_names[i] for i in combo],\n",
    "#                         'weights': weights.tolist(),\n",
    "#                         'mae': weighted_mae\n",
    "#                     }\n",
    "        \n",
    "#         print(f\"\\nMejor combinación: {best_combo['models']}\")\n",
    "#         print(f\"Pesos: {best_combo['weights']}\")\n",
    "#         print(f\"MAE ponderado: {best_combo['mae']:.4f}\")\n",
    "#         return best_combo\n",
    "\n",
    "#     def plot_ensemble_results(self):\n",
    "#         \"\"\"Visualiza resultados\"\"\"\n",
    "#         fig, ax = plt.subplots(figsize=(10, 6))\n",
    "#         ax.bar([r['name'] for r in self.results], [r['mae'] for r in self.results], label='MAE Individual')\n",
    "#         ax.axhline(self.best_combination['mae'], color='r', linestyle='--', label='MAE Ensamblado')\n",
    "#         ax.set_title('Comparación de Modelos')\n",
    "#         ax.set_ylabel('MAE')\n",
    "#         ax.legend()\n",
    "#         plt.xticks(rotation=45)\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "\n",
    "#     def predict(self, data: TimeSeriesDataFrame):\n",
    "#         \"\"\"Genera predicciones ensambladas\"\"\"\n",
    "#         all_preds = []\n",
    "#         for weight, result in zip(self.weights, self.results):\n",
    "#             preds = result['predictor'].predict(data)\n",
    "#             preds['mean'] = preds['mean'] * weight\n",
    "#             all_preds.append(preds[['mean']])\n",
    "        \n",
    "#         ensemble_pred = sum(all_preds)\n",
    "#         ensemble_pred['0.1'] = all_preds[0]['0.1']\n",
    "#         ensemble_pred['0.9'] = all_preds[0]['0.9']\n",
    "#         return ensemble_pred\n",
    "\n",
    "# # Ejemplo de uso\n",
    "# def main():\n",
    "#     # 1. Cargar y preparar datos\n",
    "#     df = pd.read_csv(\"./datasets/periodo_x_producto_con_target.csv\", sep=',', encoding='utf-8')\n",
    "#     df['timestamp'] = pd.to_datetime(df['periodo'].astype(str), format='%Y%m')\n",
    "#     df = df.rename(columns={'product_id': 'item_id'}).dropna(subset=['target'])\n",
    "    \n",
    "#     # Convertir a TimeSeriesDataFrame\n",
    "#     data = TimeSeriesDataFrame.from_data_frame(\n",
    "#         df[['item_id', 'timestamp', 'target']],\n",
    "#         id_column=\"item_id\",\n",
    "#         timestamp_column=\"timestamp\"\n",
    "#     )\n",
    "\n",
    "#     # 2. Configurar ensamblador\n",
    "#     ensemble = ErrorDrivenEnsemble(val_window_size=3)\n",
    "    \n",
    "#     # Configuraciones de modelos mejoradas\n",
    "#     ensemble.add_predictor({\n",
    "#         'name': 'deepar',\n",
    "#         'train_kwargs': {\n",
    "#             'hyperparameters': {\n",
    "#                 'DeepAR': {\n",
    "#                     'epochs': 50,\n",
    "#                     'num_cells': 64,\n",
    "#                     'context_length': 12\n",
    "#                 }\n",
    "#             },\n",
    "#             'num_val_windows': 5\n",
    "#         }\n",
    "#     })\n",
    "    \n",
    "#     ensemble.add_predictor({\n",
    "#         'name': 'feedforward',\n",
    "#         'train_kwargs': {\n",
    "#             'hyperparameters': {\n",
    "#                 'SimpleFeedForward': {\n",
    "#                     'epochs': 30,\n",
    "#                     'num_hidden_dimensions': [64]\n",
    "#                 }\n",
    "#             }\n",
    "#         }\n",
    "#     })\n",
    "\n",
    "#     # 3. Optimizar y evaluar\n",
    "#     optimization_result = ensemble.optimize_weights(data)\n",
    "#     ensemble.plot_ensemble_results()\n",
    "    \n",
    "#     # 4. Generar y guardar predicciones\n",
    "#     predictions = ensemble.predict(data)\n",
    "#     # predictions.reset_index().to_csv(\"./outputs/ensemble_predictions.csv\", index=False)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b391b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SimpleErrorDrivenEnsemble:\n",
    "    def __init__(self, prediction_length=2, val_window_size=24):\n",
    "        self.prediction_length = prediction_length\n",
    "        self.val_window_size = val_window_size\n",
    "        self.predictors = {}              # {model_name: hyperparameters}\n",
    "        self.model_errors = {}            # {model_name: mae}\n",
    "        self.model_weights = {}           # {model_name: weight}\n",
    "        self.trained_predictors = {}      # {model_name: TimeSeriesPredictor}\n",
    "\n",
    "    def prepare_data(self, df, id_col='product_id', time_col='periodo', target_col='tn'):\n",
    "        df['timestamp'] = pd.to_datetime(df[time_col].astype(str), format='%Y%m')\n",
    "        df = df.rename(columns={id_col: 'item_id'}).dropna(subset=['target'])\n",
    "        df = df.groupby('item_id').filter(lambda x: x['target'].notna().sum() >= (self.val_window_size + 12))\n",
    "        return TimeSeriesDataFrame.from_data_frame(\n",
    "            df[['item_id', 'timestamp', 'target']],\n",
    "            id_column='item_id',\n",
    "            timestamp_column='timestamp'\n",
    "        )\n",
    "\n",
    "    def add_model(self, name, hyperparameters):\n",
    "        self.predictors[name] = hyperparameters\n",
    "\n",
    "    def train_and_evaluate(self, data: TimeSeriesDataFrame):\n",
    "        train_data, val_data = data.train_test_split(self.val_window_size)\n",
    "\n",
    "        if len(val_data) == 0:\n",
    "            raise ValueError(\"El conjunto de validación quedó vacío. Revisá que haya suficientes datos por serie.\")\n",
    "\n",
    "        for name, hyperparams in self.predictors.items():\n",
    "            print(f\"Entrenando modelo: {name}\")\n",
    "            predictor = TimeSeriesPredictor(\n",
    "                target='target',\n",
    "                prediction_length=self.prediction_length,\n",
    "                freq='M',\n",
    "                eval_metric='MAPE'\n",
    "            )\n",
    "            predictor.fit(train_data, hyperparameters={name: hyperparams}, verbosity=0)\n",
    "            self.trained_predictors[name] = predictor\n",
    "\n",
    "            preds = predictor.predict(val_data)\n",
    "\n",
    "            y_true = val_data['target']\n",
    "            y_pred = preds['mean'].reindex(y_true.index)\n",
    "\n",
    "            if len(y_pred) != len(y_true):\n",
    "                raise ValueError(f\"Desajuste entre y_true ({len(y_true)}) y y_pred ({len(y_pred)}).\")\n",
    "\n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "            self.model_errors[name] = mae\n",
    "\n",
    "        self._compute_weights()\n",
    "\n",
    "\n",
    "    def _compute_weights(self):\n",
    "        errors = pd.Series(self.model_errors)\n",
    "        weights = (1 / errors) / (1 / errors).sum()\n",
    "        self.model_weights = weights.to_dict()\n",
    "        print(\"\\nPesos (calculados en base a error inverso):\")\n",
    "        print(self.model_weights)\n",
    "\n",
    "    def predict(self, data: TimeSeriesDataFrame):\n",
    "        ensemble = None\n",
    "        for name, predictor in self.trained_predictors.items():\n",
    "            preds = predictor.predict(data)['mean']\n",
    "            weight = self.model_weights.get(name, 0)\n",
    "            if ensemble is None:\n",
    "                ensemble = preds * weight\n",
    "            else:\n",
    "                ensemble += preds * weight\n",
    "        return ensemble.reset_index(name='ensemble_pred')\n",
    "\n",
    "    def plot_model_errors(self):\n",
    "        pd.Series(self.model_errors).sort_values().plot(kind='barh', title='MAE por modelo')\n",
    "        plt.xlabel('MAE')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e18bfeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Frequency 'M' stored as 'ME'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo: DeepAR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tWarning: Exception caused DeepAR to fail during training... Skipping this model.\n",
      "\tParameter 'epochs' cannot be specified when 'max_epochs' is also specified.\n",
      "Trainer has no fit models that can predict.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Trainer has no fit models that can predict.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m ensemble.add_model(\u001b[33m'\u001b[39m\u001b[33mSimpleFeedForward\u001b[39m\u001b[33m'\u001b[39m, {\u001b[33m'\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m40\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mnum_hidden_dimensions\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m64\u001b[39m]})\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# 5. Entrenar y evaluar\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mensemble\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mts_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# 6. Visualizar errores\u001b[39;00m\n\u001b[32m     18\u001b[39m ensemble.plot_model_errors()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mSimpleErrorDrivenEnsemble.train_and_evaluate\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m     43\u001b[39m predictor.fit(train_data, hyperparameters={name: hyperparams}, verbosity=\u001b[32m0\u001b[39m)\n\u001b[32m     44\u001b[39m \u001b[38;5;28mself\u001b[39m.trained_predictors[name] = predictor\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m preds = \u001b[43mpredictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m y_true = val_data[\u001b[33m'\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     49\u001b[39m y_pred = preds[\u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m].reindex(y_true.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Usuario\\.conda\\envs\\py311lab3\\Lib\\site-packages\\autogluon\\timeseries\\predictor.py:859\u001b[39m, in \u001b[36mTimeSeriesPredictor.predict\u001b[39m\u001b[34m(self, data, known_covariates, model, use_cache, random_seed)\u001b[39m\n\u001b[32m    857\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m known_covariates \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    858\u001b[39m     known_covariates = \u001b[38;5;28mself\u001b[39m._to_data_frame(known_covariates)\n\u001b[32m--> \u001b[39m\u001b[32m859\u001b[39m predictions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_learner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m    \u001b[49m\u001b[43mknown_covariates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mknown_covariates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(TimeSeriesDataFrame, predictions.reindex(original_item_id_order, level=ITEMID))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Usuario\\.conda\\envs\\py311lab3\\Lib\\site-packages\\autogluon\\timeseries\\learner.py:174\u001b[39m, in \u001b[36mTimeSeriesLearner.predict\u001b[39m\u001b[34m(self, data, known_covariates, model, use_cache, random_seed, **kwargs)\u001b[39m\n\u001b[32m    172\u001b[39m known_covariates = \u001b[38;5;28mself\u001b[39m.feature_generator.transform_future_known_covariates(known_covariates)\n\u001b[32m    173\u001b[39m known_covariates = \u001b[38;5;28mself\u001b[39m._align_covariates_with_forecast_index(known_covariates=known_covariates, data=data)\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m    \u001b[49m\u001b[43mknown_covariates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mknown_covariates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Usuario\\.conda\\envs\\py311lab3\\Lib\\site-packages\\autogluon\\timeseries\\trainer.py:778\u001b[39m, in \u001b[36mTimeSeriesTrainer.predict\u001b[39m\u001b[34m(self, data, known_covariates, model, use_cache, random_seed)\u001b[39m\n\u001b[32m    770\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\n\u001b[32m    771\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    772\u001b[39m     data: TimeSeriesDataFrame,\n\u001b[32m   (...)\u001b[39m\u001b[32m    776\u001b[39m     random_seed: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    777\u001b[39m ) -> TimeSeriesDataFrame:\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m     model_name = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_model_for_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m     model_pred_dict, _ = \u001b[38;5;28mself\u001b[39m.get_model_pred_dict(\n\u001b[32m    780\u001b[39m         model_names=[model_name],\n\u001b[32m    781\u001b[39m         data=data,\n\u001b[32m   (...)\u001b[39m\u001b[32m    784\u001b[39m         random_seed=random_seed,\n\u001b[32m    785\u001b[39m     )\n\u001b[32m    786\u001b[39m     predictions = model_pred_dict[model_name]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Usuario\\.conda\\envs\\py311lab3\\Lib\\site-packages\\autogluon\\timeseries\\trainer.py:754\u001b[39m, in \u001b[36mTimeSeriesTrainer._get_model_for_prediction\u001b[39m\u001b[34m(self, model, verbose)\u001b[39m\n\u001b[32m    752\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    753\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_best \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m754\u001b[39m         best_model_name: \u001b[38;5;28mstr\u001b[39m = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_model_best\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    755\u001b[39m         \u001b[38;5;28mself\u001b[39m.model_best = best_model_name\n\u001b[32m    756\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Usuario\\.conda\\envs\\py311lab3\\Lib\\site-packages\\autogluon\\timeseries\\trainer.py:214\u001b[39m, in \u001b[36mTimeSeriesTrainer.get_model_best\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    212\u001b[39m models = \u001b[38;5;28mself\u001b[39m.get_model_names()\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m models:\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTrainer has no fit models that can predict.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(models) == \u001b[32m1\u001b[39m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m models[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: Trainer has no fit models that can predict."
     ]
    }
   ],
   "source": [
    "# 1. Cargar tus datos originales\n",
    "df = pd.read_csv(\"./datasets/periodo_x_producto_con_target.csv\")\n",
    "\n",
    "# 2. Inicializar ensemble\n",
    "ensemble = SimpleErrorDrivenEnsemble(prediction_length=1, val_window_size=3)\n",
    "\n",
    "# 3. Preparar datos\n",
    "ts_data = ensemble.prepare_data(df)\n",
    "\n",
    "# 4. Agregar modelos a usar\n",
    "ensemble.add_model('DeepAR', {'epochs': 30, 'context_length': 12})\n",
    "ensemble.add_model('SimpleFeedForward', {'epochs': 40, 'num_hidden_dimensions': [64]})\n",
    "\n",
    "# 5. Entrenar y evaluar\n",
    "ensemble.train_and_evaluate(ts_data)\n",
    "\n",
    "# 6. Visualizar errores\n",
    "ensemble.plot_model_errors()\n",
    "\n",
    "# 7. Predecir sobre los datos\n",
    "ensemble_preds = ensemble.predict(ts_data)\n",
    "print(ensemble_preds.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311lab3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
